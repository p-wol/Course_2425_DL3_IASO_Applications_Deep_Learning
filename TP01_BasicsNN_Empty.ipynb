{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d130573",
   "metadata": {},
   "source": [
    "# Manipulate the device and the precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb149b",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "In PyTorch, each tensor has a `device` and a `dtype`. The `device` refers to the device on which the tensor is currently loaded. The `dtype` is the type of data stored in the tensor. By default, `device = torch.device(\"cpu\")` and `dtype = torch.float32`.\n",
    "\n",
    "One can build a tensor directly on a given device and with a given data type (or precision) by using the optional arguments `device` and `dtype`, or we can build them first with by-default values, and send copy them on the desired device with the desired precision with the `to` method.\n",
    "\n",
    "Use both methods with devices `torch.device(\"cuda\")` and `torch.device(\"cpu\")` and with data types other than `torch.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cuda = torch.cuda.is_available()\n",
    "if with_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33814f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "401555f6",
   "metadata": {},
   "source": [
    "# Backpropagation of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1b0bc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Code libraries such as PyTorch, TensorFlow, JAX implement **automatic differentation (AutoDiff)** methods, which are at the core of ML methods trainable by gradient-based optimization techniques. These ML method include naturally neural networks.\n",
    "\n",
    "AutoDiff is based on the construction of a computational graph, which is a *Directed Acyclic Graph* (DAG). This graph represents the different stages of computation of a function $f$ at a point $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_S)$, and we want to compute: \n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\theta_1}, \\frac{\\partial f}{\\partial \\theta_2}, \\cdots, \n",
    "\\frac{\\partial f}{\\partial \\theta_S} .\n",
    "$$\n",
    "The DAG is then made of:\n",
    " * *directed* edges: represent the flows of data;\n",
    " * the root of the graph (a node which does not has any child): contains the (scalar) result of the computation of $f(\\boldsymbol{\\theta})$;\n",
    " * the leaf nodes (nodes which do not have any parent): contain the variables $(\\theta_1, \\theta_2, \\cdots, \\theta_S)$ with repect to which we want to differentiate $f$;\n",
    " * the non-leaf nodes: contain an operation to perform on their inputs.\n",
    "\n",
    "In PyTorch, the leaf nodes are built either automatically when initializing a `torch.nn.Module` (such as a linear layer, in which weights and biases are by-default leaf nodes), or manually trough the class `torch.nn.Parameter`. Tensors that we use during a computation, but that are not variables of our function are just instances of the class `torch.Tensor` (with `requires_grad = False`).\n",
    "\n",
    "The DAG is built on-the-fly during the successive operations, and the gradients are computed only when calling the `backward` method on the root node, or the function `torch.autograd.grad` on the root node and the leaf nodes with respect to which we want to differentiate. Using `backward` is very straightforward, while `torch.autograd.grad` is slightly more complex, but it supports advanced custom operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c31df7",
   "metadata": {},
   "source": [
    "## Parameters and tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f1a65",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Build some `torch.nn.Parameter`, some `torch.Tensor` and some `torch.Tensor` with `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b329a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09b2bd38",
   "metadata": {},
   "source": [
    "Note: The main difference between a `Parameter` and a `Tensor` with `requires_grad = True` is how they are managed inside a PyTorch `Module`. To avoid any unexpected behavior, one should prefer to use `Parameter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8108cb",
   "metadata": {},
   "source": [
    "## Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9332678",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Let $f$ be the function defined below. Compute its derivative with respect to `x1` and `x2` by using `backward`, and then by using `torch.autograd.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a0ffc",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e563f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259c9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a95a34",
   "metadata": {},
   "source": [
    "### torch.autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cbc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "357e6e67",
   "metadata": {},
   "source": [
    "### Higher-order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e546633",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Compute $\\frac{\\partial f}{\\partial x_1 \\partial x_2}$ by using `torch.autograd.grad` twice. One check how to use the additional parameters `create_graph` and `allow_unused`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459cb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bee8100",
   "metadata": {},
   "source": [
    "## Access the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6daae9",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "The root node contains all the required information to perform the backpropagation and compute the derivatives. The graph and the nodes can be accessed with the methods `grad_fn` and `next_functions`. The leaf nodes can be accessed with the `variable` method. Before the backward, the intermediady results can be accessed with `_saved_self`.\n",
    "\n",
    "Access the various nodes of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cdf684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce79bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65685068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other example\n",
    "\n",
    "x = nn.Parameter(torch.randn(6))\n",
    "y = torch.split(x, 2)\n",
    "z = sum(y).sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff732117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a20d692",
   "metadata": {},
   "source": [
    "## Forward-only mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dadbf",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Sometimes, for instance when testing a model, we are just interested in the result, and not the gradients. Such computations are more efficient if we are in a \"forward-only\" mode (in that situation, the computational graph is not created and the intermediary results are not stored).\n",
    "\n",
    "The easiest way is to use `with torch.no_grad()`. One can also use the method `detach` on the parameters to use their content without building the computational graph.\n",
    "\n",
    "Compute the function $f$ without building the computational graph using both methods and check that no computational graph is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90917ce1",
   "metadata": {},
   "source": [
    "# Managing a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19cb4f6",
   "metadata": {},
   "source": [
    "Before training a model on a dataset, we have to make some basic checks on the dataset and preprocess it correctly in order to obtain the best possible results (on a validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"\" # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946f3ce",
   "metadata": {},
   "source": [
    "## Checking a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6d630",
   "metadata": {},
   "source": [
    "First, we load MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1829ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = False, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = False, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f47f7",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "By using `numpy.random.choice`, subplots and the function `imshow` of matplotlib, print 10 (or more) random data points of the training set (image + label).\n",
    "\n",
    "Is the task feasible for a human? Do the data look clean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edad397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ce31f09",
   "metadata": {},
   "source": [
    "## Check for class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f8913",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "In classification tasks, it is common that the different classes are inequally represented in the dataset. If this \"class imbalance\" is too severe, the model is likely to fail to learn well the least represented classes.\n",
    "\n",
    "Compute the number of data points in each class (in the training dataset). What do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe12f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a0442c4",
   "metadata": {},
   "source": [
    "Theoretical number of data points per class if the dataset was perfectly balanced: 6000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e481c4",
   "metadata": {},
   "source": [
    "# Managing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ee497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.act_function = torch.tanh\n",
    "        layers = [1, 6, 16, 120, 84, 10]\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(layers[0], layers[1], 5, padding = 2)\n",
    "        self.conv2 = torch.nn.Conv2d(layers[1], layers[2], 5)\n",
    "        self.fc1 = torch.nn.Linear(5 * 5 * layers[2], layers[3])\n",
    "        self.fc2 = torch.nn.Linear(layers[3], layers[4])\n",
    "        self.fc3 = torch.nn.Linear(layers[4], layers[5])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_function(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_function(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5f803",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "When a `torch.nn.Module` is created, its parameters and its submodules are automatically registered, which is necessary to optimize them with an `Optimizer`. One can access them with the methods `named_parameters`, `parameters`, `named_modules`, `modules`.\n",
    "\n",
    "Access the various elements on an instance of LeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8184ad91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0b6e939",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Create an instance of the class below and print its modules and parameters. What do we observe? Fix this issue by using the object `torch.nn.ModuleList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SomeModel, self).__init__()\n",
    "\n",
    "        self.layers = [torch.nn.Linear(5, 5),\n",
    "                      torch.nn.ReLU(),\n",
    "                      torch.nn.Linear(5, 1)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bdcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130444d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeModelFixed(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d53d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97ce5895",
   "metadata": {},
   "source": [
    "**Question 3 (optional)**\n",
    "\n",
    "Check out the method `register_buffer` of `Module`, explain why it can be useful and show a use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d9737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1ed12bf",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b782325",
   "metadata": {},
   "source": [
    "## Basic training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe731fa",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "We want to train LeNet on MNIST. Fill the blanks in the following pieces of code.\n",
    "\n",
    "Wrap the training process into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ee724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = # ...\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 50\n",
    "\n",
    "#List to store loss to visualize\n",
    "valid_loss_min = np.inf # track change in validation loss\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "acc_eval = []\n",
    "#test_counter = [i*len(train_loader.dataset) for i in n_epochs]\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # ...\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # ...\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f172534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30e09afa",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4420490",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "To make sure that the inputs of the neural network are within a controlled range, we usually transform the dataset to be sure that the data are centered with variance 1. It is not always necessary, but it is worth knowing it.\n",
    "\n",
    "Check the range of values on a sample of MNIST. Compute the mean and the standard deviation of the training dataset of MNIST and normalize the dataset accordingly by using `transforms.Normalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99f662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0c9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0aa70c8",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a7be9",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "It is very common to face overfitting when doing deep learning. So, several methods can be used to solve this problem. One of them is called \"data augmentation\". It consists in adding \"noise\" to data points of the training dataset in order to make the model resistant to small changes of the data. \n",
    "\n",
    "When training on images, it is common to perform \"random crops\", \"random flips\", and small \"random rotations\". With MNIST, it is meaningless to add random flips, because most digits are not supposed to be invariant by vertical or horizontal symmetries.\n",
    "\n",
    "Add random crops with `transforms.RandomCrop` with a reasonable number of pixels to the transforms to do on the dataset, visualize the resulting images and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca736ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66bfb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68234341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc885868",
   "metadata": {},
   "source": [
    "## Influence of the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18b085",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Build a Multilayer Perceptron with ReLU activation functions, which takes 3 arguments: \n",
    " * `layers`: the list of layer sizes;\n",
    " * `sigma_w`: the standard deviation chosen for initializing the weights;\n",
    " * `with_scaling`: if True, we multiply the generated weights by $1/\\sqrt{\\# \\text{inputs}}$.\n",
    "\n",
    "Write the `reset_parameters` method, which initialize the weights according to a Gaussian distribution, either with variance $\\sigma_w^2$, or $\\sigma_w^2/\\# \\text{inputs}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fad359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, layers, sigma_w, scaling = False):\n",
    "        super(Perceptron, self).__init__()\n",
    "\n",
    "        # ...\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        pass\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1396cf",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Train the model with various choices of initialization (which ones?) and try various numbers of layers with various widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84868847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e018b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c2676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9c4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38414621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39428313",
   "metadata": {},
   "source": [
    "**Question 5 (optional)**\n",
    "\n",
    "Implement the NTK parameterization in the `Perceptron` model: divide by $1/\\sqrt{\\# \\text{inputs}}$ the result of each layer.\n",
    "\n",
    "Train such a network with the SGD (with `scaling = False`) with learning rates of order $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3d4e1",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7de74",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Train LeNet with the SGD and Adam with default parameters and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63435e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e80b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a78a4f46",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6a816",
   "metadata": {},
   "source": [
    "**Additional questions**\n",
    "\n",
    " * explore data augmentation with the dataset CIFAR-10\n",
    " * test different batch sizes. How to change the learning rate when we change the batch size?\n",
    " * test drop-out layers\n",
    " * read the documentation on SGD and Adam an propose a way to assign different learning rates to different sets of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9bfa24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc080af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a20aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
