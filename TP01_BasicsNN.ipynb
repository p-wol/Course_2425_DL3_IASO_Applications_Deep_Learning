{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e44a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d130573",
   "metadata": {},
   "source": [
    "# Manipulate the device and the precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb149b",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "In PyTorch, each tensor has a `device` and a `dtype`. The `device` refers to the device on which the tensor is currently loaded. The `dtype` is the type of data stored in the tensor. By default, `device = torch.device(\"cpu\")` and `dtype = torch.float32`.\n",
    "\n",
    "One can build a tensor directly on a given device and with a given data type (or precision) by using the optional arguments `device` and `dtype`, or we can build them first with by-default values, and send copy them on the desired device with the desired precision with the `to` method.\n",
    "\n",
    "Use both methods with devices `torch.device(\"cuda\")` and `torch.device(\"cpu\")` and with data types other than `torch.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382fba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = # ...\n",
    "\n",
    "with_cuda = False #torch.cuda.is_available()\n",
    "if with_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62a06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1363,  1.6265,  0.5494,  0.3032, -1.2023])\n",
      "tensor([-1.8525,  0.0020, -1.9014, -1.0186,  0.0737], dtype=torch.float16)\n",
      "tensor([-0.3831,  0.6554, -0.1151, -1.7444,  0.2078])\n",
      "tensor([ 1.0215, -1.0332, -0.5967,  0.0336,  0.9536], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Intialize a tensor directly on a device\n",
    "x = torch.randn(5, device = device)\n",
    "print(x)\n",
    "\n",
    "# Intialize a tensor directly on a device with given precision\n",
    "x = torch.randn(5, device = device, dtype = torch.float16)\n",
    "print(x)\n",
    "\n",
    "# Send an object to a device\n",
    "x = torch.randn(5, device = device)\n",
    "x = x.to(device = device)\n",
    "print(x)\n",
    "# Note: the object is first created on the CPU, then copied on the GPU\n",
    "#    if it is not used on the CPU, this operation is suboptimal\n",
    "\n",
    "# Send an object to a device with given precision\n",
    "x = torch.randn(5)\n",
    "x = x.to(device = device, dtype = torch.float16)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401555f6",
   "metadata": {},
   "source": [
    "# Backpropagation of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1b0bc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Code libraries such as PyTorch, TensorFlow, JAX implement **automatic differentation (AutoDiff)** methods, which are at the core of ML methods trainable by gradient-based optimization techniques. These ML method include naturally neural networks.\n",
    "\n",
    "AutoDiff is based on the construction of a computational graph, which is a *Directed Acyclic Graph* (DAG). This graph represents the different stages of computation of a function $f$ at a point $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_S)$, and we want to compute: \n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\theta_1}, \\frac{\\partial f}{\\partial \\theta_2}, \\cdots, \n",
    "\\frac{\\partial f}{\\partial \\theta_S} .\n",
    "$$\n",
    "The DAG is then made of:\n",
    " * *directed* edges: represent the flows of data;\n",
    " * the root of the graph (a node which does not has any child): contains the (scalar) result of the computation of $f(\\boldsymbol{\\theta})$;\n",
    " * the leaf nodes (nodes which do not have any parent): contain the variables $(\\theta_1, \\theta_2, \\cdots, \\theta_S)$ with repect to which we want to differentiate $f$;\n",
    " * the non-leaf nodes: contain an operation to perform on their inputs.\n",
    "\n",
    "In PyTorch, the leaf nodes are built either automatically when initializing a `torch.nn.Module` (such as a linear layer, in which weights and biases are by-default leaf nodes), or manually trough the class `torch.nn.Parameter`. Tensors that we use during a computation, but that are not variables of our function are just instances of the class `torch.Tensor` (with `requires_grad = False`).\n",
    "\n",
    "The DAG is built on-the-fly during the successive operations, and the gradients are computed only when calling the `backward` method on the root node, or the function `torch.autograd.grad` on the root node and the leaf nodes with respect to which we want to differentiate. Using `backward` is very straightforward, while `torch.autograd.grad` is slightly more complex, but it supports advanced custom operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c31df7",
   "metadata": {},
   "source": [
    "## Parameters and tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f1a65",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Build some `torch.nn.Parameter`, some `torch.Tensor` and some `torch.Tensor` with `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5066745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.4072, -1.3295, -0.3055, -1.0222,  1.0574], requires_grad=True) requires_grad: True\n",
      "Parameter containing:\n",
      "tensor(1.9660, requires_grad=True) requires_grad: True\n",
      "tensor([-0.9121,  1.1328, -1.2622, -0.0960,  0.8719]) requires_grad: False\n",
      "tensor(1.1258)\n",
      "tensor([[-0.7159,  1.1954, -2.0856, -1.3799],\n",
      "        [-0.8468, -1.0203,  0.2898,  0.8801],\n",
      "        [ 0.8480, -0.6460, -0.4744, -1.1870]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "c = torch.randn(3, 4, requires_grad = True)\n",
    "\n",
    "print(x1, \"requires_grad:\", x1.requires_grad)\n",
    "print(x2, \"requires_grad:\", x2.requires_grad)\n",
    "print(a, \"requires_grad:\", a.requires_grad)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2bd38",
   "metadata": {},
   "source": [
    "Note: The main difference between a `Parameter` and a `Tensor` with `requires_grad = True` is how they are managed inside a PyTorch `Module`. To avoid any unexpected behavior, one should prefer to use `Parameter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8108cb",
   "metadata": {},
   "source": [
    "## Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9332678",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Let $f$ be the function defined below. Compute its derivative with respect to `x1` and `x2` by using `backward`, and then by using `torch.autograd.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a0ffc",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e563f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a259c9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor([-0.0010,  0.0258, -0.0242, -0.0190,  0.0124]) tensor(-0.9923)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pwolinski/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "print(x1.grad, x2.grad)\n",
    "y.backward()\n",
    "print(x1.grad, x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a95a34",
   "metadata": {},
   "source": [
    "### torch.autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7cbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6831205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of x1: tensor([ 0.3556, -1.0077,  0.2780,  0.4267, -0.5709])\n",
      "grad of x2: tensor(0.8219)\n"
     ]
    }
   ],
   "source": [
    "grads = torch.autograd.grad(y, (x1, x2))\n",
    "print(\"grad of x1:\", grads[0])\n",
    "print(\"grad of x2:\", grads[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e6e67",
   "metadata": {},
   "source": [
    "### Higher-order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e546633",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Compute $\\frac{\\partial f}{\\partial x_1 \\partial x_2}$ by using `torch.autograd.grad` twice. One check how to use the additional parameters `create_graph` and `allow_unused`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f459cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e11e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads: tensor([ 0.0250, -0.7845, -0.0393, -0.7373, -0.5732], grad_fn=<MulBackward0>) tensor(-0.7660, grad_fn=<MulBackward0>)\n",
      "hessian_x2_x1: (tensor([ 0.0189, -0.5938, -0.0297, -0.5580, -0.4339]),)\n"
     ]
    }
   ],
   "source": [
    "grad_x1, grad_x2 = torch.autograd.grad(y, (x1, x2), create_graph = True)\n",
    "print(\"grads:\", grad_x1, grad_x2)\n",
    "hessian_x2_x1 = torch.autograd.grad(grad_x2, (x1,), retain_graph = True, allow_unused = True)\n",
    "print(\"hessian_x2_x1:\", hessian_x2_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee8100",
   "metadata": {},
   "source": [
    "## Access the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6daae9",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "The root node contains all the required information to perform the backpropagation and compute the derivatives. The graph and the nodes can be accessed with the methods `grad_fn` and `next_functions`. The leaf nodes can be accessed with the `variable` method. Before the backward, the intermediady results can be accessed with `_saved_self`.\n",
    "\n",
    "Access the various nodes of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81cdf684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0559, grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x741b173967d0>\n",
      "((<AccumulateGrad object at 0x741b17979120>, 0), (<SinBackward0 object at 0x741b173d7b80>, 0))\n"
     ]
    }
   ],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "y.backward()\n",
    "\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dce79bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8798, grad_fn=<AddBackward0>)\n",
      "tensor(-0.3804, grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x741b173967d0>\n",
      "((<AccumulateGrad object at 0x741b1737eb00>, 0), (<SinBackward0 object at 0x741c0ad37ee0>, 0))\n",
      "Parameter containing:\n",
      "tensor(-0.4937, requires_grad=True)\n",
      "((<AddBackward0 object at 0x741b173967d0>, 0),)\n",
      "((<SumBackward0 object at 0x741c0ad37ee0>, 0), (None, 0))\n",
      "((<MulBackward0 object at 0x741b173d7e50>, 0),)\n"
     ]
    }
   ],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "\n",
    "print(y.grad_fn.next_functions[1][0]._saved_self)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions)\n",
    "\n",
    "print(y.grad_fn.next_functions[0][0].variable)\n",
    "print(y.grad_fn.next_functions[1][0].next_functions)\n",
    "print(y.grad_fn.next_functions[1][0].next_functions[0][0].next_functions)\n",
    "print(y.grad_fn.next_functions[1][0].next_functions[0][0].next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65685068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other example\n",
    "\n",
    "x = nn.Parameter(torch.randn(6))\n",
    "y = torch.split(x, 2)\n",
    "z = sum(y).sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff732117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<AddBackward0 object at 0x741b17215630>, 0),)\n",
      "((<AddBackward0 object at 0x741b17215630>, 0), (<SplitBackward0 object at 0x741b17215780>, 2))\n",
      "((<AddBackward0 object at 0x741b17215720>, 0), (<SplitBackward0 object at 0x741b17215780>, 1))\n"
     ]
    }
   ],
   "source": [
    "print(z.grad_fn.next_functions)\n",
    "print(z.grad_fn.next_functions[0][0].next_functions)\n",
    "print(z.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200da26",
   "metadata": {},
   "source": [
    "## Visualize the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dd88e",
   "metadata": {},
   "source": [
    "**Question 5 (optional)**\n",
    "\n",
    "Visualize the computational graph with the package `torchviz` (function `make_dot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363f7256",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[1;32m      3\u001b[0m x1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      4\u001b[0m x2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "\n",
    "make_dot(y, {\"x1\": x1, \"x2\": x2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20d692",
   "metadata": {},
   "source": [
    "## Forward-only mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dadbf",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Sometimes, for instance when testing a model, we are just interested in the result, and not the gradients. Such computations are more efficient if we are in a \"forward-only\" mode (in that situation, the computational graph is not created and the intermediary results are not stored).\n",
    "\n",
    "The easiest way is to use `with torch.no_grad()`. One can also use the method `detach` on the parameters to use their content without building the computational graph.\n",
    "\n",
    "Compute the function $f$ without building the computational graph using both methods and check that no computational graph is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "084f6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5674)\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad()\n",
    "\n",
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "with torch.no_grad(): # Faster computation when no gradient is needed\n",
    "    y = x2 * torch.sin((a * x1).sum() + b)\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ff3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0325)\n"
     ]
    }
   ],
   "source": [
    "# detach()\n",
    "\n",
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2.detach() * torch.sin((a * x1.detach()).sum() + b)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90917ce1",
   "metadata": {},
   "source": [
    "# Managing a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19cb4f6",
   "metadata": {},
   "source": [
    "Before training a model on a dataset, we have to make some basic checks on the dataset and preprocess it correctly in order to obtain the best possible results (on a validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b0e6791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5946f3ce",
   "metadata": {},
   "source": [
    "## Checking a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6d630",
   "metadata": {},
   "source": [
    "First, we load MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d1829ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = False, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = False, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f47f7",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "By using `numpy.random.choice`, subplots and the function `imshow` of matplotlib, print 10 (or more) random data points of the training set (image + label).\n",
    "\n",
    "Is the task feasible for a human? Do the data look clean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3edad397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x350 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_idx = np.random.choice(train_size, 10, replace = False)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (7.5, 3.5))\n",
    "\n",
    "for i, idx in enumerate(lst_idx):\n",
    "    img, target = train_data[idx]\n",
    "    ax[i//5, i%5].imshow(img.numpy().transpose(1, 2, 0), cmap=\"Greys\")\n",
    "    ax[i//5, i%5].set_title(classes[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce31f09",
   "metadata": {},
   "source": [
    "## Check for class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f8913",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "In classification tasks, it is common that the different classes are inequally represented in the dataset. If this \"class imbalance\" is too severe, the model is likely to fail to learn well the least represented classes.\n",
    "\n",
    "Compute the number of data points in each class (in the training dataset). What do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33fe12f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n"
     ]
    }
   ],
   "source": [
    "ndata_per_class = 0\n",
    "\n",
    "for data, target in train_loader:\n",
    "    ndata_per_class += torch.bincount(target)\n",
    "    \n",
    "print(ndata_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0442c4",
   "metadata": {},
   "source": [
    "Theoretical number of data points per class if the dataset was perfectly balanced: 6000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e481c4",
   "metadata": {},
   "source": [
    "# Managing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "366ee497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.act_function = torch.tanh\n",
    "        layers = [1, 6, 16, 120, 84, 10]\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(layers[0], layers[1], 5, padding = 2)\n",
    "        self.conv2 = torch.nn.Conv2d(layers[1], layers[2], 5)\n",
    "        self.fc1 = torch.nn.Linear(5 * 5 * layers[2], layers[3])\n",
    "        self.fc2 = torch.nn.Linear(layers[3], layers[4])\n",
    "        self.fc3 = torch.nn.Linear(layers[4], layers[5])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_function(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_function(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5f803",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "When a `torch.nn.Module` is created, its parameters and its submodules are automatically registered, which is necessary to optimize them with an `Optimizer`. One can access them with the methods `named_parameters`, `parameters`, `named_modules`, `modules`.\n",
    "\n",
    "Access the various elements on an instance of LeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8184ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 0.0166,  0.0098,  0.1171,  0.1481,  0.1435],\n",
      "          [-0.0837, -0.1061, -0.1612, -0.0802,  0.1040],\n",
      "          [-0.1693,  0.1245,  0.0891,  0.0770,  0.0175],\n",
      "          [-0.1351, -0.0190,  0.0285, -0.0528, -0.0789],\n",
      "          [ 0.1020, -0.0441,  0.1122, -0.1009, -0.1253]]],\n",
      "\n",
      "\n",
      "        [[[-0.1314, -0.0923,  0.1745,  0.1935, -0.1820],\n",
      "          [ 0.0216,  0.0072, -0.1638, -0.0734, -0.1431],\n",
      "          [-0.1728,  0.0815,  0.0791, -0.0109,  0.1700],\n",
      "          [-0.1798, -0.0362, -0.1030, -0.0137, -0.1918],\n",
      "          [-0.1276, -0.1142,  0.1038, -0.0566, -0.1313]]],\n",
      "\n",
      "\n",
      "        [[[-0.0615,  0.1726,  0.0978,  0.1373, -0.1087],\n",
      "          [-0.1577, -0.1407, -0.0050,  0.0577,  0.0962],\n",
      "          [-0.0598,  0.1034,  0.0396,  0.0688,  0.1844],\n",
      "          [ 0.0153, -0.0077, -0.1524, -0.0032, -0.1133],\n",
      "          [ 0.1369,  0.0074,  0.0867, -0.0545, -0.1241]]],\n",
      "\n",
      "\n",
      "        [[[-0.1310, -0.1092,  0.0953, -0.0348,  0.1947],\n",
      "          [-0.1586,  0.0826,  0.0830,  0.1152, -0.1587],\n",
      "          [ 0.1994, -0.1105, -0.0845,  0.0685, -0.0398],\n",
      "          [-0.0768,  0.1976,  0.1924, -0.0334,  0.0399],\n",
      "          [-0.0583, -0.0803,  0.1791, -0.1149,  0.1602]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1591,  0.1854, -0.0155,  0.1987,  0.0913],\n",
      "          [ 0.1879, -0.1953,  0.0897, -0.0032, -0.1249],\n",
      "          [ 0.1666,  0.1687, -0.0045,  0.0534,  0.1800],\n",
      "          [-0.0392, -0.1159, -0.1660,  0.1075,  0.1616],\n",
      "          [-0.0373, -0.1272, -0.1320, -0.1298,  0.0431]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0627, -0.0311, -0.0342, -0.1873, -0.0984],\n",
      "          [-0.1057,  0.0199,  0.0245,  0.1285,  0.1586],\n",
      "          [ 0.1751, -0.1868,  0.1746, -0.0748, -0.0514],\n",
      "          [ 0.0819, -0.0570,  0.0984,  0.1110, -0.0305],\n",
      "          [-0.1121,  0.1022,  0.0252, -0.1886,  0.0298]]]], requires_grad=True)), ('conv1.bias', Parameter containing:\n",
      "tensor([ 0.0968, -0.1809,  0.1792,  0.1625,  0.0466,  0.0539],\n",
      "       requires_grad=True)), ('conv2.weight', Parameter containing:\n",
      "tensor([[[[-4.9784e-02, -8.8020e-03,  3.3806e-02, -6.1664e-02, -2.0385e-02],\n",
      "          [ 2.0219e-03, -6.1226e-02,  2.8729e-02, -1.4043e-02,  1.3657e-02],\n",
      "          [-6.6999e-02, -1.7677e-02, -3.1090e-03,  4.1746e-02, -2.6710e-02],\n",
      "          [-7.2222e-02,  1.6029e-02,  3.2397e-02,  1.0473e-02,  4.0676e-05],\n",
      "          [ 2.9545e-02,  6.7674e-02,  3.1869e-02, -7.5961e-02,  4.1157e-02]],\n",
      "\n",
      "         [[ 3.3769e-03,  7.6592e-03,  1.0738e-02,  7.5978e-02,  7.8580e-02],\n",
      "          [-5.0427e-02, -2.3193e-03,  7.1685e-03,  4.6865e-03,  7.0581e-02],\n",
      "          [-7.3436e-03, -6.2599e-02,  1.3730e-02,  7.0737e-02,  7.6586e-02],\n",
      "          [-3.3572e-02, -2.4697e-02, -7.4393e-02, -6.9333e-02, -1.8677e-02],\n",
      "          [ 1.6996e-02,  2.2629e-02, -4.0732e-03,  5.9101e-02, -5.6704e-02]],\n",
      "\n",
      "         [[-7.2141e-03, -4.0125e-02,  3.2590e-02,  4.7092e-02,  6.0686e-02],\n",
      "          [ 2.4660e-02,  5.2729e-02,  7.5102e-02, -7.6954e-02, -7.9430e-02],\n",
      "          [ 3.0718e-03, -2.4209e-02,  6.8944e-02,  3.7830e-02, -2.4740e-02],\n",
      "          [ 7.0242e-02,  1.9075e-02,  6.3640e-02,  8.9373e-03, -5.7801e-02],\n",
      "          [ 2.7979e-02,  3.9194e-02,  3.2605e-02, -2.8981e-02, -3.5625e-02]],\n",
      "\n",
      "         [[-5.9397e-02, -2.1496e-03,  1.5468e-02, -6.2288e-02, -3.8256e-02],\n",
      "          [-1.8390e-02, -7.6376e-02,  2.4745e-02,  3.9469e-02, -6.9971e-02],\n",
      "          [ 7.2524e-03, -6.5968e-02,  6.4458e-02,  2.0347e-02, -7.5062e-02],\n",
      "          [ 3.1460e-02,  3.9623e-02,  3.6722e-02, -4.6983e-02, -7.2592e-02],\n",
      "          [-4.3837e-02, -6.1999e-02, -5.7632e-02,  6.8693e-02, -3.2305e-02]],\n",
      "\n",
      "         [[-2.8614e-02,  5.4165e-02,  5.0756e-02,  6.9144e-02, -4.9899e-02],\n",
      "          [-6.7216e-02, -1.6195e-03,  3.3708e-02, -3.6388e-02,  3.6202e-02],\n",
      "          [-5.7379e-02,  2.6729e-02, -7.7838e-02,  7.9436e-02, -4.2379e-02],\n",
      "          [ 5.5811e-02, -3.5259e-02,  3.7408e-02, -1.2907e-02,  1.8189e-02],\n",
      "          [ 3.0437e-02, -6.9886e-03,  5.2431e-02,  6.3049e-03, -2.3631e-02]],\n",
      "\n",
      "         [[ 4.0924e-02, -7.3997e-02,  2.7901e-02,  6.8320e-02, -6.1273e-02],\n",
      "          [ 3.1350e-02,  2.5773e-02, -3.3783e-03, -7.1580e-02, -6.0727e-02],\n",
      "          [ 2.0889e-02,  2.6792e-02, -5.4259e-02,  2.2482e-02,  4.9166e-02],\n",
      "          [ 4.9748e-02,  1.4919e-02,  6.6411e-02,  4.9225e-02, -2.1554e-02],\n",
      "          [-7.2189e-02, -2.0777e-02,  1.6450e-02, -3.9105e-02,  1.4538e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6743e-02,  3.4131e-02,  2.1066e-02,  4.0410e-02, -2.7548e-02],\n",
      "          [ 5.2558e-03,  4.9311e-02, -2.4628e-02,  4.0448e-02, -6.8982e-02],\n",
      "          [ 2.0021e-02,  8.2109e-03,  7.9728e-02, -1.0305e-02, -3.5775e-02],\n",
      "          [ 3.5461e-02,  2.9476e-02,  1.9318e-02,  2.6978e-02,  5.3540e-02],\n",
      "          [ 5.1098e-02, -5.7656e-02,  7.1735e-02,  7.9443e-02,  2.4841e-02]],\n",
      "\n",
      "         [[-7.0850e-02,  6.5968e-02, -2.4352e-02, -2.2046e-02, -7.8899e-02],\n",
      "          [ 2.8554e-02,  1.7381e-02,  4.8977e-02, -4.5846e-02, -3.2750e-02],\n",
      "          [ 5.4887e-02, -4.3947e-02,  6.0068e-02,  2.3903e-02,  2.2371e-02],\n",
      "          [ 6.0202e-02,  7.8618e-02, -1.7703e-02, -2.7234e-02,  5.3150e-02],\n",
      "          [-9.5099e-04, -4.6488e-02,  3.9483e-02,  6.0064e-02, -5.2675e-02]],\n",
      "\n",
      "         [[-7.3979e-02, -5.5385e-02,  5.3292e-02, -7.8281e-02,  6.5778e-03],\n",
      "          [ 3.5745e-02, -5.6347e-02, -6.7832e-02, -5.5925e-02, -1.9277e-02],\n",
      "          [ 9.0501e-04, -6.4535e-02, -4.9705e-02,  7.0491e-02, -5.7106e-02],\n",
      "          [ 6.2751e-02,  6.2785e-02,  4.9499e-02,  4.0065e-02, -5.7766e-02],\n",
      "          [ 4.4693e-02,  1.6411e-02, -3.6589e-02,  7.4557e-02,  3.7577e-02]],\n",
      "\n",
      "         [[-4.9252e-02, -7.0346e-02, -7.4271e-02, -7.0772e-03,  1.8902e-02],\n",
      "          [-2.4419e-02,  7.4822e-03, -2.9224e-02, -2.2609e-02,  4.5759e-02],\n",
      "          [-4.4804e-02, -5.9155e-04,  6.4091e-02, -5.4713e-02, -1.4638e-02],\n",
      "          [-1.4561e-02, -8.0829e-02,  5.1451e-02, -2.4927e-02,  4.8702e-02],\n",
      "          [-6.5159e-02, -5.7827e-02, -4.8802e-02,  5.9124e-02, -1.5948e-02]],\n",
      "\n",
      "         [[-2.0845e-02, -7.1838e-03,  3.1099e-02, -6.7526e-02,  5.0298e-02],\n",
      "          [ 3.4251e-03,  1.0711e-02, -6.4928e-02, -3.7936e-04,  1.8905e-02],\n",
      "          [ 6.3420e-02, -3.4558e-02,  5.9088e-02,  6.7229e-02,  2.2013e-03],\n",
      "          [ 4.7606e-02,  6.5881e-02, -5.7919e-02, -5.6762e-02,  7.5130e-02],\n",
      "          [-7.0972e-02,  6.7972e-02,  4.4282e-02, -7.5328e-03,  4.4580e-02]],\n",
      "\n",
      "         [[-3.0319e-02, -5.8513e-02, -3.1988e-02, -4.6250e-02,  1.2130e-02],\n",
      "          [-3.6514e-02, -8.9267e-03, -5.1775e-02, -4.9038e-02, -2.0930e-02],\n",
      "          [-7.3782e-02, -3.2057e-02, -3.7374e-02, -2.9978e-03, -3.1119e-02],\n",
      "          [-6.5421e-02,  4.0092e-02,  5.8277e-02,  4.9602e-02, -2.4066e-03],\n",
      "          [ 5.0189e-02,  3.3565e-02,  4.2999e-02, -6.5735e-02,  1.1043e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.8425e-02, -1.7322e-02,  1.8027e-02,  4.9933e-02,  2.5685e-02],\n",
      "          [-5.5587e-02, -1.4061e-02,  1.6798e-02, -7.2977e-02, -4.2221e-02],\n",
      "          [ 5.7305e-02, -1.0812e-02, -2.2839e-02,  4.0995e-02, -1.8669e-02],\n",
      "          [ 3.5500e-02,  3.7778e-02, -2.4694e-02, -3.6143e-04,  4.2012e-03],\n",
      "          [ 3.3243e-02,  1.4074e-02, -3.5262e-02,  6.6216e-02,  7.7936e-02]],\n",
      "\n",
      "         [[-5.5881e-02,  3.4706e-02,  2.4263e-02, -4.6005e-02, -6.9284e-02],\n",
      "          [-3.6165e-02,  3.4576e-02, -1.7397e-04, -7.0132e-02,  3.2556e-02],\n",
      "          [ 6.5203e-02, -1.6458e-03, -4.6845e-02,  8.1302e-02, -5.7705e-04],\n",
      "          [-1.5924e-02, -5.7421e-02, -2.2076e-02,  3.5943e-03, -3.6013e-02],\n",
      "          [-3.4454e-02,  5.6580e-02,  3.2507e-03,  4.2400e-02,  7.6160e-02]],\n",
      "\n",
      "         [[ 3.4424e-02, -7.6686e-02,  6.4430e-02,  4.0731e-02,  6.9303e-02],\n",
      "          [-3.5538e-02,  2.2938e-02,  1.7326e-02, -6.9645e-02, -7.9125e-02],\n",
      "          [ 1.4922e-02,  3.9540e-02, -2.9546e-02,  4.3289e-03, -1.0530e-04],\n",
      "          [ 5.7020e-02,  7.8970e-02, -4.8311e-02,  7.2310e-02,  6.5342e-03],\n",
      "          [ 6.1235e-02,  5.6018e-02,  4.4926e-02,  4.9723e-02, -1.1467e-02]],\n",
      "\n",
      "         [[ 6.2350e-02,  5.0697e-02, -1.8188e-02, -1.2132e-02, -6.7491e-02],\n",
      "          [ 1.6730e-02,  2.8368e-02, -4.1176e-02,  4.9218e-03,  7.2544e-02],\n",
      "          [ 4.8831e-02, -3.5534e-02,  8.7943e-03,  5.6116e-02, -1.4003e-02],\n",
      "          [-5.1254e-02,  4.6084e-02,  4.7406e-02,  2.1659e-02, -5.1308e-02],\n",
      "          [ 6.1038e-02, -6.0626e-02, -5.9769e-02,  7.8620e-02,  1.5603e-02]],\n",
      "\n",
      "         [[-5.7179e-02, -7.3819e-02, -3.5123e-02,  2.9697e-02, -7.4636e-03],\n",
      "          [ 6.3779e-03,  7.5243e-02,  2.9089e-02,  2.0728e-02,  2.3634e-02],\n",
      "          [-7.8026e-02,  7.8077e-02,  1.5465e-02,  7.4513e-02,  2.2388e-02],\n",
      "          [-4.0703e-02, -8.4835e-03,  2.8273e-02, -1.7584e-02, -2.0338e-02],\n",
      "          [ 9.2179e-04,  6.6597e-02, -1.1693e-02, -5.0488e-02,  6.3643e-02]],\n",
      "\n",
      "         [[-2.5328e-02,  5.7686e-02,  7.4559e-02,  2.6184e-02,  7.4698e-02],\n",
      "          [ 5.1650e-02, -5.6559e-02, -1.4781e-02,  3.9813e-03,  9.4976e-03],\n",
      "          [-3.9508e-02, -1.1305e-02, -9.1878e-03, -6.1360e-02,  4.1735e-03],\n",
      "          [-4.3263e-02,  3.3020e-02,  7.6144e-02,  5.7865e-02, -1.2142e-02],\n",
      "          [ 4.7263e-02,  4.4166e-02, -4.6585e-02,  1.7673e-02,  4.8355e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.4925e-02,  5.1529e-02, -5.8380e-02, -6.0428e-02,  8.0597e-02],\n",
      "          [-3.7400e-02, -7.2428e-02,  6.9499e-02,  7.1352e-02, -7.9926e-02],\n",
      "          [-7.6701e-02, -3.5080e-02, -1.4856e-02, -2.7902e-02,  1.3109e-02],\n",
      "          [-5.1824e-02,  1.6248e-02, -1.7668e-02, -8.0504e-02,  5.3672e-03],\n",
      "          [ 8.1058e-02, -1.8121e-02, -1.0605e-02, -7.4729e-02, -6.2174e-03]],\n",
      "\n",
      "         [[ 6.8622e-02,  2.2131e-02,  1.8704e-02, -1.0690e-02,  6.7538e-02],\n",
      "          [ 5.0823e-02, -4.6456e-03, -8.1520e-02,  4.9497e-02, -4.6866e-02],\n",
      "          [-1.8151e-02,  4.8664e-02,  8.8457e-03, -3.6617e-05, -6.9287e-02],\n",
      "          [-7.1729e-02, -7.5581e-02, -5.0315e-02,  4.4237e-02, -1.2318e-03],\n",
      "          [-8.7208e-03, -2.7629e-02, -8.6071e-03,  4.7378e-02,  8.1415e-02]],\n",
      "\n",
      "         [[ 1.5848e-02,  5.4087e-02,  7.4865e-02, -3.4224e-02, -4.2138e-02],\n",
      "          [-3.2105e-02,  2.6194e-03, -4.4061e-02, -7.6657e-02, -7.7386e-02],\n",
      "          [-4.2233e-02,  5.2364e-02,  1.0649e-02, -1.0954e-02,  2.3784e-02],\n",
      "          [-1.3674e-02, -1.4971e-02,  7.6423e-02, -4.1464e-02,  5.5515e-02],\n",
      "          [-5.2007e-02, -5.5659e-02,  2.4992e-02,  4.8466e-02, -6.8549e-02]],\n",
      "\n",
      "         [[ 2.6402e-02, -1.1306e-02, -6.7618e-03, -6.5644e-02, -7.1810e-02],\n",
      "          [ 7.1807e-03,  2.2333e-03, -7.8193e-02,  2.5998e-02,  4.5954e-02],\n",
      "          [ 6.1300e-02, -1.8685e-02, -5.0732e-02,  1.3569e-02,  2.7509e-03],\n",
      "          [ 4.2976e-02,  3.8064e-02,  1.6470e-02, -6.8050e-03,  6.7901e-02],\n",
      "          [-6.2438e-02, -1.2171e-02, -3.1944e-02, -6.3962e-02,  7.8175e-02]],\n",
      "\n",
      "         [[ 5.1016e-02,  5.1434e-02,  2.2165e-04,  7.1546e-02, -2.4277e-02],\n",
      "          [ 3.6617e-02, -2.5331e-02, -7.6061e-02,  2.3633e-04, -7.3093e-04],\n",
      "          [ 5.4361e-02, -5.9729e-02, -1.4935e-02, -3.2475e-02,  3.4733e-02],\n",
      "          [ 4.5657e-02, -6.8171e-02, -2.4262e-02,  2.4992e-02, -2.3005e-02],\n",
      "          [-6.2020e-02, -6.1371e-02,  6.1467e-02, -4.8246e-02,  5.3561e-02]],\n",
      "\n",
      "         [[ 7.5216e-02, -7.9663e-02, -3.1834e-02, -1.8917e-03,  8.0217e-04],\n",
      "          [-1.1888e-02,  1.0577e-03, -6.3434e-02,  3.1467e-03,  5.1145e-02],\n",
      "          [-7.9796e-02, -6.2511e-02, -5.7114e-02,  4.7821e-02, -2.0615e-02],\n",
      "          [ 1.4485e-02,  8.1405e-02,  2.2250e-02,  1.8341e-02,  2.0236e-02],\n",
      "          [ 3.1045e-02,  3.1560e-02,  5.3557e-03, -7.3913e-02, -2.1461e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.1480e-02,  3.1922e-02, -3.4063e-02,  7.3896e-02,  2.4564e-02],\n",
      "          [-3.6244e-02, -9.2738e-03,  5.6793e-02,  2.7830e-02, -5.6918e-02],\n",
      "          [ 1.4718e-02, -3.0342e-02,  4.5372e-02,  3.3060e-02,  6.8759e-02],\n",
      "          [ 7.4455e-02,  5.2892e-02, -2.1532e-02,  2.3787e-02,  6.4545e-02],\n",
      "          [ 6.9936e-02,  5.1657e-02, -6.6322e-02, -3.0598e-02, -2.2956e-02]],\n",
      "\n",
      "         [[-3.6781e-02, -3.2393e-02, -5.7807e-02,  5.6143e-02,  4.1236e-02],\n",
      "          [ 2.1555e-02,  5.7271e-02, -2.6097e-02, -5.5528e-02,  4.4752e-02],\n",
      "          [ 1.0520e-03, -2.6065e-02, -2.9379e-02,  7.8752e-04,  1.3453e-03],\n",
      "          [-6.2457e-02, -2.9616e-02, -5.9288e-02, -2.0961e-02, -5.5899e-03],\n",
      "          [ 6.3807e-02, -8.6046e-03, -7.2129e-02, -7.2485e-02, -5.7287e-02]],\n",
      "\n",
      "         [[-6.2380e-02,  9.9219e-03, -5.8781e-02,  7.9166e-02,  3.4656e-02],\n",
      "          [ 1.4500e-03,  3.6630e-02,  1.8102e-02, -5.1997e-02,  7.5895e-02],\n",
      "          [-1.6989e-02, -3.7875e-02, -4.9662e-02, -3.6314e-02, -1.0293e-02],\n",
      "          [-5.0993e-03, -1.4484e-02,  1.4263e-02, -1.9507e-03,  6.4106e-02],\n",
      "          [ 4.4456e-02,  6.0514e-02, -4.7662e-02,  7.4312e-02,  4.0555e-02]],\n",
      "\n",
      "         [[-2.1919e-02,  2.8460e-02, -8.1581e-02, -4.7239e-02, -4.1585e-02],\n",
      "          [-1.0830e-02, -6.2282e-02,  2.6439e-03, -2.8399e-02,  6.1900e-02],\n",
      "          [ 1.8397e-02,  3.9046e-02, -2.7724e-02, -4.8139e-02,  8.6094e-03],\n",
      "          [ 4.7123e-02, -1.3091e-02, -2.5202e-02, -3.7516e-03,  7.6466e-03],\n",
      "          [ 4.4002e-02, -3.0552e-02,  2.4271e-03,  4.8461e-02, -4.3389e-02]],\n",
      "\n",
      "         [[ 6.8245e-02, -4.2333e-02, -1.0141e-02, -7.2712e-02, -4.2968e-02],\n",
      "          [-3.5378e-02, -1.6189e-02, -2.5724e-02, -4.5088e-02,  1.4009e-02],\n",
      "          [ 2.8905e-02, -1.8618e-02,  3.3446e-02, -3.1530e-04,  3.4522e-03],\n",
      "          [ 2.9983e-02,  2.9449e-02,  6.0968e-02, -5.1968e-02, -2.1394e-02],\n",
      "          [-2.7693e-02, -4.7082e-02,  4.0460e-02,  2.8630e-03, -3.3105e-02]],\n",
      "\n",
      "         [[ 7.0929e-02, -3.4843e-04, -7.1300e-02, -1.1582e-02,  2.5321e-02],\n",
      "          [ 4.8335e-03, -1.4714e-02, -3.3328e-02, -6.3829e-02,  7.7469e-02],\n",
      "          [-4.6180e-02,  4.7400e-03, -6.6577e-02, -7.9105e-02, -7.7189e-02],\n",
      "          [-2.5183e-03, -3.3781e-02, -3.0243e-03, -2.9307e-02,  2.3181e-02],\n",
      "          [-3.4886e-04, -4.3402e-02, -6.7192e-02, -1.6831e-02,  7.6771e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.1980e-02,  3.2684e-03, -7.8014e-02, -6.6181e-02,  2.0949e-02],\n",
      "          [-3.0573e-03, -6.7873e-02, -1.3036e-02,  3.4677e-02,  3.6331e-02],\n",
      "          [-2.1699e-02,  1.8261e-02, -3.2603e-02, -1.3427e-02, -3.4164e-02],\n",
      "          [ 2.9919e-02,  1.0457e-02,  6.5064e-02,  6.1223e-02, -3.8568e-02],\n",
      "          [ 1.4370e-02,  7.5879e-02,  4.9712e-03, -4.9525e-02, -6.5813e-02]],\n",
      "\n",
      "         [[-6.3607e-02,  6.7806e-02, -5.8001e-02,  4.8203e-02,  3.0021e-02],\n",
      "          [-2.9336e-02, -1.9212e-02, -1.4468e-02, -1.5830e-02,  4.8231e-02],\n",
      "          [ 1.0253e-02,  8.1601e-02,  5.3141e-02,  2.7593e-02,  7.2460e-02],\n",
      "          [-1.0322e-02,  7.6543e-02,  6.7847e-02,  1.5936e-02, -8.1499e-02],\n",
      "          [-2.5316e-03, -5.8420e-02,  3.0269e-02, -1.0494e-02, -5.5528e-02]],\n",
      "\n",
      "         [[-7.0269e-02,  3.5080e-02,  4.6674e-02, -4.5576e-02, -2.4322e-02],\n",
      "          [ 6.3157e-02,  3.9838e-02,  4.3689e-02,  3.6460e-02, -2.3604e-02],\n",
      "          [ 5.5019e-02,  2.6073e-02, -4.0228e-02, -8.0080e-02,  5.5649e-02],\n",
      "          [ 6.8398e-02,  3.3197e-02,  2.4542e-02,  3.8909e-02,  6.8769e-02],\n",
      "          [-3.9331e-02,  4.2106e-02,  6.3203e-02,  3.1087e-02, -7.9663e-02]],\n",
      "\n",
      "         [[ 7.7614e-02, -3.9344e-02,  2.7228e-02,  5.7668e-02,  7.0432e-02],\n",
      "          [-5.4258e-02,  7.5633e-02,  1.0488e-02,  2.8783e-03,  2.4701e-02],\n",
      "          [-7.3699e-02,  7.7770e-05, -5.3456e-02, -7.4217e-02,  7.1532e-02],\n",
      "          [-8.3828e-03,  6.8417e-02, -4.6186e-02, -9.7262e-03, -2.5364e-02],\n",
      "          [ 3.0709e-02,  3.5398e-03, -3.4313e-02,  4.4168e-03, -3.6179e-02]],\n",
      "\n",
      "         [[-4.2427e-02,  6.2600e-02,  6.0563e-03, -6.6217e-02, -7.4672e-02],\n",
      "          [ 5.2877e-02, -4.2644e-02,  3.1403e-02, -4.0351e-03,  9.9650e-03],\n",
      "          [ 2.0368e-02, -6.8014e-03, -7.3867e-02,  5.9829e-02,  2.0355e-02],\n",
      "          [-4.2165e-02,  3.1820e-02,  7.1168e-02,  3.9325e-02, -7.1322e-02],\n",
      "          [-6.5819e-02, -4.1931e-02,  6.6024e-02, -4.3073e-02,  7.6761e-02]],\n",
      "\n",
      "         [[-3.3215e-02, -7.3842e-02, -7.7222e-02, -7.2374e-02, -3.7809e-02],\n",
      "          [-7.6310e-02, -3.0388e-02,  2.8749e-02, -6.6100e-03,  4.5843e-02],\n",
      "          [ 7.0256e-02, -1.1714e-02, -6.1042e-02, -4.2743e-02, -1.5203e-02],\n",
      "          [ 1.0224e-02, -3.9434e-02, -6.1942e-02, -6.1688e-02,  4.5400e-02],\n",
      "          [ 1.1396e-02,  7.0803e-02, -1.9658e-02,  5.8692e-02,  1.7449e-02]]]],\n",
      "       requires_grad=True)), ('conv2.bias', Parameter containing:\n",
      "tensor([ 0.0389,  0.0382,  0.0268, -0.0410, -0.0243, -0.0733,  0.0401,  0.0166,\n",
      "        -0.0123, -0.0805,  0.0806, -0.0778, -0.0003,  0.0090, -0.0739, -0.0353],\n",
      "       requires_grad=True)), ('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.0428, -0.0256, -0.0406,  ..., -0.0099,  0.0401, -0.0088],\n",
      "        [ 0.0048,  0.0096, -0.0397,  ...,  0.0448, -0.0258, -0.0157],\n",
      "        [ 0.0363, -0.0220, -0.0113,  ...,  0.0148, -0.0141, -0.0221],\n",
      "        ...,\n",
      "        [ 0.0178,  0.0172,  0.0213,  ..., -0.0338, -0.0315,  0.0434],\n",
      "        [ 0.0184, -0.0237,  0.0201,  ..., -0.0445, -0.0097,  0.0102],\n",
      "        [ 0.0050,  0.0307, -0.0057,  ...,  0.0499, -0.0344,  0.0287]],\n",
      "       requires_grad=True)), ('fc1.bias', Parameter containing:\n",
      "tensor([-0.0138, -0.0075,  0.0135,  0.0421,  0.0182, -0.0071, -0.0085, -0.0026,\n",
      "         0.0188, -0.0231,  0.0227,  0.0366,  0.0299,  0.0238,  0.0442,  0.0277,\n",
      "        -0.0275, -0.0295,  0.0144,  0.0109, -0.0214, -0.0009, -0.0102,  0.0178,\n",
      "         0.0024, -0.0062, -0.0478, -0.0250, -0.0066, -0.0395,  0.0388,  0.0140,\n",
      "        -0.0458, -0.0110, -0.0322, -0.0005,  0.0124,  0.0098, -0.0004,  0.0442,\n",
      "         0.0163,  0.0190,  0.0211,  0.0088,  0.0382,  0.0008,  0.0303, -0.0314,\n",
      "        -0.0484,  0.0320, -0.0400,  0.0492,  0.0088, -0.0390,  0.0468,  0.0196,\n",
      "         0.0434, -0.0042, -0.0278,  0.0098,  0.0252, -0.0175, -0.0178, -0.0051,\n",
      "         0.0047, -0.0033,  0.0008,  0.0170, -0.0450, -0.0146, -0.0289, -0.0430,\n",
      "         0.0269,  0.0244, -0.0155, -0.0297, -0.0323, -0.0488,  0.0290,  0.0101,\n",
      "         0.0358, -0.0014, -0.0460, -0.0007,  0.0146,  0.0461, -0.0147, -0.0300,\n",
      "        -0.0357,  0.0251,  0.0186,  0.0292, -0.0240, -0.0453, -0.0106, -0.0271,\n",
      "        -0.0224,  0.0364, -0.0417,  0.0457, -0.0025,  0.0072,  0.0034, -0.0307,\n",
      "         0.0416, -0.0352, -0.0392,  0.0123,  0.0046, -0.0488, -0.0391,  0.0085,\n",
      "         0.0045,  0.0346,  0.0184,  0.0245,  0.0104, -0.0395,  0.0036, -0.0296],\n",
      "       requires_grad=True)), ('fc2.weight', Parameter containing:\n",
      "tensor([[-0.0462,  0.0623,  0.0697,  ..., -0.0539,  0.0336,  0.0437],\n",
      "        [ 0.0692, -0.0430,  0.0638,  ..., -0.0316, -0.0199,  0.0238],\n",
      "        [-0.0014,  0.0865, -0.0305,  ..., -0.0041, -0.0867,  0.0660],\n",
      "        ...,\n",
      "        [-0.0285, -0.0172, -0.0326,  ...,  0.0701, -0.0149,  0.0715],\n",
      "        [ 0.0434, -0.0307,  0.0454,  ..., -0.0538,  0.0497,  0.0523],\n",
      "        [-0.0667, -0.0412, -0.0824,  ..., -0.0503,  0.0856, -0.0779]],\n",
      "       requires_grad=True)), ('fc2.bias', Parameter containing:\n",
      "tensor([-0.0861,  0.0431,  0.0645, -0.0305, -0.0254, -0.0231,  0.0622,  0.0359,\n",
      "         0.0588, -0.0052,  0.0346,  0.0821, -0.0822,  0.0623,  0.0883, -0.0331,\n",
      "         0.0062,  0.0288,  0.0019,  0.0100, -0.0587,  0.0368, -0.0315, -0.0461,\n",
      "         0.0704, -0.0101,  0.0739, -0.0533, -0.0400, -0.0199, -0.0666,  0.0832,\n",
      "         0.0159, -0.0888,  0.0239, -0.0906,  0.0268, -0.0058,  0.0307, -0.0877,\n",
      "         0.0212, -0.0281,  0.0358,  0.0711,  0.0391,  0.0813,  0.0563, -0.0413,\n",
      "         0.0904, -0.0702,  0.0805,  0.0840,  0.0566, -0.0575,  0.0816, -0.0297,\n",
      "         0.0353,  0.0907,  0.0792,  0.0301, -0.0819, -0.0662,  0.0751, -0.0620,\n",
      "         0.0011, -0.0268, -0.0004, -0.0651,  0.0531, -0.0414,  0.0309, -0.0730,\n",
      "        -0.0442, -0.0491,  0.0753, -0.0019,  0.0701,  0.0826, -0.0162,  0.0163,\n",
      "        -0.0871,  0.0512, -0.0390,  0.0723], requires_grad=True)), ('fc3.weight', Parameter containing:\n",
      "tensor([[-0.0310, -0.0083, -0.0524,  0.0438, -0.0971,  0.1068, -0.0303,  0.0443,\n",
      "          0.0280,  0.0217,  0.0217,  0.0353,  0.0266, -0.0256,  0.0974,  0.0465,\n",
      "          0.0126,  0.0099, -0.0022,  0.0963, -0.0413, -0.1039, -0.0600, -0.0213,\n",
      "          0.0394,  0.0922,  0.0974, -0.0423, -0.0134,  0.0914, -0.0401,  0.0544,\n",
      "          0.0792, -0.1002,  0.0473, -0.1024, -0.0523, -0.0146, -0.0780, -0.0140,\n",
      "          0.0999, -0.0684, -0.0726,  0.0150,  0.0149,  0.0904,  0.0240,  0.0066,\n",
      "         -0.0646, -0.0082, -0.0572, -0.0099,  0.0615,  0.0903, -0.0261,  0.0880,\n",
      "         -0.0629,  0.0182,  0.0430,  0.0986, -0.0881,  0.0116, -0.0492,  0.0272,\n",
      "         -0.0227,  0.0472,  0.0005,  0.1042, -0.0579, -0.0854, -0.0963,  0.0591,\n",
      "          0.0739,  0.0685,  0.0038, -0.0570, -0.0415,  0.0919,  0.0609,  0.0119,\n",
      "         -0.0983,  0.0701,  0.0818,  0.0867],\n",
      "        [ 0.0274,  0.0075,  0.0101,  0.0858,  0.0384, -0.0875,  0.0253, -0.0610,\n",
      "         -0.0726, -0.0509,  0.0451,  0.0629,  0.0966, -0.0113,  0.0627,  0.0991,\n",
      "         -0.0758,  0.0778, -0.0160,  0.0874, -0.0998,  0.0686,  0.0319,  0.0462,\n",
      "          0.0675,  0.0879,  0.0835, -0.0956,  0.0103, -0.0344,  0.0263,  0.0004,\n",
      "         -0.0373,  0.0380, -0.0136,  0.0181, -0.0577,  0.0949, -0.0868,  0.0366,\n",
      "         -0.0450, -0.0634, -0.0751,  0.0760,  0.0890,  0.0487, -0.0215, -0.1023,\n",
      "         -0.0382,  0.0138,  0.1009, -0.0109, -0.1039, -0.0153,  0.0929,  0.0973,\n",
      "         -0.0871, -0.0372, -0.0474,  0.0363, -0.0397, -0.0307, -0.1071, -0.0288,\n",
      "          0.0636, -0.0757, -0.0466, -0.0776, -0.0109, -0.1055,  0.0543, -0.0070,\n",
      "         -0.0177,  0.0300,  0.0327, -0.0514, -0.0835, -0.0018,  0.0458,  0.1073,\n",
      "          0.0895,  0.0431,  0.0315,  0.0729],\n",
      "        [-0.0318, -0.0677,  0.0628,  0.0013,  0.0392, -0.0389, -0.0199, -0.0713,\n",
      "         -0.0036, -0.0597, -0.0774,  0.0855,  0.0779,  0.0686,  0.0830, -0.0272,\n",
      "         -0.0303, -0.0391,  0.0691,  0.0848, -0.0663, -0.0060, -0.0905, -0.0485,\n",
      "          0.0745, -0.0788,  0.0854,  0.0969,  0.0996, -0.0262, -0.0271, -0.0562,\n",
      "          0.0952,  0.0327, -0.0617, -0.0528,  0.0421, -0.0120,  0.0464, -0.0021,\n",
      "         -0.0455, -0.0203, -0.0706,  0.1045,  0.0712,  0.0315,  0.1081, -0.0730,\n",
      "          0.0871,  0.0944,  0.0760, -0.0338,  0.0613, -0.0029,  0.1017,  0.0835,\n",
      "          0.0934, -0.0917,  0.0396,  0.0482,  0.0469,  0.0253, -0.0948, -0.1060,\n",
      "         -0.0502,  0.0561,  0.0611,  0.0486,  0.0370,  0.0927, -0.0190, -0.0695,\n",
      "          0.1023, -0.0623,  0.0921,  0.0428, -0.0067,  0.0113, -0.0986, -0.0096,\n",
      "         -0.0404,  0.0502, -0.1054,  0.0397],\n",
      "        [ 0.0027,  0.0059,  0.0595, -0.0132, -0.0794, -0.1049,  0.0190,  0.0717,\n",
      "         -0.0266, -0.0521,  0.0931, -0.0811, -0.0711,  0.0803, -0.0435,  0.1088,\n",
      "          0.0028, -0.0823, -0.0302, -0.0991, -0.0394, -0.0600,  0.0358, -0.0858,\n",
      "          0.0299, -0.0351, -0.0947,  0.0588,  0.0375,  0.1047,  0.0008,  0.0661,\n",
      "          0.0416, -0.0868,  0.0325,  0.1029,  0.0314, -0.0040, -0.0887, -0.0796,\n",
      "         -0.0444, -0.0604, -0.0818,  0.0847,  0.0318,  0.0764, -0.0799,  0.0385,\n",
      "         -0.0751,  0.1048, -0.0942, -0.0753, -0.0456,  0.0688,  0.0745,  0.0258,\n",
      "         -0.0821,  0.0345, -0.0272,  0.0631, -0.0336, -0.0694,  0.0671,  0.0793,\n",
      "          0.0297, -0.0650,  0.0233, -0.0030, -0.0663,  0.0900, -0.0428, -0.0692,\n",
      "          0.0499, -0.0459, -0.0465,  0.0621, -0.0937, -0.1048,  0.0449,  0.0545,\n",
      "         -0.0663, -0.0807, -0.0556,  0.0964],\n",
      "        [-0.0660, -0.0715,  0.0913, -0.0550, -0.0838, -0.0164,  0.0157,  0.1032,\n",
      "         -0.1085,  0.0995, -0.0669,  0.1086,  0.0545,  0.0722, -0.0905, -0.0269,\n",
      "         -0.0183, -0.0589, -0.0429, -0.0160, -0.0709,  0.0474,  0.0003, -0.0710,\n",
      "         -0.1017,  0.0431, -0.0576,  0.0189, -0.0702, -0.0515,  0.0968, -0.0216,\n",
      "          0.0486, -0.0875, -0.0904,  0.0327, -0.0042,  0.0057, -0.0438, -0.0892,\n",
      "          0.1026,  0.0726, -0.0492,  0.0756, -0.0803, -0.0959, -0.0816, -0.0096,\n",
      "          0.0250, -0.0184, -0.0715,  0.0612, -0.0074, -0.0758, -0.0673,  0.0976,\n",
      "          0.0568,  0.0542, -0.0691,  0.0826, -0.0873, -0.0912, -0.0444,  0.0948,\n",
      "         -0.0275,  0.1068,  0.0387,  0.0897,  0.0783,  0.0805, -0.0534,  0.0025,\n",
      "          0.0288,  0.0569, -0.0580,  0.0826,  0.0701, -0.0208, -0.0712, -0.0461,\n",
      "          0.0199, -0.0791,  0.0589,  0.0671],\n",
      "        [ 0.0759,  0.0558, -0.0709,  0.0823, -0.0425,  0.0988, -0.0937, -0.0026,\n",
      "          0.1069, -0.0333, -0.0775, -0.0813,  0.1089,  0.0437, -0.0060, -0.0797,\n",
      "          0.0227,  0.0592,  0.0696, -0.0468,  0.0660, -0.1068, -0.0780,  0.0814,\n",
      "         -0.0906, -0.0514,  0.0462, -0.0655, -0.0061,  0.0030,  0.0706,  0.0822,\n",
      "         -0.0958,  0.0834,  0.0259, -0.0666, -0.1074,  0.0331, -0.0463, -0.0161,\n",
      "         -0.0093, -0.0027,  0.0482, -0.0576,  0.0343,  0.0106, -0.0768, -0.0043,\n",
      "         -0.0320, -0.0107, -0.0257,  0.1042, -0.0395, -0.0671,  0.0732, -0.0709,\n",
      "         -0.0107,  0.0207,  0.0855, -0.0032, -0.0767,  0.0066, -0.0780, -0.0508,\n",
      "          0.0288,  0.1087, -0.0642,  0.0917, -0.0319, -0.0111,  0.0208,  0.0840,\n",
      "         -0.0003,  0.0565,  0.0754,  0.0066, -0.0455,  0.0605,  0.0663, -0.0568,\n",
      "          0.0280, -0.0031,  0.0601,  0.1033],\n",
      "        [-0.0724,  0.0791, -0.1044,  0.0211,  0.0589,  0.0664, -0.1033,  0.0249,\n",
      "         -0.0922,  0.0932,  0.0922,  0.0190, -0.0419, -0.0833,  0.0570,  0.0270,\n",
      "         -0.0972, -0.0539, -0.0255, -0.0789,  0.0707, -0.0919,  0.0796,  0.0763,\n",
      "          0.1067,  0.0543, -0.0717,  0.0203, -0.0927,  0.0061,  0.0895,  0.0861,\n",
      "         -0.0986,  0.0208,  0.0432, -0.0890,  0.0566,  0.0221, -0.0122,  0.0985,\n",
      "          0.0176, -0.0146,  0.0211,  0.0386, -0.0122,  0.0116, -0.0362,  0.0108,\n",
      "         -0.1029, -0.0823, -0.0743, -0.0164,  0.0471, -0.0433,  0.0243,  0.0381,\n",
      "         -0.1001,  0.0646,  0.0040, -0.1090,  0.0184,  0.0695,  0.0280,  0.0492,\n",
      "         -0.0967,  0.0630, -0.0793,  0.0215, -0.0432, -0.0847,  0.0250,  0.0923,\n",
      "         -0.0721,  0.0570, -0.0513,  0.0680,  0.0852, -0.0183, -0.0486,  0.1050,\n",
      "          0.0357, -0.0277, -0.0102,  0.0788],\n",
      "        [-0.0570,  0.0462,  0.0120,  0.0238, -0.0880,  0.0752,  0.0013, -0.0551,\n",
      "          0.0027, -0.0999,  0.0359, -0.0488,  0.0639, -0.0548, -0.0781,  0.0635,\n",
      "          0.0845,  0.0416, -0.0282, -0.0453, -0.0612,  0.0286,  0.0051, -0.0117,\n",
      "         -0.0862,  0.0477,  0.0118,  0.0915, -0.0622, -0.0884,  0.0975,  0.0780,\n",
      "          0.0011,  0.0882,  0.0450, -0.1034,  0.0991,  0.0441, -0.0806, -0.0692,\n",
      "          0.0491, -0.1083,  0.1043,  0.0020,  0.0397,  0.0930,  0.0848,  0.0069,\n",
      "         -0.1022,  0.0035, -0.0821, -0.0272,  0.0818, -0.0689,  0.1073,  0.0227,\n",
      "          0.0505, -0.0762, -0.0075, -0.0135, -0.0177,  0.0640, -0.0662,  0.1028,\n",
      "          0.0286, -0.0929,  0.0681, -0.0209,  0.0581,  0.0690, -0.0433,  0.0066,\n",
      "          0.0604, -0.0959, -0.0387, -0.0789,  0.0540, -0.1056,  0.0663,  0.0115,\n",
      "          0.0100, -0.0348,  0.0240, -0.0015],\n",
      "        [-0.0128,  0.0534,  0.0056,  0.0138,  0.0817,  0.0543, -0.0195,  0.0101,\n",
      "          0.0415, -0.0834,  0.0108,  0.0782, -0.0956, -0.0073, -0.0344,  0.1072,\n",
      "         -0.0293, -0.0333,  0.0575,  0.0311, -0.0537,  0.0201, -0.0548,  0.1006,\n",
      "          0.0503, -0.0518,  0.0818,  0.0557,  0.1047,  0.0569, -0.0980, -0.0246,\n",
      "         -0.0749, -0.0015, -0.1041,  0.0767, -0.0755, -0.0863,  0.0240,  0.0164,\n",
      "         -0.0414,  0.0911,  0.1071,  0.0004, -0.0769,  0.0819, -0.1081, -0.0113,\n",
      "          0.0099,  0.0160,  0.0010, -0.0151, -0.1040,  0.0634, -0.0965,  0.0549,\n",
      "         -0.0275,  0.0399, -0.1065,  0.0843,  0.0576, -0.0074,  0.0508,  0.1053,\n",
      "         -0.0431,  0.1027, -0.0341, -0.0030, -0.0835, -0.0372,  0.0353,  0.0998,\n",
      "         -0.0497,  0.0067,  0.0291, -0.0438, -0.0040, -0.0895,  0.0713,  0.0364,\n",
      "          0.0314, -0.0876, -0.0426,  0.0958],\n",
      "        [-0.0414,  0.1070,  0.0070,  0.0564,  0.0233, -0.0899, -0.1071, -0.0586,\n",
      "         -0.0523, -0.0124, -0.0145, -0.0168,  0.0084, -0.0841, -0.0348,  0.0413,\n",
      "         -0.1039, -0.0767,  0.0827, -0.0985,  0.0365, -0.1025, -0.0299,  0.0177,\n",
      "          0.0340, -0.0123, -0.0800, -0.0853, -0.0931,  0.0333,  0.0579,  0.0040,\n",
      "          0.0948, -0.0673,  0.0968,  0.0796,  0.0805, -0.1065,  0.0805, -0.0523,\n",
      "          0.0993,  0.0916, -0.0868, -0.0894,  0.0020,  0.0472,  0.0232,  0.0558,\n",
      "          0.0310,  0.0482,  0.0200,  0.0564,  0.0454,  0.0916, -0.0801,  0.1030,\n",
      "          0.0556, -0.0403, -0.0760, -0.0883, -0.0936,  0.0222,  0.0318,  0.0983,\n",
      "          0.0917, -0.0400, -0.0418, -0.0274, -0.0861, -0.0251, -0.0105, -0.0790,\n",
      "          0.0468,  0.0846, -0.0766, -0.0938, -0.0300, -0.0937,  0.0875, -0.0856,\n",
      "          0.0429,  0.0815,  0.0178,  0.0909]], requires_grad=True)), ('fc3.bias', Parameter containing:\n",
      "tensor([-0.0139,  0.0040, -0.0021,  0.0511, -0.0593, -0.0125,  0.0865,  0.0023,\n",
      "         0.0479, -0.1086], requires_grad=True))]\n",
      "[Parameter containing:\n",
      "tensor([[[[ 0.0166,  0.0098,  0.1171,  0.1481,  0.1435],\n",
      "          [-0.0837, -0.1061, -0.1612, -0.0802,  0.1040],\n",
      "          [-0.1693,  0.1245,  0.0891,  0.0770,  0.0175],\n",
      "          [-0.1351, -0.0190,  0.0285, -0.0528, -0.0789],\n",
      "          [ 0.1020, -0.0441,  0.1122, -0.1009, -0.1253]]],\n",
      "\n",
      "\n",
      "        [[[-0.1314, -0.0923,  0.1745,  0.1935, -0.1820],\n",
      "          [ 0.0216,  0.0072, -0.1638, -0.0734, -0.1431],\n",
      "          [-0.1728,  0.0815,  0.0791, -0.0109,  0.1700],\n",
      "          [-0.1798, -0.0362, -0.1030, -0.0137, -0.1918],\n",
      "          [-0.1276, -0.1142,  0.1038, -0.0566, -0.1313]]],\n",
      "\n",
      "\n",
      "        [[[-0.0615,  0.1726,  0.0978,  0.1373, -0.1087],\n",
      "          [-0.1577, -0.1407, -0.0050,  0.0577,  0.0962],\n",
      "          [-0.0598,  0.1034,  0.0396,  0.0688,  0.1844],\n",
      "          [ 0.0153, -0.0077, -0.1524, -0.0032, -0.1133],\n",
      "          [ 0.1369,  0.0074,  0.0867, -0.0545, -0.1241]]],\n",
      "\n",
      "\n",
      "        [[[-0.1310, -0.1092,  0.0953, -0.0348,  0.1947],\n",
      "          [-0.1586,  0.0826,  0.0830,  0.1152, -0.1587],\n",
      "          [ 0.1994, -0.1105, -0.0845,  0.0685, -0.0398],\n",
      "          [-0.0768,  0.1976,  0.1924, -0.0334,  0.0399],\n",
      "          [-0.0583, -0.0803,  0.1791, -0.1149,  0.1602]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1591,  0.1854, -0.0155,  0.1987,  0.0913],\n",
      "          [ 0.1879, -0.1953,  0.0897, -0.0032, -0.1249],\n",
      "          [ 0.1666,  0.1687, -0.0045,  0.0534,  0.1800],\n",
      "          [-0.0392, -0.1159, -0.1660,  0.1075,  0.1616],\n",
      "          [-0.0373, -0.1272, -0.1320, -0.1298,  0.0431]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0627, -0.0311, -0.0342, -0.1873, -0.0984],\n",
      "          [-0.1057,  0.0199,  0.0245,  0.1285,  0.1586],\n",
      "          [ 0.1751, -0.1868,  0.1746, -0.0748, -0.0514],\n",
      "          [ 0.0819, -0.0570,  0.0984,  0.1110, -0.0305],\n",
      "          [-0.1121,  0.1022,  0.0252, -0.1886,  0.0298]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0968, -0.1809,  0.1792,  0.1625,  0.0466,  0.0539],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[-4.9784e-02, -8.8020e-03,  3.3806e-02, -6.1664e-02, -2.0385e-02],\n",
      "          [ 2.0219e-03, -6.1226e-02,  2.8729e-02, -1.4043e-02,  1.3657e-02],\n",
      "          [-6.6999e-02, -1.7677e-02, -3.1090e-03,  4.1746e-02, -2.6710e-02],\n",
      "          [-7.2222e-02,  1.6029e-02,  3.2397e-02,  1.0473e-02,  4.0676e-05],\n",
      "          [ 2.9545e-02,  6.7674e-02,  3.1869e-02, -7.5961e-02,  4.1157e-02]],\n",
      "\n",
      "         [[ 3.3769e-03,  7.6592e-03,  1.0738e-02,  7.5978e-02,  7.8580e-02],\n",
      "          [-5.0427e-02, -2.3193e-03,  7.1685e-03,  4.6865e-03,  7.0581e-02],\n",
      "          [-7.3436e-03, -6.2599e-02,  1.3730e-02,  7.0737e-02,  7.6586e-02],\n",
      "          [-3.3572e-02, -2.4697e-02, -7.4393e-02, -6.9333e-02, -1.8677e-02],\n",
      "          [ 1.6996e-02,  2.2629e-02, -4.0732e-03,  5.9101e-02, -5.6704e-02]],\n",
      "\n",
      "         [[-7.2141e-03, -4.0125e-02,  3.2590e-02,  4.7092e-02,  6.0686e-02],\n",
      "          [ 2.4660e-02,  5.2729e-02,  7.5102e-02, -7.6954e-02, -7.9430e-02],\n",
      "          [ 3.0718e-03, -2.4209e-02,  6.8944e-02,  3.7830e-02, -2.4740e-02],\n",
      "          [ 7.0242e-02,  1.9075e-02,  6.3640e-02,  8.9373e-03, -5.7801e-02],\n",
      "          [ 2.7979e-02,  3.9194e-02,  3.2605e-02, -2.8981e-02, -3.5625e-02]],\n",
      "\n",
      "         [[-5.9397e-02, -2.1496e-03,  1.5468e-02, -6.2288e-02, -3.8256e-02],\n",
      "          [-1.8390e-02, -7.6376e-02,  2.4745e-02,  3.9469e-02, -6.9971e-02],\n",
      "          [ 7.2524e-03, -6.5968e-02,  6.4458e-02,  2.0347e-02, -7.5062e-02],\n",
      "          [ 3.1460e-02,  3.9623e-02,  3.6722e-02, -4.6983e-02, -7.2592e-02],\n",
      "          [-4.3837e-02, -6.1999e-02, -5.7632e-02,  6.8693e-02, -3.2305e-02]],\n",
      "\n",
      "         [[-2.8614e-02,  5.4165e-02,  5.0756e-02,  6.9144e-02, -4.9899e-02],\n",
      "          [-6.7216e-02, -1.6195e-03,  3.3708e-02, -3.6388e-02,  3.6202e-02],\n",
      "          [-5.7379e-02,  2.6729e-02, -7.7838e-02,  7.9436e-02, -4.2379e-02],\n",
      "          [ 5.5811e-02, -3.5259e-02,  3.7408e-02, -1.2907e-02,  1.8189e-02],\n",
      "          [ 3.0437e-02, -6.9886e-03,  5.2431e-02,  6.3049e-03, -2.3631e-02]],\n",
      "\n",
      "         [[ 4.0924e-02, -7.3997e-02,  2.7901e-02,  6.8320e-02, -6.1273e-02],\n",
      "          [ 3.1350e-02,  2.5773e-02, -3.3783e-03, -7.1580e-02, -6.0727e-02],\n",
      "          [ 2.0889e-02,  2.6792e-02, -5.4259e-02,  2.2482e-02,  4.9166e-02],\n",
      "          [ 4.9748e-02,  1.4919e-02,  6.6411e-02,  4.9225e-02, -2.1554e-02],\n",
      "          [-7.2189e-02, -2.0777e-02,  1.6450e-02, -3.9105e-02,  1.4538e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6743e-02,  3.4131e-02,  2.1066e-02,  4.0410e-02, -2.7548e-02],\n",
      "          [ 5.2558e-03,  4.9311e-02, -2.4628e-02,  4.0448e-02, -6.8982e-02],\n",
      "          [ 2.0021e-02,  8.2109e-03,  7.9728e-02, -1.0305e-02, -3.5775e-02],\n",
      "          [ 3.5461e-02,  2.9476e-02,  1.9318e-02,  2.6978e-02,  5.3540e-02],\n",
      "          [ 5.1098e-02, -5.7656e-02,  7.1735e-02,  7.9443e-02,  2.4841e-02]],\n",
      "\n",
      "         [[-7.0850e-02,  6.5968e-02, -2.4352e-02, -2.2046e-02, -7.8899e-02],\n",
      "          [ 2.8554e-02,  1.7381e-02,  4.8977e-02, -4.5846e-02, -3.2750e-02],\n",
      "          [ 5.4887e-02, -4.3947e-02,  6.0068e-02,  2.3903e-02,  2.2371e-02],\n",
      "          [ 6.0202e-02,  7.8618e-02, -1.7703e-02, -2.7234e-02,  5.3150e-02],\n",
      "          [-9.5099e-04, -4.6488e-02,  3.9483e-02,  6.0064e-02, -5.2675e-02]],\n",
      "\n",
      "         [[-7.3979e-02, -5.5385e-02,  5.3292e-02, -7.8281e-02,  6.5778e-03],\n",
      "          [ 3.5745e-02, -5.6347e-02, -6.7832e-02, -5.5925e-02, -1.9277e-02],\n",
      "          [ 9.0501e-04, -6.4535e-02, -4.9705e-02,  7.0491e-02, -5.7106e-02],\n",
      "          [ 6.2751e-02,  6.2785e-02,  4.9499e-02,  4.0065e-02, -5.7766e-02],\n",
      "          [ 4.4693e-02,  1.6411e-02, -3.6589e-02,  7.4557e-02,  3.7577e-02]],\n",
      "\n",
      "         [[-4.9252e-02, -7.0346e-02, -7.4271e-02, -7.0772e-03,  1.8902e-02],\n",
      "          [-2.4419e-02,  7.4822e-03, -2.9224e-02, -2.2609e-02,  4.5759e-02],\n",
      "          [-4.4804e-02, -5.9155e-04,  6.4091e-02, -5.4713e-02, -1.4638e-02],\n",
      "          [-1.4561e-02, -8.0829e-02,  5.1451e-02, -2.4927e-02,  4.8702e-02],\n",
      "          [-6.5159e-02, -5.7827e-02, -4.8802e-02,  5.9124e-02, -1.5948e-02]],\n",
      "\n",
      "         [[-2.0845e-02, -7.1838e-03,  3.1099e-02, -6.7526e-02,  5.0298e-02],\n",
      "          [ 3.4251e-03,  1.0711e-02, -6.4928e-02, -3.7936e-04,  1.8905e-02],\n",
      "          [ 6.3420e-02, -3.4558e-02,  5.9088e-02,  6.7229e-02,  2.2013e-03],\n",
      "          [ 4.7606e-02,  6.5881e-02, -5.7919e-02, -5.6762e-02,  7.5130e-02],\n",
      "          [-7.0972e-02,  6.7972e-02,  4.4282e-02, -7.5328e-03,  4.4580e-02]],\n",
      "\n",
      "         [[-3.0319e-02, -5.8513e-02, -3.1988e-02, -4.6250e-02,  1.2130e-02],\n",
      "          [-3.6514e-02, -8.9267e-03, -5.1775e-02, -4.9038e-02, -2.0930e-02],\n",
      "          [-7.3782e-02, -3.2057e-02, -3.7374e-02, -2.9978e-03, -3.1119e-02],\n",
      "          [-6.5421e-02,  4.0092e-02,  5.8277e-02,  4.9602e-02, -2.4066e-03],\n",
      "          [ 5.0189e-02,  3.3565e-02,  4.2999e-02, -6.5735e-02,  1.1043e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.8425e-02, -1.7322e-02,  1.8027e-02,  4.9933e-02,  2.5685e-02],\n",
      "          [-5.5587e-02, -1.4061e-02,  1.6798e-02, -7.2977e-02, -4.2221e-02],\n",
      "          [ 5.7305e-02, -1.0812e-02, -2.2839e-02,  4.0995e-02, -1.8669e-02],\n",
      "          [ 3.5500e-02,  3.7778e-02, -2.4694e-02, -3.6143e-04,  4.2012e-03],\n",
      "          [ 3.3243e-02,  1.4074e-02, -3.5262e-02,  6.6216e-02,  7.7936e-02]],\n",
      "\n",
      "         [[-5.5881e-02,  3.4706e-02,  2.4263e-02, -4.6005e-02, -6.9284e-02],\n",
      "          [-3.6165e-02,  3.4576e-02, -1.7397e-04, -7.0132e-02,  3.2556e-02],\n",
      "          [ 6.5203e-02, -1.6458e-03, -4.6845e-02,  8.1302e-02, -5.7705e-04],\n",
      "          [-1.5924e-02, -5.7421e-02, -2.2076e-02,  3.5943e-03, -3.6013e-02],\n",
      "          [-3.4454e-02,  5.6580e-02,  3.2507e-03,  4.2400e-02,  7.6160e-02]],\n",
      "\n",
      "         [[ 3.4424e-02, -7.6686e-02,  6.4430e-02,  4.0731e-02,  6.9303e-02],\n",
      "          [-3.5538e-02,  2.2938e-02,  1.7326e-02, -6.9645e-02, -7.9125e-02],\n",
      "          [ 1.4922e-02,  3.9540e-02, -2.9546e-02,  4.3289e-03, -1.0530e-04],\n",
      "          [ 5.7020e-02,  7.8970e-02, -4.8311e-02,  7.2310e-02,  6.5342e-03],\n",
      "          [ 6.1235e-02,  5.6018e-02,  4.4926e-02,  4.9723e-02, -1.1467e-02]],\n",
      "\n",
      "         [[ 6.2350e-02,  5.0697e-02, -1.8188e-02, -1.2132e-02, -6.7491e-02],\n",
      "          [ 1.6730e-02,  2.8368e-02, -4.1176e-02,  4.9218e-03,  7.2544e-02],\n",
      "          [ 4.8831e-02, -3.5534e-02,  8.7943e-03,  5.6116e-02, -1.4003e-02],\n",
      "          [-5.1254e-02,  4.6084e-02,  4.7406e-02,  2.1659e-02, -5.1308e-02],\n",
      "          [ 6.1038e-02, -6.0626e-02, -5.9769e-02,  7.8620e-02,  1.5603e-02]],\n",
      "\n",
      "         [[-5.7179e-02, -7.3819e-02, -3.5123e-02,  2.9697e-02, -7.4636e-03],\n",
      "          [ 6.3779e-03,  7.5243e-02,  2.9089e-02,  2.0728e-02,  2.3634e-02],\n",
      "          [-7.8026e-02,  7.8077e-02,  1.5465e-02,  7.4513e-02,  2.2388e-02],\n",
      "          [-4.0703e-02, -8.4835e-03,  2.8273e-02, -1.7584e-02, -2.0338e-02],\n",
      "          [ 9.2179e-04,  6.6597e-02, -1.1693e-02, -5.0488e-02,  6.3643e-02]],\n",
      "\n",
      "         [[-2.5328e-02,  5.7686e-02,  7.4559e-02,  2.6184e-02,  7.4698e-02],\n",
      "          [ 5.1650e-02, -5.6559e-02, -1.4781e-02,  3.9813e-03,  9.4976e-03],\n",
      "          [-3.9508e-02, -1.1305e-02, -9.1878e-03, -6.1360e-02,  4.1735e-03],\n",
      "          [-4.3263e-02,  3.3020e-02,  7.6144e-02,  5.7865e-02, -1.2142e-02],\n",
      "          [ 4.7263e-02,  4.4166e-02, -4.6585e-02,  1.7673e-02,  4.8355e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.4925e-02,  5.1529e-02, -5.8380e-02, -6.0428e-02,  8.0597e-02],\n",
      "          [-3.7400e-02, -7.2428e-02,  6.9499e-02,  7.1352e-02, -7.9926e-02],\n",
      "          [-7.6701e-02, -3.5080e-02, -1.4856e-02, -2.7902e-02,  1.3109e-02],\n",
      "          [-5.1824e-02,  1.6248e-02, -1.7668e-02, -8.0504e-02,  5.3672e-03],\n",
      "          [ 8.1058e-02, -1.8121e-02, -1.0605e-02, -7.4729e-02, -6.2174e-03]],\n",
      "\n",
      "         [[ 6.8622e-02,  2.2131e-02,  1.8704e-02, -1.0690e-02,  6.7538e-02],\n",
      "          [ 5.0823e-02, -4.6456e-03, -8.1520e-02,  4.9497e-02, -4.6866e-02],\n",
      "          [-1.8151e-02,  4.8664e-02,  8.8457e-03, -3.6617e-05, -6.9287e-02],\n",
      "          [-7.1729e-02, -7.5581e-02, -5.0315e-02,  4.4237e-02, -1.2318e-03],\n",
      "          [-8.7208e-03, -2.7629e-02, -8.6071e-03,  4.7378e-02,  8.1415e-02]],\n",
      "\n",
      "         [[ 1.5848e-02,  5.4087e-02,  7.4865e-02, -3.4224e-02, -4.2138e-02],\n",
      "          [-3.2105e-02,  2.6194e-03, -4.4061e-02, -7.6657e-02, -7.7386e-02],\n",
      "          [-4.2233e-02,  5.2364e-02,  1.0649e-02, -1.0954e-02,  2.3784e-02],\n",
      "          [-1.3674e-02, -1.4971e-02,  7.6423e-02, -4.1464e-02,  5.5515e-02],\n",
      "          [-5.2007e-02, -5.5659e-02,  2.4992e-02,  4.8466e-02, -6.8549e-02]],\n",
      "\n",
      "         [[ 2.6402e-02, -1.1306e-02, -6.7618e-03, -6.5644e-02, -7.1810e-02],\n",
      "          [ 7.1807e-03,  2.2333e-03, -7.8193e-02,  2.5998e-02,  4.5954e-02],\n",
      "          [ 6.1300e-02, -1.8685e-02, -5.0732e-02,  1.3569e-02,  2.7509e-03],\n",
      "          [ 4.2976e-02,  3.8064e-02,  1.6470e-02, -6.8050e-03,  6.7901e-02],\n",
      "          [-6.2438e-02, -1.2171e-02, -3.1944e-02, -6.3962e-02,  7.8175e-02]],\n",
      "\n",
      "         [[ 5.1016e-02,  5.1434e-02,  2.2165e-04,  7.1546e-02, -2.4277e-02],\n",
      "          [ 3.6617e-02, -2.5331e-02, -7.6061e-02,  2.3633e-04, -7.3093e-04],\n",
      "          [ 5.4361e-02, -5.9729e-02, -1.4935e-02, -3.2475e-02,  3.4733e-02],\n",
      "          [ 4.5657e-02, -6.8171e-02, -2.4262e-02,  2.4992e-02, -2.3005e-02],\n",
      "          [-6.2020e-02, -6.1371e-02,  6.1467e-02, -4.8246e-02,  5.3561e-02]],\n",
      "\n",
      "         [[ 7.5216e-02, -7.9663e-02, -3.1834e-02, -1.8917e-03,  8.0217e-04],\n",
      "          [-1.1888e-02,  1.0577e-03, -6.3434e-02,  3.1467e-03,  5.1145e-02],\n",
      "          [-7.9796e-02, -6.2511e-02, -5.7114e-02,  4.7821e-02, -2.0615e-02],\n",
      "          [ 1.4485e-02,  8.1405e-02,  2.2250e-02,  1.8341e-02,  2.0236e-02],\n",
      "          [ 3.1045e-02,  3.1560e-02,  5.3557e-03, -7.3913e-02, -2.1461e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.1480e-02,  3.1922e-02, -3.4063e-02,  7.3896e-02,  2.4564e-02],\n",
      "          [-3.6244e-02, -9.2738e-03,  5.6793e-02,  2.7830e-02, -5.6918e-02],\n",
      "          [ 1.4718e-02, -3.0342e-02,  4.5372e-02,  3.3060e-02,  6.8759e-02],\n",
      "          [ 7.4455e-02,  5.2892e-02, -2.1532e-02,  2.3787e-02,  6.4545e-02],\n",
      "          [ 6.9936e-02,  5.1657e-02, -6.6322e-02, -3.0598e-02, -2.2956e-02]],\n",
      "\n",
      "         [[-3.6781e-02, -3.2393e-02, -5.7807e-02,  5.6143e-02,  4.1236e-02],\n",
      "          [ 2.1555e-02,  5.7271e-02, -2.6097e-02, -5.5528e-02,  4.4752e-02],\n",
      "          [ 1.0520e-03, -2.6065e-02, -2.9379e-02,  7.8752e-04,  1.3453e-03],\n",
      "          [-6.2457e-02, -2.9616e-02, -5.9288e-02, -2.0961e-02, -5.5899e-03],\n",
      "          [ 6.3807e-02, -8.6046e-03, -7.2129e-02, -7.2485e-02, -5.7287e-02]],\n",
      "\n",
      "         [[-6.2380e-02,  9.9219e-03, -5.8781e-02,  7.9166e-02,  3.4656e-02],\n",
      "          [ 1.4500e-03,  3.6630e-02,  1.8102e-02, -5.1997e-02,  7.5895e-02],\n",
      "          [-1.6989e-02, -3.7875e-02, -4.9662e-02, -3.6314e-02, -1.0293e-02],\n",
      "          [-5.0993e-03, -1.4484e-02,  1.4263e-02, -1.9507e-03,  6.4106e-02],\n",
      "          [ 4.4456e-02,  6.0514e-02, -4.7662e-02,  7.4312e-02,  4.0555e-02]],\n",
      "\n",
      "         [[-2.1919e-02,  2.8460e-02, -8.1581e-02, -4.7239e-02, -4.1585e-02],\n",
      "          [-1.0830e-02, -6.2282e-02,  2.6439e-03, -2.8399e-02,  6.1900e-02],\n",
      "          [ 1.8397e-02,  3.9046e-02, -2.7724e-02, -4.8139e-02,  8.6094e-03],\n",
      "          [ 4.7123e-02, -1.3091e-02, -2.5202e-02, -3.7516e-03,  7.6466e-03],\n",
      "          [ 4.4002e-02, -3.0552e-02,  2.4271e-03,  4.8461e-02, -4.3389e-02]],\n",
      "\n",
      "         [[ 6.8245e-02, -4.2333e-02, -1.0141e-02, -7.2712e-02, -4.2968e-02],\n",
      "          [-3.5378e-02, -1.6189e-02, -2.5724e-02, -4.5088e-02,  1.4009e-02],\n",
      "          [ 2.8905e-02, -1.8618e-02,  3.3446e-02, -3.1530e-04,  3.4522e-03],\n",
      "          [ 2.9983e-02,  2.9449e-02,  6.0968e-02, -5.1968e-02, -2.1394e-02],\n",
      "          [-2.7693e-02, -4.7082e-02,  4.0460e-02,  2.8630e-03, -3.3105e-02]],\n",
      "\n",
      "         [[ 7.0929e-02, -3.4843e-04, -7.1300e-02, -1.1582e-02,  2.5321e-02],\n",
      "          [ 4.8335e-03, -1.4714e-02, -3.3328e-02, -6.3829e-02,  7.7469e-02],\n",
      "          [-4.6180e-02,  4.7400e-03, -6.6577e-02, -7.9105e-02, -7.7189e-02],\n",
      "          [-2.5183e-03, -3.3781e-02, -3.0243e-03, -2.9307e-02,  2.3181e-02],\n",
      "          [-3.4886e-04, -4.3402e-02, -6.7192e-02, -1.6831e-02,  7.6771e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.1980e-02,  3.2684e-03, -7.8014e-02, -6.6181e-02,  2.0949e-02],\n",
      "          [-3.0573e-03, -6.7873e-02, -1.3036e-02,  3.4677e-02,  3.6331e-02],\n",
      "          [-2.1699e-02,  1.8261e-02, -3.2603e-02, -1.3427e-02, -3.4164e-02],\n",
      "          [ 2.9919e-02,  1.0457e-02,  6.5064e-02,  6.1223e-02, -3.8568e-02],\n",
      "          [ 1.4370e-02,  7.5879e-02,  4.9712e-03, -4.9525e-02, -6.5813e-02]],\n",
      "\n",
      "         [[-6.3607e-02,  6.7806e-02, -5.8001e-02,  4.8203e-02,  3.0021e-02],\n",
      "          [-2.9336e-02, -1.9212e-02, -1.4468e-02, -1.5830e-02,  4.8231e-02],\n",
      "          [ 1.0253e-02,  8.1601e-02,  5.3141e-02,  2.7593e-02,  7.2460e-02],\n",
      "          [-1.0322e-02,  7.6543e-02,  6.7847e-02,  1.5936e-02, -8.1499e-02],\n",
      "          [-2.5316e-03, -5.8420e-02,  3.0269e-02, -1.0494e-02, -5.5528e-02]],\n",
      "\n",
      "         [[-7.0269e-02,  3.5080e-02,  4.6674e-02, -4.5576e-02, -2.4322e-02],\n",
      "          [ 6.3157e-02,  3.9838e-02,  4.3689e-02,  3.6460e-02, -2.3604e-02],\n",
      "          [ 5.5019e-02,  2.6073e-02, -4.0228e-02, -8.0080e-02,  5.5649e-02],\n",
      "          [ 6.8398e-02,  3.3197e-02,  2.4542e-02,  3.8909e-02,  6.8769e-02],\n",
      "          [-3.9331e-02,  4.2106e-02,  6.3203e-02,  3.1087e-02, -7.9663e-02]],\n",
      "\n",
      "         [[ 7.7614e-02, -3.9344e-02,  2.7228e-02,  5.7668e-02,  7.0432e-02],\n",
      "          [-5.4258e-02,  7.5633e-02,  1.0488e-02,  2.8783e-03,  2.4701e-02],\n",
      "          [-7.3699e-02,  7.7770e-05, -5.3456e-02, -7.4217e-02,  7.1532e-02],\n",
      "          [-8.3828e-03,  6.8417e-02, -4.6186e-02, -9.7262e-03, -2.5364e-02],\n",
      "          [ 3.0709e-02,  3.5398e-03, -3.4313e-02,  4.4168e-03, -3.6179e-02]],\n",
      "\n",
      "         [[-4.2427e-02,  6.2600e-02,  6.0563e-03, -6.6217e-02, -7.4672e-02],\n",
      "          [ 5.2877e-02, -4.2644e-02,  3.1403e-02, -4.0351e-03,  9.9650e-03],\n",
      "          [ 2.0368e-02, -6.8014e-03, -7.3867e-02,  5.9829e-02,  2.0355e-02],\n",
      "          [-4.2165e-02,  3.1820e-02,  7.1168e-02,  3.9325e-02, -7.1322e-02],\n",
      "          [-6.5819e-02, -4.1931e-02,  6.6024e-02, -4.3073e-02,  7.6761e-02]],\n",
      "\n",
      "         [[-3.3215e-02, -7.3842e-02, -7.7222e-02, -7.2374e-02, -3.7809e-02],\n",
      "          [-7.6310e-02, -3.0388e-02,  2.8749e-02, -6.6100e-03,  4.5843e-02],\n",
      "          [ 7.0256e-02, -1.1714e-02, -6.1042e-02, -4.2743e-02, -1.5203e-02],\n",
      "          [ 1.0224e-02, -3.9434e-02, -6.1942e-02, -6.1688e-02,  4.5400e-02],\n",
      "          [ 1.1396e-02,  7.0803e-02, -1.9658e-02,  5.8692e-02,  1.7449e-02]]]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0389,  0.0382,  0.0268, -0.0410, -0.0243, -0.0733,  0.0401,  0.0166,\n",
      "        -0.0123, -0.0805,  0.0806, -0.0778, -0.0003,  0.0090, -0.0739, -0.0353],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0428, -0.0256, -0.0406,  ..., -0.0099,  0.0401, -0.0088],\n",
      "        [ 0.0048,  0.0096, -0.0397,  ...,  0.0448, -0.0258, -0.0157],\n",
      "        [ 0.0363, -0.0220, -0.0113,  ...,  0.0148, -0.0141, -0.0221],\n",
      "        ...,\n",
      "        [ 0.0178,  0.0172,  0.0213,  ..., -0.0338, -0.0315,  0.0434],\n",
      "        [ 0.0184, -0.0237,  0.0201,  ..., -0.0445, -0.0097,  0.0102],\n",
      "        [ 0.0050,  0.0307, -0.0057,  ...,  0.0499, -0.0344,  0.0287]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0138, -0.0075,  0.0135,  0.0421,  0.0182, -0.0071, -0.0085, -0.0026,\n",
      "         0.0188, -0.0231,  0.0227,  0.0366,  0.0299,  0.0238,  0.0442,  0.0277,\n",
      "        -0.0275, -0.0295,  0.0144,  0.0109, -0.0214, -0.0009, -0.0102,  0.0178,\n",
      "         0.0024, -0.0062, -0.0478, -0.0250, -0.0066, -0.0395,  0.0388,  0.0140,\n",
      "        -0.0458, -0.0110, -0.0322, -0.0005,  0.0124,  0.0098, -0.0004,  0.0442,\n",
      "         0.0163,  0.0190,  0.0211,  0.0088,  0.0382,  0.0008,  0.0303, -0.0314,\n",
      "        -0.0484,  0.0320, -0.0400,  0.0492,  0.0088, -0.0390,  0.0468,  0.0196,\n",
      "         0.0434, -0.0042, -0.0278,  0.0098,  0.0252, -0.0175, -0.0178, -0.0051,\n",
      "         0.0047, -0.0033,  0.0008,  0.0170, -0.0450, -0.0146, -0.0289, -0.0430,\n",
      "         0.0269,  0.0244, -0.0155, -0.0297, -0.0323, -0.0488,  0.0290,  0.0101,\n",
      "         0.0358, -0.0014, -0.0460, -0.0007,  0.0146,  0.0461, -0.0147, -0.0300,\n",
      "        -0.0357,  0.0251,  0.0186,  0.0292, -0.0240, -0.0453, -0.0106, -0.0271,\n",
      "        -0.0224,  0.0364, -0.0417,  0.0457, -0.0025,  0.0072,  0.0034, -0.0307,\n",
      "         0.0416, -0.0352, -0.0392,  0.0123,  0.0046, -0.0488, -0.0391,  0.0085,\n",
      "         0.0045,  0.0346,  0.0184,  0.0245,  0.0104, -0.0395,  0.0036, -0.0296],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0462,  0.0623,  0.0697,  ..., -0.0539,  0.0336,  0.0437],\n",
      "        [ 0.0692, -0.0430,  0.0638,  ..., -0.0316, -0.0199,  0.0238],\n",
      "        [-0.0014,  0.0865, -0.0305,  ..., -0.0041, -0.0867,  0.0660],\n",
      "        ...,\n",
      "        [-0.0285, -0.0172, -0.0326,  ...,  0.0701, -0.0149,  0.0715],\n",
      "        [ 0.0434, -0.0307,  0.0454,  ..., -0.0538,  0.0497,  0.0523],\n",
      "        [-0.0667, -0.0412, -0.0824,  ..., -0.0503,  0.0856, -0.0779]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0861,  0.0431,  0.0645, -0.0305, -0.0254, -0.0231,  0.0622,  0.0359,\n",
      "         0.0588, -0.0052,  0.0346,  0.0821, -0.0822,  0.0623,  0.0883, -0.0331,\n",
      "         0.0062,  0.0288,  0.0019,  0.0100, -0.0587,  0.0368, -0.0315, -0.0461,\n",
      "         0.0704, -0.0101,  0.0739, -0.0533, -0.0400, -0.0199, -0.0666,  0.0832,\n",
      "         0.0159, -0.0888,  0.0239, -0.0906,  0.0268, -0.0058,  0.0307, -0.0877,\n",
      "         0.0212, -0.0281,  0.0358,  0.0711,  0.0391,  0.0813,  0.0563, -0.0413,\n",
      "         0.0904, -0.0702,  0.0805,  0.0840,  0.0566, -0.0575,  0.0816, -0.0297,\n",
      "         0.0353,  0.0907,  0.0792,  0.0301, -0.0819, -0.0662,  0.0751, -0.0620,\n",
      "         0.0011, -0.0268, -0.0004, -0.0651,  0.0531, -0.0414,  0.0309, -0.0730,\n",
      "        -0.0442, -0.0491,  0.0753, -0.0019,  0.0701,  0.0826, -0.0162,  0.0163,\n",
      "        -0.0871,  0.0512, -0.0390,  0.0723], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0310, -0.0083, -0.0524,  0.0438, -0.0971,  0.1068, -0.0303,  0.0443,\n",
      "          0.0280,  0.0217,  0.0217,  0.0353,  0.0266, -0.0256,  0.0974,  0.0465,\n",
      "          0.0126,  0.0099, -0.0022,  0.0963, -0.0413, -0.1039, -0.0600, -0.0213,\n",
      "          0.0394,  0.0922,  0.0974, -0.0423, -0.0134,  0.0914, -0.0401,  0.0544,\n",
      "          0.0792, -0.1002,  0.0473, -0.1024, -0.0523, -0.0146, -0.0780, -0.0140,\n",
      "          0.0999, -0.0684, -0.0726,  0.0150,  0.0149,  0.0904,  0.0240,  0.0066,\n",
      "         -0.0646, -0.0082, -0.0572, -0.0099,  0.0615,  0.0903, -0.0261,  0.0880,\n",
      "         -0.0629,  0.0182,  0.0430,  0.0986, -0.0881,  0.0116, -0.0492,  0.0272,\n",
      "         -0.0227,  0.0472,  0.0005,  0.1042, -0.0579, -0.0854, -0.0963,  0.0591,\n",
      "          0.0739,  0.0685,  0.0038, -0.0570, -0.0415,  0.0919,  0.0609,  0.0119,\n",
      "         -0.0983,  0.0701,  0.0818,  0.0867],\n",
      "        [ 0.0274,  0.0075,  0.0101,  0.0858,  0.0384, -0.0875,  0.0253, -0.0610,\n",
      "         -0.0726, -0.0509,  0.0451,  0.0629,  0.0966, -0.0113,  0.0627,  0.0991,\n",
      "         -0.0758,  0.0778, -0.0160,  0.0874, -0.0998,  0.0686,  0.0319,  0.0462,\n",
      "          0.0675,  0.0879,  0.0835, -0.0956,  0.0103, -0.0344,  0.0263,  0.0004,\n",
      "         -0.0373,  0.0380, -0.0136,  0.0181, -0.0577,  0.0949, -0.0868,  0.0366,\n",
      "         -0.0450, -0.0634, -0.0751,  0.0760,  0.0890,  0.0487, -0.0215, -0.1023,\n",
      "         -0.0382,  0.0138,  0.1009, -0.0109, -0.1039, -0.0153,  0.0929,  0.0973,\n",
      "         -0.0871, -0.0372, -0.0474,  0.0363, -0.0397, -0.0307, -0.1071, -0.0288,\n",
      "          0.0636, -0.0757, -0.0466, -0.0776, -0.0109, -0.1055,  0.0543, -0.0070,\n",
      "         -0.0177,  0.0300,  0.0327, -0.0514, -0.0835, -0.0018,  0.0458,  0.1073,\n",
      "          0.0895,  0.0431,  0.0315,  0.0729],\n",
      "        [-0.0318, -0.0677,  0.0628,  0.0013,  0.0392, -0.0389, -0.0199, -0.0713,\n",
      "         -0.0036, -0.0597, -0.0774,  0.0855,  0.0779,  0.0686,  0.0830, -0.0272,\n",
      "         -0.0303, -0.0391,  0.0691,  0.0848, -0.0663, -0.0060, -0.0905, -0.0485,\n",
      "          0.0745, -0.0788,  0.0854,  0.0969,  0.0996, -0.0262, -0.0271, -0.0562,\n",
      "          0.0952,  0.0327, -0.0617, -0.0528,  0.0421, -0.0120,  0.0464, -0.0021,\n",
      "         -0.0455, -0.0203, -0.0706,  0.1045,  0.0712,  0.0315,  0.1081, -0.0730,\n",
      "          0.0871,  0.0944,  0.0760, -0.0338,  0.0613, -0.0029,  0.1017,  0.0835,\n",
      "          0.0934, -0.0917,  0.0396,  0.0482,  0.0469,  0.0253, -0.0948, -0.1060,\n",
      "         -0.0502,  0.0561,  0.0611,  0.0486,  0.0370,  0.0927, -0.0190, -0.0695,\n",
      "          0.1023, -0.0623,  0.0921,  0.0428, -0.0067,  0.0113, -0.0986, -0.0096,\n",
      "         -0.0404,  0.0502, -0.1054,  0.0397],\n",
      "        [ 0.0027,  0.0059,  0.0595, -0.0132, -0.0794, -0.1049,  0.0190,  0.0717,\n",
      "         -0.0266, -0.0521,  0.0931, -0.0811, -0.0711,  0.0803, -0.0435,  0.1088,\n",
      "          0.0028, -0.0823, -0.0302, -0.0991, -0.0394, -0.0600,  0.0358, -0.0858,\n",
      "          0.0299, -0.0351, -0.0947,  0.0588,  0.0375,  0.1047,  0.0008,  0.0661,\n",
      "          0.0416, -0.0868,  0.0325,  0.1029,  0.0314, -0.0040, -0.0887, -0.0796,\n",
      "         -0.0444, -0.0604, -0.0818,  0.0847,  0.0318,  0.0764, -0.0799,  0.0385,\n",
      "         -0.0751,  0.1048, -0.0942, -0.0753, -0.0456,  0.0688,  0.0745,  0.0258,\n",
      "         -0.0821,  0.0345, -0.0272,  0.0631, -0.0336, -0.0694,  0.0671,  0.0793,\n",
      "          0.0297, -0.0650,  0.0233, -0.0030, -0.0663,  0.0900, -0.0428, -0.0692,\n",
      "          0.0499, -0.0459, -0.0465,  0.0621, -0.0937, -0.1048,  0.0449,  0.0545,\n",
      "         -0.0663, -0.0807, -0.0556,  0.0964],\n",
      "        [-0.0660, -0.0715,  0.0913, -0.0550, -0.0838, -0.0164,  0.0157,  0.1032,\n",
      "         -0.1085,  0.0995, -0.0669,  0.1086,  0.0545,  0.0722, -0.0905, -0.0269,\n",
      "         -0.0183, -0.0589, -0.0429, -0.0160, -0.0709,  0.0474,  0.0003, -0.0710,\n",
      "         -0.1017,  0.0431, -0.0576,  0.0189, -0.0702, -0.0515,  0.0968, -0.0216,\n",
      "          0.0486, -0.0875, -0.0904,  0.0327, -0.0042,  0.0057, -0.0438, -0.0892,\n",
      "          0.1026,  0.0726, -0.0492,  0.0756, -0.0803, -0.0959, -0.0816, -0.0096,\n",
      "          0.0250, -0.0184, -0.0715,  0.0612, -0.0074, -0.0758, -0.0673,  0.0976,\n",
      "          0.0568,  0.0542, -0.0691,  0.0826, -0.0873, -0.0912, -0.0444,  0.0948,\n",
      "         -0.0275,  0.1068,  0.0387,  0.0897,  0.0783,  0.0805, -0.0534,  0.0025,\n",
      "          0.0288,  0.0569, -0.0580,  0.0826,  0.0701, -0.0208, -0.0712, -0.0461,\n",
      "          0.0199, -0.0791,  0.0589,  0.0671],\n",
      "        [ 0.0759,  0.0558, -0.0709,  0.0823, -0.0425,  0.0988, -0.0937, -0.0026,\n",
      "          0.1069, -0.0333, -0.0775, -0.0813,  0.1089,  0.0437, -0.0060, -0.0797,\n",
      "          0.0227,  0.0592,  0.0696, -0.0468,  0.0660, -0.1068, -0.0780,  0.0814,\n",
      "         -0.0906, -0.0514,  0.0462, -0.0655, -0.0061,  0.0030,  0.0706,  0.0822,\n",
      "         -0.0958,  0.0834,  0.0259, -0.0666, -0.1074,  0.0331, -0.0463, -0.0161,\n",
      "         -0.0093, -0.0027,  0.0482, -0.0576,  0.0343,  0.0106, -0.0768, -0.0043,\n",
      "         -0.0320, -0.0107, -0.0257,  0.1042, -0.0395, -0.0671,  0.0732, -0.0709,\n",
      "         -0.0107,  0.0207,  0.0855, -0.0032, -0.0767,  0.0066, -0.0780, -0.0508,\n",
      "          0.0288,  0.1087, -0.0642,  0.0917, -0.0319, -0.0111,  0.0208,  0.0840,\n",
      "         -0.0003,  0.0565,  0.0754,  0.0066, -0.0455,  0.0605,  0.0663, -0.0568,\n",
      "          0.0280, -0.0031,  0.0601,  0.1033],\n",
      "        [-0.0724,  0.0791, -0.1044,  0.0211,  0.0589,  0.0664, -0.1033,  0.0249,\n",
      "         -0.0922,  0.0932,  0.0922,  0.0190, -0.0419, -0.0833,  0.0570,  0.0270,\n",
      "         -0.0972, -0.0539, -0.0255, -0.0789,  0.0707, -0.0919,  0.0796,  0.0763,\n",
      "          0.1067,  0.0543, -0.0717,  0.0203, -0.0927,  0.0061,  0.0895,  0.0861,\n",
      "         -0.0986,  0.0208,  0.0432, -0.0890,  0.0566,  0.0221, -0.0122,  0.0985,\n",
      "          0.0176, -0.0146,  0.0211,  0.0386, -0.0122,  0.0116, -0.0362,  0.0108,\n",
      "         -0.1029, -0.0823, -0.0743, -0.0164,  0.0471, -0.0433,  0.0243,  0.0381,\n",
      "         -0.1001,  0.0646,  0.0040, -0.1090,  0.0184,  0.0695,  0.0280,  0.0492,\n",
      "         -0.0967,  0.0630, -0.0793,  0.0215, -0.0432, -0.0847,  0.0250,  0.0923,\n",
      "         -0.0721,  0.0570, -0.0513,  0.0680,  0.0852, -0.0183, -0.0486,  0.1050,\n",
      "          0.0357, -0.0277, -0.0102,  0.0788],\n",
      "        [-0.0570,  0.0462,  0.0120,  0.0238, -0.0880,  0.0752,  0.0013, -0.0551,\n",
      "          0.0027, -0.0999,  0.0359, -0.0488,  0.0639, -0.0548, -0.0781,  0.0635,\n",
      "          0.0845,  0.0416, -0.0282, -0.0453, -0.0612,  0.0286,  0.0051, -0.0117,\n",
      "         -0.0862,  0.0477,  0.0118,  0.0915, -0.0622, -0.0884,  0.0975,  0.0780,\n",
      "          0.0011,  0.0882,  0.0450, -0.1034,  0.0991,  0.0441, -0.0806, -0.0692,\n",
      "          0.0491, -0.1083,  0.1043,  0.0020,  0.0397,  0.0930,  0.0848,  0.0069,\n",
      "         -0.1022,  0.0035, -0.0821, -0.0272,  0.0818, -0.0689,  0.1073,  0.0227,\n",
      "          0.0505, -0.0762, -0.0075, -0.0135, -0.0177,  0.0640, -0.0662,  0.1028,\n",
      "          0.0286, -0.0929,  0.0681, -0.0209,  0.0581,  0.0690, -0.0433,  0.0066,\n",
      "          0.0604, -0.0959, -0.0387, -0.0789,  0.0540, -0.1056,  0.0663,  0.0115,\n",
      "          0.0100, -0.0348,  0.0240, -0.0015],\n",
      "        [-0.0128,  0.0534,  0.0056,  0.0138,  0.0817,  0.0543, -0.0195,  0.0101,\n",
      "          0.0415, -0.0834,  0.0108,  0.0782, -0.0956, -0.0073, -0.0344,  0.1072,\n",
      "         -0.0293, -0.0333,  0.0575,  0.0311, -0.0537,  0.0201, -0.0548,  0.1006,\n",
      "          0.0503, -0.0518,  0.0818,  0.0557,  0.1047,  0.0569, -0.0980, -0.0246,\n",
      "         -0.0749, -0.0015, -0.1041,  0.0767, -0.0755, -0.0863,  0.0240,  0.0164,\n",
      "         -0.0414,  0.0911,  0.1071,  0.0004, -0.0769,  0.0819, -0.1081, -0.0113,\n",
      "          0.0099,  0.0160,  0.0010, -0.0151, -0.1040,  0.0634, -0.0965,  0.0549,\n",
      "         -0.0275,  0.0399, -0.1065,  0.0843,  0.0576, -0.0074,  0.0508,  0.1053,\n",
      "         -0.0431,  0.1027, -0.0341, -0.0030, -0.0835, -0.0372,  0.0353,  0.0998,\n",
      "         -0.0497,  0.0067,  0.0291, -0.0438, -0.0040, -0.0895,  0.0713,  0.0364,\n",
      "          0.0314, -0.0876, -0.0426,  0.0958],\n",
      "        [-0.0414,  0.1070,  0.0070,  0.0564,  0.0233, -0.0899, -0.1071, -0.0586,\n",
      "         -0.0523, -0.0124, -0.0145, -0.0168,  0.0084, -0.0841, -0.0348,  0.0413,\n",
      "         -0.1039, -0.0767,  0.0827, -0.0985,  0.0365, -0.1025, -0.0299,  0.0177,\n",
      "          0.0340, -0.0123, -0.0800, -0.0853, -0.0931,  0.0333,  0.0579,  0.0040,\n",
      "          0.0948, -0.0673,  0.0968,  0.0796,  0.0805, -0.1065,  0.0805, -0.0523,\n",
      "          0.0993,  0.0916, -0.0868, -0.0894,  0.0020,  0.0472,  0.0232,  0.0558,\n",
      "          0.0310,  0.0482,  0.0200,  0.0564,  0.0454,  0.0916, -0.0801,  0.1030,\n",
      "          0.0556, -0.0403, -0.0760, -0.0883, -0.0936,  0.0222,  0.0318,  0.0983,\n",
      "          0.0917, -0.0400, -0.0418, -0.0274, -0.0861, -0.0251, -0.0105, -0.0790,\n",
      "          0.0468,  0.0846, -0.0766, -0.0938, -0.0300, -0.0937,  0.0875, -0.0856,\n",
      "          0.0429,  0.0815,  0.0178,  0.0909]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0139,  0.0040, -0.0021,  0.0511, -0.0593, -0.0125,  0.0865,  0.0023,\n",
      "         0.0479, -0.1086], requires_grad=True)]\n",
      "[('', LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")), ('conv1', Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))), ('conv2', Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))), ('fc1', Linear(in_features=400, out_features=120, bias=True)), ('fc2', Linear(in_features=120, out_features=84, bias=True)), ('fc3', Linear(in_features=84, out_features=10, bias=True))]\n",
      "[LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)), Linear(in_features=400, out_features=120, bias=True), Linear(in_features=120, out_features=84, bias=True), Linear(in_features=84, out_features=10, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "print(list(model.named_parameters()))\n",
    "print(list(model.parameters()))\n",
    "print(list(model.named_modules()))\n",
    "print(list(model.modules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6e939",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Create an instance of the class below and print its modules and parameters. What do we observe? Fix this issue by using the object `torch.nn.ModuleList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b6a3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SomeModel, self).__init__()\n",
    "\n",
    "        self.layers = [torch.nn.Linear(5, 5),\n",
    "                      torch.nn.ReLU(),\n",
    "                      torch.nn.Linear(5, 1)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b61bdcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[('', SomeModel())]\n",
      "[SomeModel()]\n"
     ]
    }
   ],
   "source": [
    "some_model = SomeModel()\n",
    "print(list(some_model.named_parameters()))\n",
    "print(list(some_model.parameters()))\n",
    "print(list(some_model.named_modules()))\n",
    "print(list(some_model.modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "130444d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeModelFixed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SomeModelFixed, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([torch.nn.Linear(5, 5),\n",
    "                      torch.nn.ReLU(),\n",
    "                      torch.nn.Linear(5, 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89d53d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layers.0.weight', Parameter containing:\n",
      "tensor([[-0.3557,  0.2064,  0.3078, -0.1833,  0.1075],\n",
      "        [-0.3880, -0.0510,  0.4237,  0.0250,  0.2725],\n",
      "        [-0.3423,  0.3546,  0.3932, -0.1455, -0.2228],\n",
      "        [-0.2960, -0.0474,  0.0858,  0.1520,  0.1151],\n",
      "        [-0.1502,  0.4360, -0.1503, -0.1928,  0.1469]], requires_grad=True)), ('layers.0.bias', Parameter containing:\n",
      "tensor([ 0.2585, -0.1512,  0.1303, -0.0894, -0.0190], requires_grad=True)), ('layers.2.weight', Parameter containing:\n",
      "tensor([[ 0.1877,  0.1054, -0.1832,  0.3872,  0.0706]], requires_grad=True)), ('layers.2.bias', Parameter containing:\n",
      "tensor([-0.0975], requires_grad=True))]\n",
      "[Parameter containing:\n",
      "tensor([[-0.3557,  0.2064,  0.3078, -0.1833,  0.1075],\n",
      "        [-0.3880, -0.0510,  0.4237,  0.0250,  0.2725],\n",
      "        [-0.3423,  0.3546,  0.3932, -0.1455, -0.2228],\n",
      "        [-0.2960, -0.0474,  0.0858,  0.1520,  0.1151],\n",
      "        [-0.1502,  0.4360, -0.1503, -0.1928,  0.1469]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2585, -0.1512,  0.1303, -0.0894, -0.0190], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1877,  0.1054, -0.1832,  0.3872,  0.0706]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0975], requires_grad=True)]\n",
      "[('', SomeModelFixed(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")), ('layers', ModuleList(\n",
      "  (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")), ('layers.0', Linear(in_features=5, out_features=5, bias=True)), ('layers.1', ReLU()), ('layers.2', Linear(in_features=5, out_features=1, bias=True))]\n",
      "[SomeModelFixed(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      "), ModuleList(\n",
      "  (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "), Linear(in_features=5, out_features=5, bias=True), ReLU(), Linear(in_features=5, out_features=1, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "some_model = SomeModelFixed()\n",
    "print(list(some_model.named_parameters()))\n",
    "print(list(some_model.parameters()))\n",
    "print(list(some_model.named_modules()))\n",
    "print(list(some_model.modules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce5895",
   "metadata": {},
   "source": [
    "**Question 3 (optional)**\n",
    "\n",
    "Check out the method `register_buffer` of `Module`, explain why it can be useful and show a use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed12bf",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b782325",
   "metadata": {},
   "source": [
    "## Basic training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe731fa",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "We want to train LeNet on MNIST. Fill the blanks in the following pieces of code.\n",
    "\n",
    "Wrap the training process into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01889385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nepochs):\n",
    "    #List to store loss to visualize\n",
    "    valid_loss_min = np.inf # track change in validation loss\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    acc_eval = []\n",
    "    #test_counter = [i*len(train_loader.dataset) for i in n_epochs]\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device = device)\n",
    "            target = target.to(device = device)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device = device)\n",
    "                target = target.to(device = device)\n",
    "\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "        acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f172534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.675757 \tValidation Loss: 0.748479\n",
      "Epoch: 1 \tTraining Loss: 0.506517 \tValidation Loss: 0.350556\n",
      "Epoch: 2 \tTraining Loss: 0.303011 \tValidation Loss: 0.240242\n",
      "Epoch: 3 \tTraining Loss: 0.221030 \tValidation Loss: 0.182419\n",
      "Epoch: 4 \tTraining Loss: 0.173921 \tValidation Loss: 0.146838\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e09afa",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4420490",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "To make sure that the inputs of the neural network are within a controlled range, we usually transform the dataset to be sure that the data are centered with variance 1. It is not always necessary, but it is worth knowing it.\n",
    "\n",
    "Check the range of values on a sample of MNIST. Compute the mean and the standard deviation of the training dataset of MNIST and normalize the dataset accordingly by using `transforms.Normalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af99f662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1307) tensor(0.3081)\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "\n",
    "total_sum = 0\n",
    "for data, target in train_loader:\n",
    "    total_sum += data.sum()\n",
    "mean = total_sum / (28**2 * len(train_loader.dataset))\n",
    "\n",
    "total_var = 0\n",
    "for data, target in train_loader:\n",
    "    total_var += (data - mean).pow(2).sum()\n",
    "std = (total_var / (28**2 * len(train_loader.dataset))).sqrt()\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61f0c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa70c8",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a7be9",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "It is very common to face overfitting when doing deep learning. So, several methods can be used to solve this problem. One of them is called \"data augmentation\". It consists in adding \"noise\" to data points of the training dataset in order to make the model resistant to small changes of the data. \n",
    "\n",
    "When training on images, it is common to perform \"random crops\", \"random flips\", and small \"random rotations\". With MNIST, it is meaningless to add random flips, because most digits are not supposed to be invariant by vertical or horizontal symmetries.\n",
    "\n",
    "Add random crops with `transforms.RandomCrop` with a reasonable number of pixels to the transforms to do on the dataset, visualize the resulting images and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eca736ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomCrop(28, padding = 3)\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e66bfb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x350 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_idx = np.random.choice(64, 10, replace = False)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (7.5, 3.5))\n",
    "\n",
    "for data, target in train_loader:\n",
    "    for i, idx in enumerate(lst_idx):\n",
    "        img, tgt = data[idx], target[idx]\n",
    "        ax[i//5, i%5].imshow(img.numpy().transpose(1, 2, 0), cmap=\"Greys\")\n",
    "        ax[i//5, i%5].set_title(classes[tgt])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68234341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.933978 \tValidation Loss: 1.378455\n",
      "Epoch: 1 \tTraining Loss: 1.104678 \tValidation Loss: 0.802460\n",
      "Epoch: 2 \tTraining Loss: 0.584697 \tValidation Loss: 0.405548\n",
      "Epoch: 3 \tTraining Loss: 0.339450 \tValidation Loss: 0.267051\n",
      "Epoch: 4 \tTraining Loss: 0.249938 \tValidation Loss: 0.205249\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc885868",
   "metadata": {},
   "source": [
    "## Influence of the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18b085",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Build a Multilayer Perceptron with ReLU activation functions, which takes 3 arguments: \n",
    " * `layers`: the list of layer sizes;\n",
    " * `sigma_w`: the standard deviation chosen for initializing the weights;\n",
    " * `with_scaling`: if True, we multiply the generated weights by $1/\\sqrt{\\# \\text{inputs}}$.\n",
    "\n",
    "Write the `reset_parameters` method, which initialize the weights according to a Gaussian distribution, either with variance $\\sigma_w^2$, or $\\sigma_w^2/\\# \\text{inputs}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0fad359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, layers, sigma_w, scaling = False):\n",
    "        super(Perceptron, self).__init__()\n",
    "\n",
    "        self.act_function = torch.relu\n",
    "        self.scaling = scaling\n",
    "        self.sigma_w = sigma_w\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for l_in, l_out in zip(layers[:-1], layers[1:]):\n",
    "            self.layers.append(nn.Linear(l_in, l_out))\n",
    "        self.nb_layers = len(self.layers)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            for i, l in enumerate(self.layers):\n",
    "                l.weight.data.normal_()\n",
    "                l.weight.data.mul_(self.sigma_w)\n",
    "                \n",
    "                if self.scaling:\n",
    "                    l.weight.data.div_(np.sqrt(l.weight.size(1)))\n",
    "                    \n",
    "                l.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for l in self.layers[:-1]:\n",
    "            x = l(x)\n",
    "            x = self.act_function(x)\n",
    "                                       \n",
    "        x = self.layers[-1](x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1396cf",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Train the model with various choices of initialization (which ones?) and try various numbers of layers with various widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84868847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 2.301863 \tValidation Loss: 2.301313\n",
      "Epoch: 1 \tTraining Loss: 2.301321 \tValidation Loss: 2.301091\n",
      "Epoch: 2 \tTraining Loss: 2.301232 \tValidation Loss: 2.301045\n",
      "Epoch: 3 \tTraining Loss: 2.301216 \tValidation Loss: 2.301033\n",
      "Epoch: 4 \tTraining Loss: 2.301213 \tValidation Loss: 2.301030\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 50, 10], 0).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d4e018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.120220 \tValidation Loss: 0.665494\n",
      "Epoch: 1 \tTraining Loss: 0.551310 \tValidation Loss: 0.453000\n",
      "Epoch: 2 \tTraining Loss: 0.407433 \tValidation Loss: 0.361835\n",
      "Epoch: 3 \tTraining Loss: 0.346228 \tValidation Loss: 0.311518\n",
      "Epoch: 4 \tTraining Loss: 0.302627 \tValidation Loss: 0.280478\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 50, 10], np.sqrt(2), True).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "310c2676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 29.924207 \tValidation Loss: 2.869874\n",
      "Epoch: 1 \tTraining Loss: 2.696083 \tValidation Loss: 2.508532\n",
      "Epoch: 2 \tTraining Loss: 2.453636 \tValidation Loss: 2.371809\n",
      "Epoch: 3 \tTraining Loss: 2.428252 \tValidation Loss: 2.393013\n",
      "Epoch: 4 \tTraining Loss: 2.377721 \tValidation Loss: 2.343491\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 50, 10], np.sqrt(2), False).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cb9c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 2.115622 \tValidation Loss: 1.761019\n",
      "Epoch: 1 \tTraining Loss: 1.293322 \tValidation Loss: 1.012153\n",
      "Epoch: 2 \tTraining Loss: 0.827263 \tValidation Loss: 0.662592\n",
      "Epoch: 3 \tTraining Loss: 0.610450 \tValidation Loss: 0.538347\n",
      "Epoch: 4 \tTraining Loss: 0.495704 \tValidation Loss: 0.459998\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 100, 10, 100, 50, 10], .1, False).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38414621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.297253 \tValidation Loss: 0.657036\n",
      "Epoch: 1 \tTraining Loss: 0.547711 \tValidation Loss: 0.485779\n",
      "Epoch: 2 \tTraining Loss: 0.407126 \tValidation Loss: 0.399306\n",
      "Epoch: 3 \tTraining Loss: 0.342990 \tValidation Loss: 0.348774\n",
      "Epoch: 4 \tTraining Loss: 0.301646 \tValidation Loss: 0.290041\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 100, 10, 100, 50, 10], 1.4, True).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39428313",
   "metadata": {},
   "source": [
    "**Question 5 (optional)**\n",
    "\n",
    "Implement the NTK parameterization in the `Perceptron` model: divide by $1/\\sqrt{\\# \\text{inputs}}$ the result of each layer.\n",
    "\n",
    "Train such a network with the SGD (with `scaling = False`) with learning rates of order $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3d4e1",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7de74",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Train LeNet with the SGD and Adam with default parameters and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c63435e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.382929 \tValidation Loss: 0.150886\n",
      "Epoch: 1 \tTraining Loss: 0.121737 \tValidation Loss: 0.109628\n",
      "Epoch: 2 \tTraining Loss: 0.095810 \tValidation Loss: 0.095509\n",
      "Epoch: 3 \tTraining Loss: 0.083188 \tValidation Loss: 0.076180\n",
      "Epoch: 4 \tTraining Loss: 0.075783 \tValidation Loss: 0.061905\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c86e80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.784894 \tValidation Loss: 1.275867\n",
      "Epoch: 1 \tTraining Loss: 0.980806 \tValidation Loss: 0.665495\n",
      "Epoch: 2 \tTraining Loss: 0.516080 \tValidation Loss: 0.388775\n",
      "Epoch: 3 \tTraining Loss: 0.338161 \tValidation Loss: 0.270794\n",
      "Epoch: 4 \tTraining Loss: 0.265009 \tValidation Loss: 0.219891\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a4f46",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6a816",
   "metadata": {},
   "source": [
    "**Additional questions**\n",
    "\n",
    " * explore data augmentation with the dataset CIFAR-10\n",
    " * test different batch sizes. How to change the learning rate when we change the batch size?\n",
    " * test drop-out layers\n",
    " * read the documentation on SGD and Adam an propose a way to assign different learning rates to different sets of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6037039",
   "metadata": {},
   "source": [
    "# Saving and loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73aa21",
   "metadata": {},
   "source": [
    "When training a model, it is a good practice to make checkpoints every few epochs. That is, store the current state of the model **and the optimizer**. Doing so, we are able to reload the current state of a training process if:\n",
    " * the training has been interrupted for an unknown reason (which occurs when using a cluster);\n",
    " * we want to diagnose an issue with the model at an early stage (drop of performance, instabilities of the loss, etc.).\n",
    "\n",
    "It is essential to save the current state of the optimizer, since it contains frequently information acquired during the early stages of training (momentum with SGD + momentum, running means and moments with Adam).\n",
    "\n",
    "Above all, we define again a model to train and a training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c9bfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.act_function = torch.tanh\n",
    "        layers = [1, 6, 16, 120, 84, 10]\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(layers[0], layers[1], 5, padding = 2)\n",
    "        self.conv2 = torch.nn.Conv2d(layers[1], layers[2], 5)\n",
    "        self.fc1 = torch.nn.Linear(5 * 5 * layers[2], layers[3])\n",
    "        self.fc2 = torch.nn.Linear(layers[3], layers[4])\n",
    "        self.fc3 = torch.nn.Linear(layers[4], layers[5])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_function(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_function(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e634d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = .1307, .3081\n",
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = False, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = False, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14a20aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial version of train_model\n",
    "\n",
    "def train_model(model, criterion, optimizer, nepochs):\n",
    "    #List to store loss to visualize\n",
    "    valid_loss_min = np.inf # track change in validation loss\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    acc_eval = []\n",
    "    #test_counter = [i*len(train_loader.dataset) for i in n_epochs]\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device = device)\n",
    "            target = target.to(device = device)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device = device)\n",
    "                target = target.to(device = device)\n",
    "\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "        acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46718925",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Add lines of code to the function `train_model` to store both the model and the optimizer. One can use `torch.save` and the methods `state_dict` of `Module` and `Optimizer`. Note: one can use `torch.save` to save any PyTorch object **and** any native Python object, such as `dict`.\n",
    "\n",
    "Add options to `train_model` to resume training from a specific checkpoint.\n",
    "\n",
    "Launch a training with Adam, stop it, and then resume it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad1f2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nepochs, save_path = None,\n",
    "               load_path = None):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    acc_eval = []\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if load_path is not None:\n",
    "        checkpoint = torch.load(load_path, weights_only = False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        acc_eval = checkpoint[\"acc_eval\"]\n",
    "        train_losses = checkpoint[\"train_losses\"]\n",
    "        test_losses = checkpoint[\"test_losses\"]\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device = device)\n",
    "            target = target.to(device = device)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device = device)\n",
    "                target = target.to(device = device)\n",
    "\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "        acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "        \n",
    "        # Save\n",
    "        if save_path is not None:\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"acc_eval\": acc_eval,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"test_losses\": test_losses\n",
    "            }, save_path)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6ab82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.221681 \tValidation Loss: 0.097727\n",
      "Epoch: 1 \tTraining Loss: 0.066561 \tValidation Loss: 0.066933\n",
      "Epoch: 2 \tTraining Loss: 0.045085 \tValidation Loss: 0.060652\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nepochs = 3\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs, save_path = \"checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc3bcb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 0.033534 \tValidation Loss: 0.058244\n",
      "Epoch: 4 \tTraining Loss: 0.025514 \tValidation Loss: 0.052174\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs, load_path = \"checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ffe12",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Define a model and a Adam optimizer attached to it. Train them for a few epochs, then examine their `state_dict` and compare it to their complete list of attributes (by using the `vars` function or the attribute `__dict__`). Are all the attributes stored?\n",
    "\n",
    "Examine `optimizer.state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ea17b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.214726 \tValidation Loss: 0.075817\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device = device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "nepochs = 1\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d6d5962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52726dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state', 'param_groups'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81481145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['defaults', '_optimizer_step_pre_hooks', '_optimizer_step_post_hooks', '_optimizer_state_dict_pre_hooks', '_optimizer_state_dict_post_hooks', '_optimizer_load_state_dict_pre_hooks', '_optimizer_load_state_dict_post_hooks', '_zero_grad_profile_name', 'state', 'param_groups', '_warned_capturable_if_run_uncaptured'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c315a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.001,\n",
       " 'betas': (0.9, 0.999),\n",
       " 'eps': 1e-08,\n",
       " 'weight_decay': 0,\n",
       " 'amsgrad': False,\n",
       " 'maximize': False,\n",
       " 'foreach': None,\n",
       " 'capturable': False,\n",
       " 'differentiable': False,\n",
       " 'fused': None}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ff06818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[[[ 1.2111e-01, -6.4034e-02,  3.1799e-02, -4.7571e-02, -2.6972e-01],\n",
       "                       [ 1.3388e-01,  2.4175e-02, -3.9812e-02, -2.0724e-01, -2.1764e-01],\n",
       "                       [-8.8096e-02,  1.5533e-01, -7.5839e-05, -2.0076e-01, -1.1777e-01],\n",
       "                       [-8.0312e-03,  6.6289e-02, -4.4284e-02, -2.5540e-01,  1.3134e-02],\n",
       "                       [-9.2619e-02,  1.5221e-01, -7.4998e-02, -1.9236e-01, -6.2910e-02]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 2.3816e-01, -3.9617e-02,  1.8663e-01,  1.7814e-02,  1.3682e-01],\n",
       "                       [ 1.5346e-01,  3.9953e-02,  1.5768e-01,  1.1038e-01,  1.9835e-01],\n",
       "                       [-1.1521e-01, -2.9974e-02, -1.5184e-01,  1.0774e-01,  7.9503e-02],\n",
       "                       [-1.1507e-01, -1.9940e-01,  1.1349e-01, -6.1508e-02, -1.8455e-01],\n",
       "                       [-1.0082e-01, -5.0745e-02,  5.2754e-02,  4.1347e-02, -2.1872e-01]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 1.9692e-03, -4.3452e-02,  1.1518e-01,  2.1474e-01,  1.6470e-01],\n",
       "                       [-2.3618e-01,  5.7591e-02, -1.8384e-01, -1.1858e-02, -1.7499e-01],\n",
       "                       [-1.9412e-01, -2.2075e-01, -2.4126e-01, -2.4273e-01, -2.3312e-01],\n",
       "                       [ 1.0318e-01, -7.6705e-02, -3.7772e-02,  5.3983e-02,  3.9793e-02],\n",
       "                       [ 1.7106e-01,  7.3750e-02,  4.5119e-02, -6.9851e-03, -1.2250e-01]]],\n",
       "             \n",
       "             \n",
       "                     [[[-2.7488e-02, -1.7844e-01, -1.6807e-01,  4.3308e-02, -1.7927e-01],\n",
       "                       [ 2.0172e-02,  5.3912e-02,  1.6049e-01, -2.0638e-01,  8.9314e-02],\n",
       "                       [-4.8006e-02,  7.1965e-02,  1.5845e-03,  6.7130e-02, -2.4702e-02],\n",
       "                       [-8.1595e-04,  4.3987e-02,  2.0839e-01, -1.2662e-01,  1.2193e-01],\n",
       "                       [ 1.6931e-01,  1.9619e-01,  9.2808e-02, -6.5769e-02, -1.5557e-02]]],\n",
       "             \n",
       "             \n",
       "                     [[[-2.1963e-02,  3.1450e-02, -1.1954e-01,  9.3611e-02, -1.6448e-01],\n",
       "                       [-1.4885e-01,  1.1477e-01, -1.2768e-01, -1.3171e-01, -1.5409e-01],\n",
       "                       [-9.0676e-02,  2.0658e-02, -1.4215e-01,  2.0743e-01,  1.4672e-01],\n",
       "                       [ 2.1752e-01,  1.0974e-01,  7.6558e-02,  2.4702e-02,  2.5999e-01],\n",
       "                       [ 1.9885e-01,  2.0579e-01,  4.5479e-02,  3.0305e-01,  1.4743e-01]]],\n",
       "             \n",
       "             \n",
       "                     [[[-2.8131e-01,  5.1327e-02, -1.4096e-01, -6.7173e-03, -8.8629e-03],\n",
       "                       [-2.0545e-01, -1.8671e-01,  1.7756e-02, -9.7450e-02, -1.6165e-02],\n",
       "                       [ 2.1405e-01,  1.4150e-01,  1.7077e-01, -1.3090e-02, -1.2201e-01],\n",
       "                       [ 1.3894e-01,  2.5236e-02, -4.3540e-02,  4.2282e-03,  1.7656e-01],\n",
       "                       [ 1.3782e-02, -1.8015e-01, -3.6817e-02,  2.2201e-01,  1.9014e-01]]]],\n",
       "                    requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([[[[ 1.3853e-02,  1.0077e-02,  5.5091e-03,  5.6391e-03,  6.9340e-03],\n",
       "                        [ 4.0890e-03, -1.7630e-03, -1.6727e-03, -1.4676e-03,  2.6821e-03],\n",
       "                        [-2.3371e-03, -3.4740e-03,  2.2815e-04,  2.0633e-03,  2.3716e-03],\n",
       "                        [-1.0678e-02, -9.1764e-03, -4.7966e-03, -3.2291e-03, -7.4294e-03],\n",
       "                        [-1.6492e-02, -1.3329e-02, -6.7367e-03, -8.3464e-03, -1.3792e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0702e-02,  7.4099e-03,  9.3417e-03,  7.9201e-03,  4.7073e-03],\n",
       "                        [ 7.7515e-03,  8.4140e-03,  7.9724e-03, -1.6069e-07, -4.0578e-03],\n",
       "                        [ 2.4554e-03,  5.9460e-03,  1.7124e-03, -7.8875e-03, -8.5245e-03],\n",
       "                        [ 2.5045e-03, -6.3308e-04, -6.6618e-03, -7.7758e-03, -2.1822e-03],\n",
       "                        [ 8.3465e-05, -7.2128e-03, -1.1419e-02, -1.0723e-02, -7.2970e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4987e-03,  8.4343e-04, -4.6041e-03, -6.3654e-03, -8.1272e-03],\n",
       "                        [-1.4605e-03, -9.2293e-04, -4.3131e-03, -2.4517e-03, -5.5344e-03],\n",
       "                        [-5.6966e-03,  2.3809e-03,  6.2778e-03,  6.4959e-03,  3.0521e-03],\n",
       "                        [-3.6943e-03,  5.9871e-03,  1.4628e-02,  1.4381e-02,  1.3371e-02],\n",
       "                        [-1.2208e-03,  1.4787e-02,  2.3967e-02,  1.9352e-02,  1.4939e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.5724e-03,  7.5797e-03,  5.3744e-03,  4.5748e-03,  6.7327e-04],\n",
       "                        [ 2.3761e-03,  2.0308e-03, -3.6445e-03, -3.4526e-03, -4.5704e-03],\n",
       "                        [-3.9950e-03, -2.1699e-03, -5.0161e-03, -2.0074e-03, -1.3386e-03],\n",
       "                        [ 3.2403e-03,  4.1652e-03,  2.1285e-03,  5.5712e-04,  1.2458e-03],\n",
       "                        [ 7.0872e-03,  4.7911e-03, -9.1919e-04,  1.2319e-03,  1.9088e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8094e-03,  9.0465e-04,  8.3602e-04,  3.2424e-03,  2.9972e-03],\n",
       "                        [-4.7062e-04, -3.5590e-03, -1.7191e-03, -3.4981e-04,  9.5467e-04],\n",
       "                        [-5.0728e-03, -9.2622e-03, -7.1729e-03, -4.7968e-03, -1.9873e-03],\n",
       "                        [-1.3252e-03, -2.9566e-03, -3.1270e-03, -5.5921e-04,  5.1853e-04],\n",
       "                        [ 5.4964e-03,  3.9777e-03,  2.2393e-03,  1.6515e-04, -2.2525e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6427e-02,  1.1048e-02,  5.4980e-03, -2.6352e-03, -6.7588e-03],\n",
       "                        [ 4.1617e-03, -2.8152e-03, -9.2623e-03, -7.8285e-03, -5.7096e-03],\n",
       "                        [-4.6705e-03, -6.8196e-03, -6.8076e-03, -5.5677e-03, -8.6353e-03],\n",
       "                        [ 1.6363e-03,  3.4589e-03,  6.9929e-03,  2.0806e-03, -2.8984e-03],\n",
       "                        [ 6.1974e-03,  8.0558e-03,  5.9680e-03, -3.3774e-04, -6.5989e-03]]]]),\n",
       "              'exp_avg_sq': tensor([[[[0.0012, 0.0012, 0.0012, 0.0009, 0.0007],\n",
       "                        [0.0013, 0.0011, 0.0008, 0.0007, 0.0007],\n",
       "                        [0.0011, 0.0011, 0.0008, 0.0007, 0.0006],\n",
       "                        [0.0012, 0.0012, 0.0009, 0.0007, 0.0008],\n",
       "                        [0.0013, 0.0014, 0.0010, 0.0010, 0.0017]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.0009, 0.0007, 0.0007, 0.0006, 0.0007],\n",
       "                        [0.0009, 0.0007, 0.0006, 0.0006, 0.0009],\n",
       "                        [0.0009, 0.0007, 0.0007, 0.0010, 0.0012],\n",
       "                        [0.0009, 0.0008, 0.0012, 0.0013, 0.0010],\n",
       "                        [0.0011, 0.0012, 0.0017, 0.0014, 0.0010]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.0023, 0.0022, 0.0032, 0.0040, 0.0032],\n",
       "                        [0.0015, 0.0019, 0.0019, 0.0015, 0.0010],\n",
       "                        [0.0014, 0.0016, 0.0012, 0.0008, 0.0008],\n",
       "                        [0.0025, 0.0019, 0.0014, 0.0012, 0.0017],\n",
       "                        [0.0042, 0.0033, 0.0025, 0.0022, 0.0023]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.0006, 0.0006, 0.0007, 0.0008, 0.0007],\n",
       "                        [0.0006, 0.0007, 0.0008, 0.0007, 0.0007],\n",
       "                        [0.0005, 0.0006, 0.0006, 0.0005, 0.0005],\n",
       "                        [0.0004, 0.0004, 0.0004, 0.0004, 0.0005],\n",
       "                        [0.0004, 0.0003, 0.0003, 0.0004, 0.0006]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.0006, 0.0007, 0.0007, 0.0007, 0.0006],\n",
       "                        [0.0005, 0.0006, 0.0005, 0.0005, 0.0005],\n",
       "                        [0.0005, 0.0005, 0.0004, 0.0004, 0.0005],\n",
       "                        [0.0005, 0.0003, 0.0002, 0.0002, 0.0003],\n",
       "                        [0.0004, 0.0002, 0.0002, 0.0001, 0.0002]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.0012, 0.0018, 0.0020, 0.0019, 0.0017],\n",
       "                        [0.0014, 0.0021, 0.0022, 0.0016, 0.0013],\n",
       "                        [0.0025, 0.0024, 0.0018, 0.0011, 0.0009],\n",
       "                        [0.0025, 0.0015, 0.0009, 0.0009, 0.0009],\n",
       "                        [0.0017, 0.0008, 0.0007, 0.0010, 0.0010]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.0739,  0.1987, -0.1799, -0.0540, -0.1115, -0.1690],\n",
       "                    requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([-0.0002, -0.0092, -0.0054, -0.0060, -0.0087, -0.0227]),\n",
       "              'exp_avg_sq': tensor([0.0067, 0.0065, 0.0066, 0.0014, 0.0020, 0.0098])},\n",
       "             Parameter containing:\n",
       "             tensor([[[[-0.0775, -0.0344, -0.0742, -0.0899, -0.1116],\n",
       "                       [-0.1483, -0.1800, -0.1216, -0.0757, -0.1045],\n",
       "                       [-0.1028, -0.1627, -0.2306, -0.1794, -0.0288],\n",
       "                       [ 0.0393, -0.0648,  0.0230, -0.0284,  0.0176],\n",
       "                       [ 0.0207, -0.0008,  0.0946, -0.0137, -0.0333]],\n",
       "             \n",
       "                      [[-0.0134, -0.1059, -0.1136, -0.1254, -0.1191],\n",
       "                       [ 0.0546, -0.1181, -0.1817, -0.1033, -0.1354],\n",
       "                       [ 0.0692,  0.0016,  0.0127, -0.0067,  0.0193],\n",
       "                       [ 0.0578,  0.0395, -0.0701, -0.0420, -0.0784],\n",
       "                       [-0.0761, -0.1314, -0.0061,  0.0123,  0.0358]],\n",
       "             \n",
       "                      [[-0.0040,  0.0753,  0.1761,  0.1494,  0.1893],\n",
       "                       [ 0.0384, -0.0383, -0.0066, -0.0401,  0.0472],\n",
       "                       [ 0.1503,  0.0846,  0.1198,  0.0030, -0.0059],\n",
       "                       [-0.0066,  0.1324,  0.1985,  0.0602, -0.0394],\n",
       "                       [-0.0088,  0.0032,  0.0854,  0.0307,  0.0110]],\n",
       "             \n",
       "                      [[ 0.0129,  0.0646,  0.0822,  0.0829, -0.0166],\n",
       "                       [ 0.0206,  0.0658,  0.0374,  0.1240,  0.0231],\n",
       "                       [-0.0274,  0.0025, -0.0503, -0.0479, -0.0080],\n",
       "                       [-0.0403, -0.0162, -0.0313, -0.0834,  0.0177],\n",
       "                       [-0.0422,  0.0412,  0.0568,  0.0406, -0.0789]],\n",
       "             \n",
       "                      [[ 0.0304,  0.0384,  0.0604,  0.0184, -0.0406],\n",
       "                       [-0.0485,  0.0123, -0.0005, -0.0103,  0.0759],\n",
       "                       [ 0.0096, -0.0669, -0.1264, -0.0935,  0.0679],\n",
       "                       [-0.1274, -0.1166, -0.0746, -0.0354,  0.0238],\n",
       "                       [ 0.0651, -0.0364,  0.0231, -0.0801, -0.1091]],\n",
       "             \n",
       "                      [[ 0.0663,  0.0792,  0.0639,  0.0220,  0.0005],\n",
       "                       [ 0.0217,  0.1285,  0.0452,  0.1553,  0.1071],\n",
       "                       [-0.0934, -0.1185, -0.0820, -0.0814,  0.0022],\n",
       "                       [ 0.0412,  0.0156, -0.0584,  0.0120, -0.0104],\n",
       "                       [-0.0302,  0.0140,  0.0755, -0.0552, -0.0895]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0052, -0.0229, -0.0839, -0.1679, -0.1209],\n",
       "                       [-0.0166,  0.0109, -0.0710, -0.1619, -0.0310],\n",
       "                       [-0.0042,  0.0417, -0.0156, -0.1324, -0.0399],\n",
       "                       [-0.0718,  0.0522, -0.0100, -0.0312, -0.0813],\n",
       "                       [ 0.0188, -0.0425, -0.1090, -0.1260, -0.0991]],\n",
       "             \n",
       "                      [[-0.1241, -0.0365, -0.0102, -0.0177, -0.0907],\n",
       "                       [ 0.0383, -0.0031,  0.0699,  0.0754,  0.0043],\n",
       "                       [ 0.0309,  0.0435,  0.0404, -0.0650, -0.0110],\n",
       "                       [-0.1578, -0.1357, -0.1799, -0.1347, -0.0943],\n",
       "                       [-0.0065, -0.0192, -0.0721, -0.0904, -0.0426]],\n",
       "             \n",
       "                      [[-0.1372, -0.1148, -0.0976, -0.1132, -0.0133],\n",
       "                       [-0.0563, -0.0554,  0.0188, -0.0647,  0.0512],\n",
       "                       [ 0.0405,  0.0855,  0.0912,  0.1435, -0.0066],\n",
       "                       [ 0.1625,  0.2043,  0.0659,  0.0713,  0.0442],\n",
       "                       [-0.0966,  0.0217,  0.0125,  0.0171,  0.0007]],\n",
       "             \n",
       "                      [[ 0.0496,  0.0314,  0.0158,  0.0727, -0.0315],\n",
       "                       [-0.0394, -0.0296, -0.0842, -0.1043,  0.0003],\n",
       "                       [-0.0571, -0.1225, -0.1454, -0.0588,  0.0238],\n",
       "                       [ 0.0448, -0.0463, -0.0034,  0.0572, -0.0380],\n",
       "                       [ 0.1121,  0.1127,  0.1332,  0.0616, -0.0410]],\n",
       "             \n",
       "                      [[ 0.0611,  0.1250,  0.0805, -0.0359, -0.0737],\n",
       "                       [-0.0124, -0.0240, -0.0864, -0.1234, -0.0599],\n",
       "                       [-0.1346, -0.1692, -0.1405, -0.0445, -0.1319],\n",
       "                       [-0.0534,  0.0965,  0.0856,  0.0223, -0.0078],\n",
       "                       [ 0.1186,  0.1249, -0.0163, -0.0624, -0.0237]],\n",
       "             \n",
       "                      [[ 0.0909,  0.0542,  0.0162,  0.0302,  0.1035],\n",
       "                       [ 0.0134,  0.0094, -0.0817, -0.0728, -0.0052],\n",
       "                       [-0.0429, -0.1837, -0.1159,  0.0163, -0.1179],\n",
       "                       [-0.0697,  0.0411,  0.0914,  0.0121,  0.1048],\n",
       "                       [ 0.0264,  0.1305,  0.0430,  0.0620, -0.0384]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0383, -0.1256, -0.0136, -0.1148, -0.0627],\n",
       "                       [-0.0169,  0.0085, -0.0374, -0.0594, -0.1035],\n",
       "                       [-0.0086,  0.0294,  0.0323, -0.0064, -0.0379],\n",
       "                       [-0.0290,  0.1144,  0.0004, -0.0114, -0.0077],\n",
       "                       [-0.0449,  0.0949,  0.0115,  0.0118,  0.0147]],\n",
       "             \n",
       "                      [[ 0.0169, -0.0754, -0.0456,  0.0143, -0.1070],\n",
       "                       [-0.0739, -0.0451,  0.1127,  0.1027,  0.0613],\n",
       "                       [-0.1306, -0.1695, -0.0946,  0.0155, -0.0499],\n",
       "                       [-0.0251, -0.0614, -0.0547, -0.0999, -0.0064],\n",
       "                       [ 0.0784,  0.0459, -0.0747, -0.0016, -0.0577]],\n",
       "             \n",
       "                      [[ 0.0682, -0.0799, -0.0584, -0.0167, -0.0023],\n",
       "                       [ 0.0868,  0.0547,  0.1552,  0.0557,  0.0683],\n",
       "                       [ 0.1000,  0.1377,  0.1706,  0.1710,  0.1893],\n",
       "                       [-0.0722,  0.0729, -0.0289,  0.0175, -0.0291],\n",
       "                       [-0.0733, -0.0619,  0.0360, -0.0614, -0.0897]],\n",
       "             \n",
       "                      [[-0.0755,  0.0069, -0.0077, -0.0166,  0.1206],\n",
       "                       [ 0.0589, -0.1045, -0.0588, -0.0022, -0.0278],\n",
       "                       [ 0.0366, -0.0350, -0.0129, -0.0565, -0.0944],\n",
       "                       [ 0.0414,  0.0662,  0.0867, -0.0617, -0.0037],\n",
       "                       [-0.0375,  0.0518, -0.0204, -0.0087, -0.0089]],\n",
       "             \n",
       "                      [[-0.0177,  0.0474,  0.0297,  0.0542,  0.0402],\n",
       "                       [-0.0394, -0.0972, -0.1235, -0.0407, -0.0283],\n",
       "                       [-0.0162, -0.0004, -0.0675, -0.0275, -0.1055],\n",
       "                       [ 0.0527,  0.0150,  0.0188, -0.0404,  0.0527],\n",
       "                       [ 0.0237,  0.0727,  0.0553,  0.0590, -0.0770]],\n",
       "             \n",
       "                      [[ 0.0352,  0.0416,  0.0872,  0.1402,  0.0530],\n",
       "                       [-0.0284, -0.1485, -0.0897, -0.1360, -0.0063],\n",
       "                       [ 0.0380, -0.0600, -0.0930, -0.0359, -0.0157],\n",
       "                       [ 0.0724,  0.0340, -0.0730,  0.0224,  0.0453],\n",
       "                       [-0.0880, -0.0716, -0.0256, -0.0872, -0.0277]]],\n",
       "             \n",
       "             \n",
       "                     ...,\n",
       "             \n",
       "             \n",
       "                     [[[-0.0481, -0.1308, -0.0463, -0.0980, -0.0562],\n",
       "                       [ 0.0300, -0.0515, -0.0539, -0.0493, -0.1468],\n",
       "                       [-0.1075, -0.0917, -0.0666,  0.0437, -0.1355],\n",
       "                       [-0.0359, -0.0240,  0.1003,  0.0901, -0.1143],\n",
       "                       [-0.0091,  0.1547,  0.1691, -0.0304,  0.0262]],\n",
       "             \n",
       "                      [[-0.0537,  0.0547, -0.0519, -0.0485,  0.0161],\n",
       "                       [ 0.0503, -0.0748,  0.0343,  0.0312, -0.0044],\n",
       "                       [-0.0175, -0.0156, -0.0338,  0.0970,  0.1268],\n",
       "                       [ 0.0234, -0.0656, -0.0654, -0.0382,  0.0948],\n",
       "                       [-0.0608, -0.0884, -0.0455, -0.0393, -0.1015]],\n",
       "             \n",
       "                      [[-0.0396,  0.0641, -0.0462,  0.0159, -0.0080],\n",
       "                       [-0.0060, -0.0740,  0.0676,  0.0809, -0.0854],\n",
       "                       [-0.0028,  0.0550,  0.0795,  0.0303,  0.0221],\n",
       "                       [ 0.0874,  0.1792,  0.0051,  0.1086,  0.0372],\n",
       "                       [ 0.0402,  0.0805,  0.0181,  0.0367, -0.0235]],\n",
       "             \n",
       "                      [[-0.0182,  0.0594,  0.0679,  0.0802, -0.0294],\n",
       "                       [-0.1025, -0.0525, -0.0079,  0.0517,  0.0016],\n",
       "                       [-0.0246, -0.0319, -0.0566, -0.0764,  0.0136],\n",
       "                       [-0.0330, -0.0886, -0.1093, -0.1884, -0.0423],\n",
       "                       [ 0.0010, -0.0395, -0.0201, -0.0413,  0.0037]],\n",
       "             \n",
       "                      [[-0.0040, -0.0348, -0.0527,  0.0316,  0.0860],\n",
       "                       [ 0.0098,  0.0689, -0.0579, -0.0045, -0.0014],\n",
       "                       [ 0.0134, -0.0314, -0.0656, -0.0224, -0.0659],\n",
       "                       [ 0.0029, -0.1410, -0.0532, -0.1322, -0.1018],\n",
       "                       [-0.1165, -0.0037, -0.0349,  0.0725,  0.0603]],\n",
       "             \n",
       "                      [[ 0.0282, -0.0105, -0.0488, -0.0282,  0.0043],\n",
       "                       [ 0.0097, -0.0307,  0.0280, -0.0069,  0.0349],\n",
       "                       [-0.0633, -0.0884, -0.1057, -0.0897,  0.0646],\n",
       "                       [-0.0801, -0.0595, -0.1410, -0.1664, -0.1053],\n",
       "                       [-0.0142, -0.0459, -0.0465,  0.0839, -0.0607]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.1165,  0.0429, -0.0757, -0.0248, -0.0420],\n",
       "                       [ 0.1819,  0.0395, -0.0411,  0.0626,  0.0756],\n",
       "                       [ 0.0877,  0.0258, -0.0230,  0.0615,  0.0913],\n",
       "                       [ 0.0013,  0.0532, -0.0237,  0.0235,  0.0474],\n",
       "                       [ 0.0575,  0.0105, -0.0754, -0.0676, -0.0058]],\n",
       "             \n",
       "                      [[-0.0807, -0.0778, -0.0734, -0.0101,  0.0702],\n",
       "                       [-0.0473, -0.0937, -0.1244, -0.0012,  0.0726],\n",
       "                       [-0.1533,  0.0152,  0.0638,  0.0211,  0.0132],\n",
       "                       [-0.1386,  0.0107,  0.0971,  0.0267, -0.1639],\n",
       "                       [-0.1065, -0.0641,  0.0009, -0.0634, -0.2224]],\n",
       "             \n",
       "                      [[ 0.0112,  0.0319,  0.0915, -0.0550, -0.0647],\n",
       "                       [ 0.0636, -0.0263, -0.0605, -0.1126, -0.1442],\n",
       "                       [-0.0290,  0.0138, -0.1276, -0.0819, -0.0326],\n",
       "                       [-0.0368, -0.0842, -0.1578, -0.0415, -0.0659],\n",
       "                       [ 0.0149, -0.0171, -0.0567,  0.0462,  0.0901]],\n",
       "             \n",
       "                      [[-0.0397, -0.1608,  0.0117,  0.0681,  0.0661],\n",
       "                       [-0.0286,  0.0501, -0.0089,  0.1027, -0.1039],\n",
       "                       [-0.0312,  0.0688,  0.0727, -0.0321, -0.1349],\n",
       "                       [-0.0912, -0.0499, -0.0382,  0.0031, -0.1451],\n",
       "                       [-0.0610,  0.0252, -0.0594, -0.0342, -0.0950]],\n",
       "             \n",
       "                      [[-0.0734, -0.0972,  0.0677,  0.1542,  0.0970],\n",
       "                       [-0.0116,  0.0655,  0.1450,  0.0548, -0.0634],\n",
       "                       [-0.0789, -0.0367,  0.0492,  0.0807, -0.1109],\n",
       "                       [ 0.0129, -0.0365, -0.0254, -0.0570, -0.0162],\n",
       "                       [-0.0753,  0.0278, -0.0360, -0.0584, -0.0467]],\n",
       "             \n",
       "                      [[-0.0884, -0.0147,  0.1057,  0.1276, -0.0811],\n",
       "                       [ 0.0617,  0.1232,  0.0851,  0.1051, -0.1054],\n",
       "                       [ 0.0477,  0.0525,  0.0795,  0.0354, -0.1399],\n",
       "                       [-0.0457,  0.0892,  0.0334, -0.1462, -0.1325],\n",
       "                       [-0.0149, -0.0855, -0.0435, -0.0905, -0.1002]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.0110, -0.0449, -0.0451, -0.0294, -0.0219],\n",
       "                       [ 0.0074, -0.0140, -0.0167, -0.0364,  0.1564],\n",
       "                       [ 0.0814,  0.0359, -0.0529,  0.0557,  0.1492],\n",
       "                       [ 0.0315,  0.0518, -0.0137,  0.1968,  0.1403],\n",
       "                       [-0.0204, -0.0382,  0.0228,  0.0610, -0.0117]],\n",
       "             \n",
       "                      [[ 0.0827,  0.0570, -0.0810, -0.0103,  0.0139],\n",
       "                       [ 0.0577,  0.0286, -0.0462,  0.0237,  0.0468],\n",
       "                       [ 0.0637,  0.0694,  0.0315, -0.0728, -0.0353],\n",
       "                       [-0.1098, -0.0471, -0.0032, -0.0865, -0.1125],\n",
       "                       [-0.1517,  0.0334, -0.0630, -0.0023, -0.0357]],\n",
       "             \n",
       "                      [[-0.0798,  0.0587,  0.0978,  0.0285,  0.1472],\n",
       "                       [-0.0519,  0.0219,  0.0592,  0.0244,  0.0741],\n",
       "                       [ 0.1200,  0.0066,  0.1204,  0.0027,  0.0367],\n",
       "                       [ 0.1996,  0.2239,  0.1354,  0.0560,  0.1071],\n",
       "                       [ 0.0965,  0.0350,  0.0443,  0.0383,  0.0226]],\n",
       "             \n",
       "                      [[-0.0499,  0.0486,  0.0604,  0.0348, -0.0208],\n",
       "                       [-0.1011,  0.0393,  0.0090,  0.0886, -0.0234],\n",
       "                       [-0.1239, -0.0966, -0.0180, -0.0928, -0.0551],\n",
       "                       [ 0.0065,  0.0502, -0.0286,  0.0103,  0.0764],\n",
       "                       [ 0.0671,  0.0894,  0.0265,  0.0038,  0.0434]],\n",
       "             \n",
       "                      [[ 0.0133, -0.0294, -0.0606, -0.0487, -0.0380],\n",
       "                       [ 0.0060, -0.0851,  0.0102, -0.1135, -0.1227],\n",
       "                       [-0.1033, -0.1173, -0.0529, -0.1261, -0.0253],\n",
       "                       [-0.1247, -0.0935, -0.0631,  0.0387, -0.0299],\n",
       "                       [ 0.0588,  0.0327,  0.0673, -0.0779, -0.0277]],\n",
       "             \n",
       "                      [[-0.0720, -0.0644, -0.0363, -0.0795, -0.1244],\n",
       "                       [-0.1350, -0.0111,  0.0977, -0.0034, -0.1415],\n",
       "                       [-0.0947,  0.0343, -0.0121, -0.0449, -0.1344],\n",
       "                       [ 0.0033,  0.0251,  0.0409, -0.0254, -0.0326],\n",
       "                       [ 0.0290,  0.0840,  0.0665,  0.0540,  0.0514]]]], requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([[[[ 5.5335e-05,  1.0529e-03, -1.1935e-03,  1.0236e-03, -1.6795e-03],\n",
       "                        [-2.0719e-03, -2.2341e-03, -2.1424e-03,  5.7066e-04,  1.5046e-05],\n",
       "                        [-5.0707e-03, -3.7823e-03, -1.0251e-03,  8.5249e-04,  3.1563e-03],\n",
       "                        [-4.6808e-03, -4.0279e-03, -2.2061e-03, -6.0544e-05,  1.7753e-03],\n",
       "                        [-5.3572e-03, -5.5901e-03, -3.4885e-03, -2.5863e-03,  6.9101e-05]],\n",
       "              \n",
       "                       [[-2.7751e-03, -4.3405e-03, -5.3519e-03, -5.9386e-03, -3.3544e-03],\n",
       "                        [-7.7742e-04, -1.1122e-03, -5.9509e-04, -3.3497e-05,  1.8635e-03],\n",
       "                        [ 1.3000e-03,  3.0536e-04,  1.7638e-03,  1.4986e-03,  1.3630e-03],\n",
       "                        [ 1.5214e-03,  2.6226e-03,  2.1023e-03, -2.0561e-04, -3.4342e-04],\n",
       "                        [ 3.4612e-03,  3.1953e-03,  1.5728e-03,  5.3975e-04,  5.8198e-04]],\n",
       "              \n",
       "                       [[ 2.8303e-04,  1.3195e-03,  2.7462e-03,  2.4804e-03,  7.7198e-05],\n",
       "                        [ 1.0206e-03, -1.1405e-03, -2.9128e-03, -2.8405e-03, -1.6458e-03],\n",
       "                        [-3.7520e-03, -4.6702e-03, -4.4095e-03, -3.2343e-03, -1.4589e-03],\n",
       "                        [-3.7770e-03, -4.8390e-03, -4.9081e-03, -4.8202e-03, -3.7541e-03],\n",
       "                        [-3.9975e-03, -4.6750e-03, -4.3552e-03, -3.9469e-03, -3.8721e-03]],\n",
       "              \n",
       "                       [[ 3.8528e-04,  2.5231e-03,  4.2730e-03,  4.7953e-03,  2.9625e-03],\n",
       "                        [ 2.3335e-03,  5.6336e-03,  5.3255e-03,  2.8819e-03,  1.2918e-03],\n",
       "                        [ 6.1574e-03,  5.7192e-03,  3.9663e-03,  2.7273e-03,  1.6454e-03],\n",
       "                        [ 5.5438e-03,  4.4138e-03,  4.2544e-03,  4.3594e-03,  2.8902e-03],\n",
       "                        [ 2.8664e-03,  3.2793e-03,  3.5081e-03,  1.5618e-03,  2.3183e-03]],\n",
       "              \n",
       "                       [[ 2.5558e-03,  4.8164e-03,  4.5783e-03,  4.2904e-03,  3.1291e-03],\n",
       "                        [ 5.9922e-03,  6.9675e-03,  5.2084e-03,  3.0137e-03,  7.3050e-04],\n",
       "                        [ 7.4523e-03,  7.5891e-03,  3.9294e-03,  3.1431e-03,  4.1497e-04],\n",
       "                        [ 6.2321e-03,  6.3008e-03,  5.7989e-03,  4.4296e-03,  2.2350e-03],\n",
       "                        [ 4.4175e-03,  4.9052e-03,  3.1732e-03,  1.9032e-03,  2.3152e-03]],\n",
       "              \n",
       "                       [[ 4.0016e-03,  4.5512e-03,  4.1458e-03,  3.8256e-03,  3.3155e-03],\n",
       "                        [ 4.6832e-03,  3.7597e-03,  3.4172e-03,  3.0611e-03,  2.2058e-04],\n",
       "                        [ 6.5564e-03,  6.9287e-03,  2.5063e-03,  1.0695e-03, -1.8096e-03],\n",
       "                        [ 5.6448e-03,  2.6411e-03,  1.0364e-03,  1.4106e-03, -1.1204e-03],\n",
       "                        [ 3.2035e-03,  1.8756e-03,  1.9805e-03, -4.9807e-05, -4.2229e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8414e-03,  2.4853e-03,  1.2696e-03,  2.1791e-03,  4.3549e-03],\n",
       "                        [ 6.3452e-04, -1.3367e-03, -7.7404e-04,  1.1953e-03,  3.7881e-03],\n",
       "                        [-2.4029e-03, -1.4649e-03, -7.4910e-04,  3.8100e-04,  1.8599e-03],\n",
       "                        [-2.5183e-03, -2.0182e-03, -3.4194e-03, -3.8171e-03,  5.8038e-04],\n",
       "                        [-5.0490e-03, -5.7746e-03, -7.9980e-03, -5.4773e-03, -5.5298e-04]],\n",
       "              \n",
       "                       [[-1.1393e-03, -1.3627e-03, -2.6750e-03, -2.5360e-03, -1.2216e-03],\n",
       "                        [-1.8097e-03, -2.8833e-03, -3.7187e-03, -1.3458e-03, -2.2236e-03],\n",
       "                        [ 2.4258e-03,  1.3284e-03,  8.8277e-04, -1.0614e-04, -3.8825e-04],\n",
       "                        [ 6.1435e-04,  9.1176e-05, -1.0981e-03,  4.2795e-04, -1.3446e-04],\n",
       "                        [-1.2249e-05,  1.7725e-03,  2.8349e-03,  3.4448e-03,  2.9276e-03]],\n",
       "              \n",
       "                       [[ 3.9766e-03,  5.9792e-03,  6.6230e-03,  6.0806e-03,  5.0784e-03],\n",
       "                        [ 4.6353e-04, -7.0999e-04, -6.9616e-04, -1.4513e-03,  5.8416e-04],\n",
       "                        [ 1.3189e-03, -4.9482e-04, -4.1799e-04,  6.3132e-04,  7.0771e-04],\n",
       "                        [ 2.7441e-03,  6.8265e-04,  6.8746e-04, -1.0787e-03, -9.9700e-04],\n",
       "                        [ 1.9254e-03, -3.5336e-03, -5.1936e-03, -5.5372e-03, -7.1570e-03]],\n",
       "              \n",
       "                       [[-2.7902e-03, -1.7130e-03,  5.7420e-05,  6.4295e-04, -2.2982e-04],\n",
       "                        [-1.6198e-03,  2.4739e-03,  3.6319e-03,  3.0214e-03,  1.2845e-03],\n",
       "                        [-2.2853e-03,  1.4367e-03,  2.7542e-03,  3.7868e-03,  5.2763e-03],\n",
       "                        [-1.1429e-03,  2.8728e-03,  6.1273e-03,  7.6350e-03,  8.1977e-03],\n",
       "                        [ 3.6183e-04,  4.2402e-03,  7.3652e-03,  7.2011e-03,  7.0396e-03]],\n",
       "              \n",
       "                       [[-2.1844e-03, -3.2252e-03, -1.7018e-03, -1.9732e-03, -2.4099e-03],\n",
       "                        [-1.8423e-03, -1.7980e-04,  1.8356e-03,  5.6782e-04,  1.6045e-03],\n",
       "                        [-2.0783e-03,  7.2497e-04,  5.9596e-03,  5.4178e-03,  5.6619e-03],\n",
       "                        [ 1.0406e-03,  5.5266e-03,  8.9746e-03,  9.3158e-03,  9.1879e-03],\n",
       "                        [ 3.6551e-03,  8.0069e-03,  1.0551e-02,  8.6248e-03,  7.9018e-03]],\n",
       "              \n",
       "                       [[ 9.0377e-04, -9.0725e-04, -3.7867e-04, -1.3206e-03, -1.7270e-03],\n",
       "                        [ 2.2884e-03,  2.4190e-03,  2.9711e-03,  2.5148e-03,  3.9486e-03],\n",
       "                        [ 3.1683e-03,  2.8152e-03,  3.5456e-03,  2.8393e-03,  1.8473e-03],\n",
       "                        [ 4.4873e-03,  5.0757e-03,  5.9392e-03,  6.4686e-03,  7.0922e-03],\n",
       "                        [ 6.0747e-03,  6.5934e-03,  7.6620e-03,  6.0616e-03,  4.4839e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.8476e-03,  2.4071e-03,  3.0859e-03,  2.5681e-03,  1.1384e-03],\n",
       "                        [ 2.0783e-03,  2.5405e-04,  1.6131e-03,  1.5204e-03,  2.0544e-05],\n",
       "                        [ 2.4300e-03, -4.8206e-04,  5.8644e-04,  2.7514e-04,  7.6157e-04],\n",
       "                        [ 2.1553e-03, -5.2717e-04,  8.4440e-04,  2.2050e-04,  4.2931e-04],\n",
       "                        [ 6.5431e-04, -9.2690e-04, -6.4321e-04, -1.5392e-03, -4.8801e-03]],\n",
       "              \n",
       "                       [[-7.2447e-05, -8.5107e-04, -1.3538e-03, -1.7513e-03, -9.7124e-05],\n",
       "                        [-1.1674e-03, -8.4251e-04, -1.4465e-03, -2.0262e-03, -3.4257e-03],\n",
       "                        [-1.6040e-03,  1.5987e-03,  5.8481e-04,  5.9163e-04,  1.9976e-04],\n",
       "                        [ 7.9890e-04,  1.5767e-03, -6.0284e-04,  4.1439e-04, -5.6622e-04],\n",
       "                        [-6.8968e-04, -1.3823e-04, -1.6619e-04,  5.3619e-04,  6.8911e-04]],\n",
       "              \n",
       "                       [[ 3.0522e-03,  2.4428e-03,  3.0563e-03,  4.5407e-03,  3.3079e-03],\n",
       "                        [ 9.8503e-04, -7.5204e-04, -1.4247e-03, -5.2934e-04,  1.3091e-03],\n",
       "                        [ 1.1804e-03,  1.6120e-03,  9.9469e-04,  1.2197e-03,  1.4030e-03],\n",
       "                        [ 1.8660e-03,  1.1294e-03,  3.0133e-04,  3.0733e-04,  3.6571e-04],\n",
       "                        [ 1.7924e-03,  2.0248e-03,  8.5980e-04,  2.0332e-04, -2.7220e-03]],\n",
       "              \n",
       "                       [[-2.4537e-03, -9.8483e-04,  2.4635e-04,  2.4852e-04, -7.3110e-04],\n",
       "                        [ 9.6703e-04,  5.9276e-04,  2.1249e-03,  1.3564e-03,  2.3799e-04],\n",
       "                        [-4.4783e-04, -1.2087e-03,  1.2421e-03,  8.6003e-04,  2.5081e-04],\n",
       "                        [-9.4254e-04, -1.5032e-04,  1.2382e-03,  6.7236e-04,  2.3245e-03],\n",
       "                        [ 2.1936e-03,  8.0461e-04,  3.0667e-04,  3.5188e-04,  2.9782e-03]],\n",
       "              \n",
       "                       [[-2.0908e-03, -1.9704e-03, -8.1902e-04, -8.7359e-04, -3.2724e-03],\n",
       "                        [-5.8421e-04,  7.8533e-04, -6.2537e-04,  3.2073e-05, -4.3805e-04],\n",
       "                        [-2.0663e-03, -1.3068e-03, -3.8332e-04,  2.0371e-04,  5.5670e-04],\n",
       "                        [-5.7633e-04, -5.1446e-04,  5.5645e-05,  1.6887e-03,  4.0681e-03],\n",
       "                        [ 1.2974e-03, -7.7340e-05,  1.2326e-03,  1.9176e-03,  5.6109e-03]],\n",
       "              \n",
       "                       [[-4.4808e-04, -1.6636e-04,  2.7116e-04, -1.7044e-04, -1.2835e-03],\n",
       "                        [ 1.6328e-03,  2.7986e-03,  2.1611e-03,  1.0022e-03,  9.2404e-05],\n",
       "                        [ 7.7451e-04,  7.9441e-04, -9.7475e-04,  6.5828e-04,  1.4797e-03],\n",
       "                        [ 1.0431e-03,  5.5123e-04,  1.2166e-03,  2.7268e-03,  4.3474e-03],\n",
       "                        [ 7.3942e-04,  7.5437e-04,  1.2558e-03,  1.0480e-03,  4.0113e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 9.4370e-04, -5.3369e-04,  2.1539e-03,  1.6223e-03, -6.7980e-04],\n",
       "                        [ 6.3073e-04, -5.5215e-04,  3.3007e-04,  5.9580e-04, -1.9723e-03],\n",
       "                        [-7.9670e-04, -4.7231e-04,  7.6599e-04,  6.3066e-04, -1.2298e-03],\n",
       "                        [-5.4138e-04,  1.7344e-03,  1.3523e-03, -1.7232e-04, -4.5204e-04],\n",
       "                        [ 1.4479e-03,  2.4189e-03,  2.2946e-04, -2.1893e-03, -5.8200e-04]],\n",
       "              \n",
       "                       [[ 1.6329e-03,  1.1737e-03, -5.9447e-05, -3.3949e-04, -2.3474e-04],\n",
       "                        [ 1.4273e-03,  5.7544e-04,  4.6242e-04,  1.0065e-03,  2.1984e-03],\n",
       "                        [ 1.5171e-03,  1.3830e-03,  1.6533e-03,  1.9260e-03,  2.9123e-03],\n",
       "                        [ 2.4432e-03,  3.1988e-03,  2.9193e-03,  2.3379e-03,  1.9072e-03],\n",
       "                        [ 2.3795e-03,  2.3397e-03,  1.8521e-03,  2.1364e-03,  2.2058e-03]],\n",
       "              \n",
       "                       [[ 6.1908e-04,  1.1978e-03,  1.6211e-03,  2.2001e-03,  1.3385e-03],\n",
       "                        [ 8.1709e-04,  1.1549e-03,  2.0366e-04,  9.1891e-05, -5.5116e-04],\n",
       "                        [ 1.2152e-03, -1.7446e-04,  2.0356e-04,  8.1734e-04,  1.2317e-03],\n",
       "                        [-1.2564e-03, -1.5729e-03, -1.0013e-03, -1.2760e-03, -8.7799e-04],\n",
       "                        [ 1.0356e-03,  6.2498e-04,  7.1227e-04, -6.4073e-04, -1.7561e-03]],\n",
       "              \n",
       "                       [[-5.8117e-04,  6.3222e-05,  1.3023e-03,  1.1432e-03,  1.5417e-03],\n",
       "                        [-3.5327e-04,  2.2049e-03,  2.7582e-03,  2.3177e-03,  1.0293e-03],\n",
       "                        [ 1.9471e-03,  2.8040e-03,  1.4822e-03,  8.5160e-04,  1.3869e-03],\n",
       "                        [ 1.4492e-03,  8.2454e-04,  1.4477e-04,  1.3853e-03,  3.2435e-03],\n",
       "                        [-3.8299e-04, -3.7720e-04, -2.0793e-05,  7.0107e-04,  1.8473e-03]],\n",
       "              \n",
       "                       [[-7.2053e-04,  3.6821e-04,  9.8573e-04,  1.1702e-03,  2.1517e-03],\n",
       "                        [ 1.3585e-03,  2.2833e-03,  2.5997e-03,  1.7394e-03,  9.8397e-04],\n",
       "                        [ 2.0423e-03,  1.7918e-03,  1.3274e-03,  1.4969e-03,  2.6545e-03],\n",
       "                        [ 4.4369e-04,  9.5714e-05,  1.2678e-03,  2.9285e-03,  3.5280e-03],\n",
       "                        [-7.3513e-04, -9.8875e-04,  7.0489e-04,  1.8973e-03,  1.9554e-03]],\n",
       "              \n",
       "                       [[ 5.0282e-04,  5.6193e-04,  1.1441e-03,  1.3050e-03,  2.9936e-03],\n",
       "                        [ 2.3810e-03,  2.4874e-03,  1.9183e-03,  1.6757e-03,  1.8689e-03],\n",
       "                        [ 2.5501e-03,  1.7354e-03,  2.2459e-03,  2.4124e-03,  2.6958e-03],\n",
       "                        [ 1.6447e-03,  1.0696e-03,  6.7362e-04,  2.7427e-03,  2.9068e-03],\n",
       "                        [ 3.6788e-04, -6.1457e-05,  1.1433e-03,  1.8895e-03,  1.5937e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7798e-03, -2.4643e-03, -7.3399e-04,  2.0291e-03,  1.3939e-03],\n",
       "                        [-1.2289e-03, -1.3415e-03,  1.9764e-03,  2.9211e-03,  2.1458e-03],\n",
       "                        [-8.6548e-04, -1.7245e-03,  2.1219e-03,  1.3335e-03,  7.2768e-04],\n",
       "                        [-3.9652e-04, -1.1008e-03,  3.1808e-03,  3.5390e-03,  2.1869e-03],\n",
       "                        [ 8.8434e-04,  1.8724e-03,  5.6539e-03,  4.9756e-03,  3.3614e-03]],\n",
       "              \n",
       "                       [[ 2.0596e-03,  1.7188e-03,  5.0383e-04, -8.3000e-05,  4.3464e-04],\n",
       "                        [ 3.7048e-03,  3.3164e-03,  4.2043e-03,  2.1933e-03,  1.3955e-03],\n",
       "                        [-2.6140e-04, -5.1116e-04, -7.5687e-04,  2.7837e-05,  1.0945e-05],\n",
       "                        [-1.2741e-03, -1.1837e-03, -1.5314e-03, -2.5684e-04, -8.1788e-04],\n",
       "                        [ 5.1472e-04, -2.0837e-04, -1.0596e-03, -2.0040e-03, -2.0364e-03]],\n",
       "              \n",
       "                       [[-3.4978e-03, -3.2632e-03, -4.2756e-03, -2.8854e-03, -2.3540e-04],\n",
       "                        [-2.4007e-04, -8.5336e-04, -2.0324e-04, -5.0796e-04, -8.4834e-05],\n",
       "                        [ 3.1603e-03,  3.6670e-03,  4.0080e-03,  3.2535e-03,  7.0650e-04],\n",
       "                        [ 1.4662e-03,  1.8282e-03,  1.9152e-03,  3.7793e-03,  5.1135e-03],\n",
       "                        [ 4.1021e-04,  2.4511e-04,  1.3252e-03,  2.6577e-03,  3.2317e-03]],\n",
       "              \n",
       "                       [[-8.7084e-04, -3.5208e-04,  8.8809e-06, -6.1616e-04, -2.7581e-03],\n",
       "                        [-2.2712e-03, -2.6730e-03, -1.8947e-03, -1.4835e-03, -1.6851e-03],\n",
       "                        [-1.2025e-03, -1.1223e-03, -6.6542e-04, -1.0234e-03, -2.0593e-03],\n",
       "                        [-3.2402e-04, -2.1171e-04,  7.2016e-05, -2.0117e-03, -3.9721e-03],\n",
       "                        [-9.5203e-04, -1.2559e-03, -1.6437e-03, -2.1314e-03, -2.5025e-03]],\n",
       "              \n",
       "                       [[ 1.3065e-04, -1.1297e-03, -2.2900e-03, -1.9094e-03, -2.3686e-03],\n",
       "                        [-2.1176e-03, -2.6558e-03, -2.6444e-03, -1.6686e-03, -3.2927e-03],\n",
       "                        [-7.6447e-04, -1.5860e-03, -2.7358e-03, -1.5169e-03, -4.3957e-03],\n",
       "                        [-3.7608e-04, -2.5733e-03, -2.4768e-03, -3.6249e-03, -5.4741e-03],\n",
       "                        [-1.7206e-03, -3.3214e-03, -4.4581e-03, -3.7633e-03, -3.9074e-03]],\n",
       "              \n",
       "                       [[-1.8340e-03, -9.8772e-04, -1.4463e-03, -1.6190e-03, -7.9466e-05],\n",
       "                        [-2.1907e-03, -2.7452e-03, -3.6442e-03, -2.8543e-03, -2.9262e-03],\n",
       "                        [-6.3188e-04, -2.1764e-03, -2.5270e-03, -1.1611e-03, -2.4592e-03],\n",
       "                        [ 1.3680e-03,  7.0057e-05, -7.0553e-04, -4.7901e-04, -1.3176e-03],\n",
       "                        [-9.9208e-04, -1.6782e-03, -1.9597e-03, -1.9075e-03, -5.4125e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.0436e-03,  5.1256e-04, -7.3771e-05,  1.5025e-05, -1.8984e-03],\n",
       "                        [-3.2859e-04, -1.0035e-03, -2.0125e-03, -4.4962e-04, -1.4897e-03],\n",
       "                        [-6.7602e-04,  1.3155e-03,  9.7491e-04, -4.2459e-04, -3.3640e-05],\n",
       "                        [ 2.4202e-04,  1.4456e-03,  1.6097e-03,  1.7147e-04, -1.0842e-03],\n",
       "                        [-7.2663e-04, -3.2854e-05, -7.6811e-06, -1.6304e-04, -1.6447e-03]],\n",
       "              \n",
       "                       [[-1.2093e-03, -7.5006e-04, -7.1773e-04, -1.0385e-03, -9.3949e-04],\n",
       "                        [-1.7774e-03, -1.0247e-03,  1.9845e-04,  6.8310e-04,  1.4288e-03],\n",
       "                        [ 2.3872e-03,  1.9739e-03,  2.0490e-03,  2.4671e-03,  2.1236e-03],\n",
       "                        [ 2.1297e-03, -1.0707e-04, -1.6922e-04, -5.7630e-04,  1.3375e-04],\n",
       "                        [ 7.9856e-05,  1.0659e-04,  2.0706e-04,  1.0845e-03,  2.3700e-03]],\n",
       "              \n",
       "                       [[ 1.1559e-03,  1.4016e-03,  8.9014e-04,  5.4795e-05,  8.7396e-04],\n",
       "                        [ 1.6180e-04,  1.5750e-03,  6.2574e-04, -4.4994e-04, -1.1798e-03],\n",
       "                        [-1.7386e-03,  3.0438e-04,  2.2033e-03,  2.0870e-03,  1.0929e-03],\n",
       "                        [ 1.4921e-03,  1.3107e-03,  1.6191e-03,  5.3122e-04,  1.9264e-04],\n",
       "                        [ 1.2403e-03, -2.1836e-04, -2.3529e-04, -4.6612e-04, -1.1352e-03]],\n",
       "              \n",
       "                       [[ 9.2129e-04,  1.0951e-03,  1.6249e-03,  2.6074e-03,  1.7082e-03],\n",
       "                        [ 1.9564e-03,  1.8731e-03,  9.2620e-04,  7.5927e-05,  4.1168e-05],\n",
       "                        [-4.2792e-05,  7.1870e-04, -5.0248e-04, -1.1186e-03, -5.3589e-04],\n",
       "                        [-4.6594e-04,  8.7426e-04,  7.2673e-04,  5.0097e-04,  1.2114e-03],\n",
       "                        [ 8.0630e-04,  1.2422e-04, -3.9450e-04, -6.6151e-04, -8.1532e-06]],\n",
       "              \n",
       "                       [[ 7.6295e-04,  1.0046e-03,  1.8671e-03,  1.7781e-03,  1.6554e-03],\n",
       "                        [ 1.3922e-03,  3.1752e-04, -7.5177e-04, -4.1568e-05,  3.8452e-04],\n",
       "                        [-1.0135e-03, -1.8678e-03, -1.5528e-03, -1.2475e-03,  9.1458e-04],\n",
       "                        [ 1.3110e-04,  4.7156e-04,  4.8108e-04,  1.5864e-03,  2.3336e-03],\n",
       "                        [-8.5126e-04, -1.0636e-03, -8.0243e-04, -9.9494e-04,  3.6720e-04]],\n",
       "              \n",
       "                       [[ 1.4183e-03,  1.3165e-03,  1.1911e-03,  9.8766e-04,  2.2124e-03],\n",
       "                        [ 1.1504e-03,  6.4886e-04,  7.1696e-05,  1.3086e-03,  1.2662e-03],\n",
       "                        [ 8.8927e-06, -6.4432e-04, -7.6812e-04, -7.4043e-04,  1.0080e-03],\n",
       "                        [ 2.8378e-04, -7.2937e-04, -5.1698e-05,  2.1872e-03,  2.7800e-03],\n",
       "                        [ 3.2800e-05, -3.8323e-04,  1.1845e-03,  1.8999e-03,  1.4834e-03]]]]),\n",
       "              'exp_avg_sq': tensor([[[[6.3681e-05, 6.6763e-05, 6.1951e-05, 6.1759e-05, 5.5292e-05],\n",
       "                        [4.9622e-05, 6.3931e-05, 9.8073e-05, 8.2350e-05, 6.7450e-05],\n",
       "                        [6.9640e-05, 9.5241e-05, 1.1687e-04, 8.9469e-05, 7.2548e-05],\n",
       "                        [8.1958e-05, 1.0540e-04, 7.8504e-05, 8.6707e-05, 6.8712e-05],\n",
       "                        [9.2452e-05, 1.2209e-04, 8.5137e-05, 9.4167e-05, 7.0632e-05]],\n",
       "              \n",
       "                       [[5.2235e-05, 6.0905e-05, 7.3321e-05, 8.2089e-05, 8.7727e-05],\n",
       "                        [6.0240e-05, 6.9461e-05, 8.6003e-05, 8.9034e-05, 9.0766e-05],\n",
       "                        [7.8284e-05, 1.2442e-04, 1.7928e-04, 1.7376e-04, 1.5712e-04],\n",
       "                        [9.4536e-05, 1.4491e-04, 1.6942e-04, 1.8214e-04, 1.6750e-04],\n",
       "                        [1.1117e-04, 1.2993e-04, 1.2192e-04, 1.3978e-04, 1.3308e-04]],\n",
       "              \n",
       "                       [[5.8984e-05, 6.9527e-05, 9.7120e-05, 1.0848e-04, 9.9355e-05],\n",
       "                        [5.0421e-05, 5.1980e-05, 8.0489e-05, 8.4002e-05, 6.8114e-05],\n",
       "                        [6.7667e-05, 8.4066e-05, 9.9383e-05, 1.0252e-04, 7.6738e-05],\n",
       "                        [7.2479e-05, 9.4079e-05, 8.0166e-05, 8.8198e-05, 7.5142e-05],\n",
       "                        [7.8935e-05, 1.0833e-04, 1.1875e-04, 1.0675e-04, 8.4568e-05]],\n",
       "              \n",
       "                       [[4.3447e-05, 5.4503e-05, 9.6074e-05, 1.4750e-04, 1.3430e-04],\n",
       "                        [5.5631e-05, 1.0411e-04, 1.6573e-04, 1.8864e-04, 1.5333e-04],\n",
       "                        [9.3788e-05, 1.2006e-04, 1.2651e-04, 1.0902e-04, 1.0107e-04],\n",
       "                        [9.8344e-05, 1.0538e-04, 1.2569e-04, 1.1327e-04, 1.1537e-04],\n",
       "                        [9.3267e-05, 1.3212e-04, 1.6889e-04, 1.4652e-04, 1.1336e-04]],\n",
       "              \n",
       "                       [[8.9731e-05, 1.2065e-04, 2.0837e-04, 2.3404e-04, 1.9987e-04],\n",
       "                        [1.1156e-04, 1.9432e-04, 2.4884e-04, 2.6022e-04, 2.2027e-04],\n",
       "                        [1.6871e-04, 2.1768e-04, 2.1322e-04, 2.2454e-04, 1.9161e-04],\n",
       "                        [1.9007e-04, 2.2760e-04, 2.6353e-04, 2.7060e-04, 2.0165e-04],\n",
       "                        [2.1130e-04, 2.6001e-04, 2.9029e-04, 2.5069e-04, 1.7404e-04]],\n",
       "              \n",
       "                       [[5.7418e-05, 1.0852e-04, 1.5202e-04, 1.2821e-04, 9.4819e-05],\n",
       "                        [9.1514e-05, 1.4074e-04, 1.6914e-04, 1.7691e-04, 1.2869e-04],\n",
       "                        [1.1116e-04, 1.2968e-04, 1.0103e-04, 9.1832e-05, 5.6918e-05],\n",
       "                        [1.1759e-04, 1.0948e-04, 8.0990e-05, 7.6906e-05, 5.6385e-05],\n",
       "                        [1.0582e-04, 1.1383e-04, 1.1063e-04, 8.3112e-05, 5.9638e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0723e-04, 9.1158e-05, 9.2916e-05, 9.3408e-05, 8.5435e-05],\n",
       "                        [1.0368e-04, 1.0273e-04, 1.1298e-04, 1.1279e-04, 9.0591e-05],\n",
       "                        [1.0159e-04, 9.9646e-05, 1.0726e-04, 9.7852e-05, 9.2296e-05],\n",
       "                        [8.3177e-05, 7.7431e-05, 8.5810e-05, 1.2275e-04, 1.4183e-04],\n",
       "                        [7.4301e-05, 6.9310e-05, 1.2808e-04, 1.5254e-04, 1.2526e-04]],\n",
       "              \n",
       "                       [[5.0407e-05, 5.6189e-05, 6.0341e-05, 7.8298e-05, 7.7417e-05],\n",
       "                        [5.5734e-05, 7.5985e-05, 1.0526e-04, 1.1700e-04, 1.0201e-04],\n",
       "                        [8.0269e-05, 9.5041e-05, 1.1915e-04, 1.3202e-04, 1.4156e-04],\n",
       "                        [6.3391e-05, 6.1573e-05, 7.1995e-05, 1.1072e-04, 1.4259e-04],\n",
       "                        [6.4960e-05, 6.8058e-05, 1.1920e-04, 1.7126e-04, 2.3193e-04]],\n",
       "              \n",
       "                       [[7.4288e-05, 7.3183e-05, 7.0702e-05, 7.3666e-05, 7.4230e-05],\n",
       "                        [5.8883e-05, 5.4811e-05, 5.9998e-05, 6.9368e-05, 6.9102e-05],\n",
       "                        [5.7922e-05, 5.7280e-05, 5.4795e-05, 5.6492e-05, 6.5426e-05],\n",
       "                        [6.1149e-05, 5.9799e-05, 6.8210e-05, 8.5688e-05, 1.0693e-04],\n",
       "                        [6.9748e-05, 5.8161e-05, 8.6031e-05, 1.1099e-04, 1.1662e-04]],\n",
       "              \n",
       "                       [[6.7345e-05, 6.4887e-05, 7.6218e-05, 1.0291e-04, 1.1305e-04],\n",
       "                        [5.6318e-05, 5.2540e-05, 5.0772e-05, 5.8913e-05, 8.6800e-05],\n",
       "                        [6.1416e-05, 3.9633e-05, 3.2180e-05, 6.4534e-05, 1.6581e-04],\n",
       "                        [7.7962e-05, 6.8922e-05, 1.0600e-04, 1.9451e-04, 2.3074e-04],\n",
       "                        [7.3220e-05, 1.2021e-04, 1.8708e-04, 2.0011e-04, 1.5779e-04]],\n",
       "              \n",
       "                       [[1.4646e-04, 1.4898e-04, 1.7677e-04, 1.9628e-04, 1.9608e-04],\n",
       "                        [1.3317e-04, 1.2845e-04, 1.2989e-04, 1.5671e-04, 2.1048e-04],\n",
       "                        [1.4541e-04, 1.0241e-04, 1.2676e-04, 2.4366e-04, 3.2057e-04],\n",
       "                        [1.6906e-04, 1.5917e-04, 2.6019e-04, 3.3683e-04, 3.5615e-04],\n",
       "                        [1.7195e-04, 2.4407e-04, 2.8705e-04, 2.7788e-04, 2.5471e-04]],\n",
       "              \n",
       "                       [[5.8564e-05, 6.4084e-05, 8.8598e-05, 1.0115e-04, 9.8809e-05],\n",
       "                        [4.5720e-05, 4.6905e-05, 5.9959e-05, 7.9563e-05, 1.0395e-04],\n",
       "                        [4.6331e-05, 3.8891e-05, 6.1706e-05, 1.1549e-04, 1.1996e-04],\n",
       "                        [7.6061e-05, 1.0484e-04, 1.6192e-04, 1.7148e-04, 1.8114e-04],\n",
       "                        [8.9394e-05, 1.4710e-04, 1.6269e-04, 1.6550e-04, 1.2375e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.2402e-04, 1.0491e-04, 1.0144e-04, 9.1664e-05, 7.6773e-05],\n",
       "                        [1.2733e-04, 1.0640e-04, 9.0001e-05, 8.0268e-05, 6.7526e-05],\n",
       "                        [9.3474e-05, 8.0552e-05, 6.6323e-05, 6.3066e-05, 5.8395e-05],\n",
       "                        [9.0374e-05, 6.7816e-05, 6.2900e-05, 4.9374e-05, 5.8811e-05],\n",
       "                        [9.6630e-05, 7.2633e-05, 5.6700e-05, 5.3757e-05, 7.8561e-05]],\n",
       "              \n",
       "                       [[5.0848e-05, 5.5908e-05, 6.8250e-05, 8.4235e-05, 9.5444e-05],\n",
       "                        [4.4345e-05, 6.6669e-05, 1.0241e-04, 1.4402e-04, 1.4723e-04],\n",
       "                        [4.6507e-05, 6.2862e-05, 8.1789e-05, 1.2181e-04, 1.2878e-04],\n",
       "                        [5.8446e-05, 6.1881e-05, 6.9842e-05, 8.9908e-05, 1.0917e-04],\n",
       "                        [9.8140e-05, 1.0001e-04, 9.0820e-05, 1.0759e-04, 1.2686e-04]],\n",
       "              \n",
       "                       [[6.5717e-05, 7.4927e-05, 7.7638e-05, 8.1037e-05, 8.2904e-05],\n",
       "                        [5.3324e-05, 6.8707e-05, 5.9313e-05, 5.7162e-05, 5.9730e-05],\n",
       "                        [4.3517e-05, 5.7088e-05, 6.5534e-05, 6.5589e-05, 5.8939e-05],\n",
       "                        [5.8834e-05, 5.7189e-05, 5.4558e-05, 5.6438e-05, 5.5887e-05],\n",
       "                        [7.1212e-05, 7.2992e-05, 5.6303e-05, 5.0439e-05, 5.4371e-05]],\n",
       "              \n",
       "                       [[4.9499e-05, 6.4908e-05, 7.9364e-05, 8.6938e-05, 9.8455e-05],\n",
       "                        [3.7871e-05, 4.5195e-05, 4.9029e-05, 4.5064e-05, 5.1471e-05],\n",
       "                        [6.2138e-05, 7.1952e-05, 5.5993e-05, 4.1866e-05, 4.7573e-05],\n",
       "                        [7.7260e-05, 8.9642e-05, 8.0836e-05, 7.1546e-05, 8.3319e-05],\n",
       "                        [5.1688e-05, 6.4999e-05, 6.8687e-05, 7.4309e-05, 7.9702e-05]],\n",
       "              \n",
       "                       [[1.2531e-04, 1.5873e-04, 1.8235e-04, 1.9556e-04, 1.8192e-04],\n",
       "                        [9.9647e-05, 1.2093e-04, 1.3097e-04, 1.3033e-04, 1.2816e-04],\n",
       "                        [1.3388e-04, 1.3726e-04, 1.1529e-04, 9.9737e-05, 1.1274e-04],\n",
       "                        [1.6524e-04, 1.6124e-04, 1.3717e-04, 1.2648e-04, 1.4210e-04],\n",
       "                        [1.2907e-04, 1.3423e-04, 1.2797e-04, 1.2050e-04, 1.1958e-04]],\n",
       "              \n",
       "                       [[6.3557e-05, 7.2135e-05, 8.4681e-05, 1.0065e-04, 8.3886e-05],\n",
       "                        [4.4468e-05, 4.4199e-05, 4.3819e-05, 4.5355e-05, 5.2939e-05],\n",
       "                        [6.1416e-05, 5.3298e-05, 4.3934e-05, 4.3705e-05, 5.8798e-05],\n",
       "                        [7.5685e-05, 7.2804e-05, 6.9635e-05, 9.1849e-05, 1.0361e-04],\n",
       "                        [5.1064e-05, 4.8285e-05, 6.4756e-05, 6.9737e-05, 7.6619e-05]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[3.5624e-05, 3.7710e-05, 4.0333e-05, 3.5467e-05, 2.7674e-05],\n",
       "                        [3.6440e-05, 4.0720e-05, 4.2986e-05, 3.7099e-05, 3.1644e-05],\n",
       "                        [3.6815e-05, 3.7042e-05, 4.1655e-05, 3.5952e-05, 3.2821e-05],\n",
       "                        [3.2743e-05, 3.8112e-05, 5.3725e-05, 3.2432e-05, 3.3430e-05],\n",
       "                        [3.2212e-05, 4.4432e-05, 4.2707e-05, 2.9132e-05, 3.6748e-05]],\n",
       "              \n",
       "                       [[2.9597e-05, 3.2680e-05, 3.2189e-05, 3.1746e-05, 3.1093e-05],\n",
       "                        [3.8313e-05, 4.3718e-05, 4.4079e-05, 4.3337e-05, 4.6031e-05],\n",
       "                        [7.2539e-05, 8.0957e-05, 7.7342e-05, 8.0816e-05, 8.2450e-05],\n",
       "                        [8.7440e-05, 8.2081e-05, 7.3566e-05, 6.9668e-05, 7.5795e-05],\n",
       "                        [6.4480e-05, 4.5109e-05, 3.6252e-05, 4.1375e-05, 5.3326e-05]],\n",
       "              \n",
       "                       [[1.8212e-05, 2.3556e-05, 2.9291e-05, 3.2231e-05, 2.7693e-05],\n",
       "                        [2.2583e-05, 2.5550e-05, 2.7122e-05, 2.6162e-05, 2.5398e-05],\n",
       "                        [2.3693e-05, 2.4193e-05, 2.3444e-05, 2.1586e-05, 2.4067e-05],\n",
       "                        [1.9069e-05, 2.0476e-05, 2.3250e-05, 2.1798e-05, 2.2484e-05],\n",
       "                        [2.3173e-05, 2.1594e-05, 2.4631e-05, 2.7654e-05, 2.8002e-05]],\n",
       "              \n",
       "                       [[2.8901e-05, 3.8927e-05, 4.3587e-05, 4.2186e-05, 3.9137e-05],\n",
       "                        [3.8211e-05, 5.1875e-05, 5.0286e-05, 3.9103e-05, 3.4406e-05],\n",
       "                        [3.7473e-05, 3.5882e-05, 1.9477e-05, 1.1881e-05, 1.8747e-05],\n",
       "                        [2.6663e-05, 2.0639e-05, 1.2498e-05, 1.2751e-05, 2.8511e-05],\n",
       "                        [2.4184e-05, 2.6206e-05, 2.7183e-05, 3.5012e-05, 4.8975e-05]],\n",
       "              \n",
       "                       [[5.6086e-05, 6.4448e-05, 6.6576e-05, 6.0433e-05, 5.9055e-05],\n",
       "                        [7.0689e-05, 7.6758e-05, 6.4545e-05, 5.3115e-05, 5.5159e-05],\n",
       "                        [6.2943e-05, 4.6786e-05, 3.4286e-05, 3.2760e-05, 4.8405e-05],\n",
       "                        [4.6745e-05, 3.5628e-05, 3.4682e-05, 4.2886e-05, 6.4586e-05],\n",
       "                        [4.3263e-05, 4.8637e-05, 5.3027e-05, 5.8372e-05, 6.6227e-05]],\n",
       "              \n",
       "                       [[3.1314e-05, 3.3726e-05, 3.1929e-05, 3.1394e-05, 3.5688e-05],\n",
       "                        [4.1334e-05, 3.7892e-05, 3.2344e-05, 3.3129e-05, 3.6241e-05],\n",
       "                        [2.6792e-05, 1.6623e-05, 1.6271e-05, 2.0653e-05, 2.5222e-05],\n",
       "                        [1.8443e-05, 1.4757e-05, 1.5382e-05, 2.2168e-05, 2.8455e-05],\n",
       "                        [1.9763e-05, 2.0822e-05, 3.0528e-05, 3.9886e-05, 3.5613e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0905e-04, 8.9268e-05, 7.8293e-05, 7.0065e-05, 5.8358e-05],\n",
       "                        [8.4535e-05, 7.2869e-05, 7.5490e-05, 6.5393e-05, 6.9289e-05],\n",
       "                        [7.5996e-05, 7.2572e-05, 7.1713e-05, 6.5146e-05, 7.3391e-05],\n",
       "                        [7.4418e-05, 7.0808e-05, 6.3553e-05, 7.3148e-05, 7.2640e-05],\n",
       "                        [6.7145e-05, 5.7175e-05, 7.0961e-05, 8.2514e-05, 6.9269e-05]],\n",
       "              \n",
       "                       [[4.0572e-05, 4.3766e-05, 4.2053e-05, 3.4894e-05, 2.9623e-05],\n",
       "                        [4.2567e-05, 4.6776e-05, 4.7714e-05, 3.5821e-05, 3.0096e-05],\n",
       "                        [5.5060e-05, 6.4783e-05, 6.7451e-05, 5.3028e-05, 3.7066e-05],\n",
       "                        [8.1891e-05, 1.0644e-04, 1.0263e-04, 7.8589e-05, 4.6182e-05],\n",
       "                        [9.0520e-05, 1.0927e-04, 1.0324e-04, 7.3983e-05, 4.3659e-05]],\n",
       "              \n",
       "                       [[7.8310e-05, 8.4071e-05, 8.9444e-05, 8.3419e-05, 5.9965e-05],\n",
       "                        [6.0850e-05, 6.8789e-05, 7.5179e-05, 7.9352e-05, 5.2417e-05],\n",
       "                        [4.3730e-05, 5.6727e-05, 6.2951e-05, 5.2320e-05, 3.4270e-05],\n",
       "                        [4.9908e-05, 5.6121e-05, 5.8894e-05, 4.8619e-05, 3.1440e-05],\n",
       "                        [5.1934e-05, 5.0643e-05, 4.8737e-05, 4.0872e-05, 3.1255e-05]],\n",
       "              \n",
       "                       [[3.7892e-05, 4.1101e-05, 5.4998e-05, 5.3257e-05, 3.3307e-05],\n",
       "                        [6.3088e-05, 6.5937e-05, 8.5406e-05, 7.5670e-05, 3.9903e-05],\n",
       "                        [7.2744e-05, 7.8547e-05, 9.2986e-05, 6.4618e-05, 3.3068e-05],\n",
       "                        [7.6928e-05, 6.7222e-05, 6.5657e-05, 4.6996e-05, 2.8352e-05],\n",
       "                        [7.5003e-05, 6.6383e-05, 5.5136e-05, 3.8954e-05, 2.5324e-05]],\n",
       "              \n",
       "                       [[8.3861e-05, 1.0712e-04, 1.0254e-04, 8.3616e-05, 5.8973e-05],\n",
       "                        [9.1465e-05, 1.1123e-04, 1.0529e-04, 8.7130e-05, 5.3172e-05],\n",
       "                        [1.1213e-04, 1.2667e-04, 1.1361e-04, 8.1348e-05, 5.0137e-05],\n",
       "                        [1.1789e-04, 1.1830e-04, 1.0009e-04, 7.4252e-05, 5.1239e-05],\n",
       "                        [1.2825e-04, 1.2608e-04, 8.2098e-05, 5.9513e-05, 4.6971e-05]],\n",
       "              \n",
       "                       [[3.9078e-05, 5.4805e-05, 4.4899e-05, 3.0209e-05, 2.4487e-05],\n",
       "                        [6.4056e-05, 8.2583e-05, 6.4919e-05, 4.7484e-05, 2.8405e-05],\n",
       "                        [6.9915e-05, 8.0493e-05, 5.6704e-05, 3.4378e-05, 2.3504e-05],\n",
       "                        [5.7182e-05, 5.7746e-05, 3.4932e-05, 2.5875e-05, 2.4596e-05],\n",
       "                        [5.0161e-05, 4.3760e-05, 3.2717e-05, 2.2293e-05, 2.2182e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[4.5656e-05, 4.5184e-05, 4.6607e-05, 4.4744e-05, 4.2182e-05],\n",
       "                        [5.1526e-05, 4.6347e-05, 4.7616e-05, 4.6503e-05, 4.8808e-05],\n",
       "                        [4.5115e-05, 3.4174e-05, 3.9275e-05, 4.1180e-05, 5.3562e-05],\n",
       "                        [5.0453e-05, 3.2718e-05, 3.2294e-05, 3.6583e-05, 4.9724e-05],\n",
       "                        [4.4227e-05, 3.1634e-05, 3.5707e-05, 4.1838e-05, 5.4632e-05]],\n",
       "              \n",
       "                       [[5.6768e-05, 7.2268e-05, 8.7496e-05, 1.1502e-04, 1.1736e-04],\n",
       "                        [5.6107e-05, 7.9806e-05, 1.1618e-04, 1.2808e-04, 1.0857e-04],\n",
       "                        [8.6775e-05, 1.2147e-04, 1.5775e-04, 1.2949e-04, 1.0771e-04],\n",
       "                        [5.3066e-05, 7.9764e-05, 1.1382e-04, 1.0133e-04, 9.7000e-05],\n",
       "                        [3.6758e-05, 6.0801e-05, 7.7509e-05, 1.0151e-04, 1.0983e-04]],\n",
       "              \n",
       "                       [[2.7183e-05, 3.2110e-05, 3.6810e-05, 4.2974e-05, 4.1052e-05],\n",
       "                        [2.8052e-05, 3.3449e-05, 4.1102e-05, 4.0222e-05, 3.8419e-05],\n",
       "                        [3.5202e-05, 3.3593e-05, 3.5963e-05, 3.4864e-05, 3.3764e-05],\n",
       "                        [4.2182e-05, 4.7883e-05, 4.1258e-05, 3.3965e-05, 3.3615e-05],\n",
       "                        [2.9753e-05, 2.8124e-05, 2.6658e-05, 3.4780e-05, 3.5798e-05]],\n",
       "              \n",
       "                       [[3.2723e-05, 5.0353e-05, 6.3797e-05, 8.0654e-05, 7.1356e-05],\n",
       "                        [3.4533e-05, 4.1719e-05, 4.8046e-05, 6.6357e-05, 5.3020e-05],\n",
       "                        [2.4015e-05, 2.0912e-05, 2.5649e-05, 4.9764e-05, 5.2221e-05],\n",
       "                        [2.6847e-05, 2.5766e-05, 3.7797e-05, 6.3494e-05, 7.5324e-05],\n",
       "                        [3.5813e-05, 5.1906e-05, 6.6831e-05, 8.2774e-05, 8.1055e-05]],\n",
       "              \n",
       "                       [[6.8133e-05, 8.8357e-05, 1.1649e-04, 1.1600e-04, 1.0125e-04],\n",
       "                        [6.4264e-05, 7.8852e-05, 1.0000e-04, 9.4103e-05, 9.2247e-05],\n",
       "                        [4.9169e-05, 4.9427e-05, 6.2478e-05, 8.5466e-05, 1.0686e-04],\n",
       "                        [5.9396e-05, 5.6527e-05, 7.2627e-05, 1.0727e-04, 1.1386e-04],\n",
       "                        [7.1145e-05, 8.3531e-05, 1.1531e-04, 1.2551e-04, 1.2911e-04]],\n",
       "              \n",
       "                       [[4.1538e-05, 5.3415e-05, 6.6226e-05, 4.9580e-05, 4.4089e-05],\n",
       "                        [3.1644e-05, 4.3105e-05, 5.9738e-05, 4.0749e-05, 4.0540e-05],\n",
       "                        [1.9740e-05, 2.6604e-05, 3.6222e-05, 3.8288e-05, 4.4218e-05],\n",
       "                        [2.4852e-05, 3.2162e-05, 4.4351e-05, 5.4607e-05, 6.2423e-05],\n",
       "                        [3.6899e-05, 5.4597e-05, 6.9917e-05, 6.5394e-05, 5.8931e-05]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.0734, -0.1406, -0.1281, -0.0846,  0.0363, -0.1584, -0.1189, -0.1589,\n",
       "                     -0.0495, -0.0513, -0.0558, -0.1153, -0.1481,  0.0627, -0.0460,  0.0108],\n",
       "                    requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([ 0.0011,  0.0061,  0.0040, -0.0057,  0.0074, -0.0005, -0.0037, -0.0019,\n",
       "                       0.0012, -0.0043, -0.0031,  0.0054,  0.0036,  0.0051, -0.0006,  0.0026]),\n",
       "              'exp_avg_sq': tensor([0.0004, 0.0007, 0.0005, 0.0004, 0.0010, 0.0001, 0.0004, 0.0003, 0.0005,\n",
       "                      0.0003, 0.0003, 0.0005, 0.0003, 0.0003, 0.0004, 0.0005])},\n",
       "             Parameter containing:\n",
       "             tensor([[-0.0180,  0.0042, -0.0316,  ..., -0.0443,  0.0018, -0.0418],\n",
       "                     [ 0.0168, -0.0065, -0.0695,  ...,  0.0274,  0.0445, -0.0064],\n",
       "                     [-0.0628, -0.1010, -0.0617,  ..., -0.0071, -0.0383, -0.0343],\n",
       "                     ...,\n",
       "                     [ 0.1037,  0.0039,  0.0452,  ...,  0.0236,  0.0273,  0.0062],\n",
       "                     [-0.0156, -0.0562, -0.0368,  ...,  0.0162, -0.0708,  0.0263],\n",
       "                     [ 0.0083, -0.0420, -0.0598,  ...,  0.0261, -0.0599, -0.0602]],\n",
       "                    requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([[-3.0710e-04, -1.3480e-04,  8.6495e-06,  ...,  6.5267e-04,\n",
       "                        4.9416e-04,  4.7773e-04],\n",
       "                      [-2.1261e-04, -2.3482e-04, -1.1861e-04,  ..., -1.2672e-04,\n",
       "                        1.0540e-04,  1.7418e-04],\n",
       "                      [ 1.6648e-04,  2.4402e-04, -2.0088e-04,  ..., -2.2612e-04,\n",
       "                       -2.2184e-04, -1.9063e-04],\n",
       "                      ...,\n",
       "                      [ 3.7656e-04,  3.1233e-04,  4.8463e-04,  ..., -8.8882e-05,\n",
       "                       -1.5828e-04, -2.7326e-04],\n",
       "                      [-4.3679e-04, -7.6870e-04, -2.5769e-04,  ...,  1.2214e-03,\n",
       "                        1.4848e-03,  1.0114e-03],\n",
       "                      [-6.8965e-05, -2.8953e-05, -1.2307e-04,  ...,  8.1431e-05,\n",
       "                        1.0503e-04,  1.2847e-04]]),\n",
       "              'exp_avg_sq': tensor([[1.0070e-06, 1.3497e-06, 1.5164e-06,  ..., 1.0431e-06, 1.5691e-06,\n",
       "                       2.5075e-06],\n",
       "                      [3.9390e-07, 5.2409e-07, 6.4455e-07,  ..., 6.3726e-07, 1.3503e-06,\n",
       "                       1.6469e-06],\n",
       "                      [6.5300e-07, 8.6142e-07, 1.0605e-06,  ..., 1.3430e-06, 1.9013e-06,\n",
       "                       2.4083e-06],\n",
       "                      ...,\n",
       "                      [2.7915e-06, 4.1621e-06, 5.3277e-06,  ..., 2.7729e-06, 4.7053e-06,\n",
       "                       7.1560e-06],\n",
       "                      [2.5407e-06, 3.4926e-06, 4.1463e-06,  ..., 3.9439e-06, 5.0759e-06,\n",
       "                       6.8182e-06],\n",
       "                      [1.4090e-06, 2.2143e-06, 2.7763e-06,  ..., 2.8691e-06, 3.6074e-06,\n",
       "                       4.4106e-06]])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.0114,  0.0370,  0.0449, -0.0188,  0.0694,  0.0129,  0.0423, -0.0559,\n",
       "                      0.0210, -0.0309,  0.0026,  0.0423, -0.0033,  0.0388,  0.0041,  0.0413,\n",
       "                      0.0456,  0.0186,  0.0506, -0.0073,  0.0178, -0.0377, -0.0257,  0.0136,\n",
       "                      0.0440, -0.0471, -0.0087,  0.0615, -0.0203,  0.0568, -0.0612,  0.0086,\n",
       "                      0.0239, -0.0134, -0.0129,  0.0592, -0.0623,  0.0138,  0.0030,  0.0151,\n",
       "                     -0.0351,  0.0300,  0.0544,  0.0119, -0.0197, -0.0611, -0.0476,  0.0075,\n",
       "                      0.0297,  0.0041,  0.0298,  0.0236, -0.0273,  0.0681,  0.0408,  0.0411,\n",
       "                     -0.0055,  0.0269, -0.0322,  0.0427, -0.0387,  0.0080, -0.0136,  0.0793,\n",
       "                     -0.0441,  0.0059,  0.0017,  0.0263,  0.0305, -0.0119,  0.0143, -0.0526,\n",
       "                     -0.0328,  0.0242,  0.0139, -0.0029, -0.0032, -0.0427, -0.0513, -0.0017,\n",
       "                      0.0080, -0.0277, -0.0267, -0.0329,  0.0413,  0.0129, -0.0422, -0.0133,\n",
       "                     -0.0594, -0.0477, -0.0162,  0.0213,  0.0567, -0.0196,  0.0511, -0.0118,\n",
       "                      0.0342, -0.0600, -0.0082, -0.0647, -0.0080,  0.0498,  0.0561,  0.0318,\n",
       "                      0.0507, -0.0157, -0.0061, -0.0291,  0.0440, -0.0174,  0.0012, -0.0081,\n",
       "                     -0.0215, -0.0423, -0.0194, -0.0328, -0.0605,  0.0146, -0.0166,  0.0321],\n",
       "                    requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([ 5.8505e-04,  1.7540e-04, -2.0481e-04, -4.6973e-05,  3.7218e-04,\n",
       "                       1.5960e-04, -9.2316e-05, -8.4918e-04, -4.5973e-04, -1.2109e-03,\n",
       "                       3.4869e-04, -1.2760e-04, -5.9634e-04,  4.0421e-04, -6.9626e-04,\n",
       "                      -9.2908e-05,  1.9023e-04,  1.3306e-03, -9.7585e-06, -1.0838e-03,\n",
       "                       1.2037e-03,  7.1252e-05, -4.3906e-04, -7.9942e-05, -3.5187e-04,\n",
       "                      -9.4772e-04, -3.3383e-04, -6.3465e-04,  7.8361e-04, -1.5339e-04,\n",
       "                      -6.8565e-04, -9.8037e-04,  1.5741e-03,  2.0904e-04, -5.5115e-04,\n",
       "                       9.8785e-04,  2.9637e-04, -2.0049e-04,  1.0265e-03,  2.2254e-04,\n",
       "                       6.9725e-04,  1.1499e-03, -1.0267e-03, -2.8352e-04,  1.9177e-04,\n",
       "                      -6.2876e-04, -1.4620e-04, -4.8404e-04, -1.5766e-04,  5.5285e-04,\n",
       "                      -7.2826e-04, -1.9755e-04, -4.2953e-04, -3.3221e-04, -3.9465e-04,\n",
       "                      -1.2825e-04, -3.0530e-04,  5.0601e-04, -7.6193e-05,  2.9949e-04,\n",
       "                       2.9330e-04,  1.3726e-03, -1.0211e-03, -2.1964e-04, -4.7402e-04,\n",
       "                      -1.0386e-03,  1.5272e-04,  7.1874e-04, -2.3364e-04,  5.6591e-04,\n",
       "                       7.9427e-04,  3.9161e-04, -2.1797e-04,  5.1466e-04, -8.6905e-04,\n",
       "                      -8.3635e-04, -8.5568e-05,  1.3651e-03, -5.0649e-04,  2.3713e-04,\n",
       "                       1.1187e-04, -6.4046e-04, -2.7162e-05,  7.0801e-04, -1.0201e-04,\n",
       "                      -5.1850e-04,  2.2004e-04,  1.0430e-04,  6.9175e-04,  3.8789e-04,\n",
       "                       1.8385e-04, -2.5495e-04,  4.1955e-05,  3.7171e-04, -5.9127e-04,\n",
       "                       6.6833e-04, -5.8433e-05,  3.3865e-04,  7.9901e-04, -1.3983e-03,\n",
       "                       6.7327e-04, -1.6047e-03,  4.2579e-04,  1.4683e-04, -1.3412e-04,\n",
       "                      -5.9247e-04,  1.9771e-04,  1.4945e-04, -1.0772e-04, -5.2224e-04,\n",
       "                      -7.6086e-05,  3.0206e-04, -6.9633e-04,  6.1053e-04,  3.0421e-04,\n",
       "                       3.8659e-05, -1.7583e-03, -5.5984e-04,  7.0342e-04,  1.8388e-04]),\n",
       "              'exp_avg_sq': tensor([4.2624e-06, 2.3410e-06, 3.4229e-06, 3.1372e-06, 7.9752e-06, 6.0772e-06,\n",
       "                      6.5458e-06, 1.2241e-05, 3.1084e-06, 6.9559e-06, 7.3319e-06, 1.2194e-05,\n",
       "                      9.0995e-06, 6.5243e-06, 1.5461e-05, 4.6921e-06, 6.1324e-06, 4.0483e-06,\n",
       "                      4.3917e-06, 7.9961e-06, 6.4882e-06, 3.8785e-06, 3.8585e-06, 3.1064e-06,\n",
       "                      6.2477e-06, 1.2867e-05, 4.8625e-06, 5.3978e-06, 8.8156e-06, 5.8577e-06,\n",
       "                      7.4300e-06, 4.8355e-06, 1.1381e-05, 3.9716e-06, 7.3634e-06, 5.0362e-06,\n",
       "                      5.3435e-06, 8.0060e-06, 7.0127e-06, 3.8903e-06, 7.3389e-06, 5.9731e-06,\n",
       "                      7.3480e-06, 5.5717e-06, 3.7150e-06, 6.1671e-06, 4.4165e-06, 3.6279e-06,\n",
       "                      2.5407e-06, 5.8056e-06, 7.5392e-06, 1.0286e-05, 3.2080e-06, 4.3203e-06,\n",
       "                      3.2152e-06, 3.0078e-06, 5.0442e-06, 6.2773e-06, 3.6323e-06, 6.6260e-06,\n",
       "                      9.1828e-06, 1.1614e-05, 4.4013e-06, 5.7368e-06, 8.5747e-06, 6.8388e-06,\n",
       "                      5.2127e-06, 5.9213e-06, 5.3315e-06, 3.3983e-06, 3.4091e-06, 6.0585e-06,\n",
       "                      5.4524e-06, 7.2219e-06, 8.1839e-06, 7.4771e-06, 4.0497e-06, 5.4213e-06,\n",
       "                      6.1140e-06, 3.9115e-06, 5.6019e-06, 6.3732e-06, 4.3839e-06, 7.0808e-06,\n",
       "                      4.2057e-06, 8.8895e-06, 3.6733e-06, 5.0311e-06, 6.6796e-06, 9.9852e-06,\n",
       "                      5.7985e-06, 7.7393e-06, 6.2364e-06, 2.9663e-06, 5.0687e-06, 1.0590e-05,\n",
       "                      8.8359e-06, 4.1860e-06, 9.2084e-06, 1.0633e-05, 6.2974e-06, 1.4100e-05,\n",
       "                      6.3997e-06, 6.9805e-06, 5.0788e-06, 7.2977e-06, 8.7295e-06, 5.0402e-06,\n",
       "                      8.1271e-06, 4.5176e-06, 2.3817e-06, 8.2174e-06, 6.5754e-06, 6.3774e-06,\n",
       "                      3.8406e-06, 5.2981e-06, 1.3410e-05, 1.0901e-05, 1.1275e-05, 6.6497e-06])},\n",
       "             Parameter containing:\n",
       "             tensor([[ 0.0060, -0.0574, -0.0932,  ..., -0.1363,  0.0219,  0.1091],\n",
       "                     [ 0.0517,  0.0648,  0.0138,  ...,  0.1235, -0.1977,  0.1014],\n",
       "                     [-0.0832,  0.0167,  0.0765,  ...,  0.0269,  0.0093,  0.1346],\n",
       "                     ...,\n",
       "                     [ 0.0446, -0.1428, -0.0109,  ..., -0.0269, -0.0717, -0.1303],\n",
       "                     [-0.0985,  0.0803,  0.1534,  ...,  0.0256,  0.0874,  0.1562],\n",
       "                     [ 0.0088, -0.0371, -0.0428,  ..., -0.0335, -0.0699, -0.0785]],\n",
       "                    requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([[ 7.6606e-04,  2.9703e-04,  1.8503e-04,  ..., -3.9695e-04,\n",
       "                        9.6788e-05, -8.9300e-05],\n",
       "                      [-2.0741e-04, -2.8223e-04,  2.2615e-04,  ..., -2.1244e-05,\n",
       "                       -2.6779e-05,  3.9816e-04],\n",
       "                      [ 2.5355e-06, -7.3096e-05,  2.1161e-04,  ...,  2.6669e-04,\n",
       "                        2.7998e-05, -2.9649e-06],\n",
       "                      ...,\n",
       "                      [-1.3367e-03, -5.7950e-04,  1.1088e-03,  ...,  1.3236e-04,\n",
       "                       -5.6676e-04,  3.7591e-04],\n",
       "                      [-7.6881e-05,  1.2328e-06,  2.8507e-04,  ...,  3.7819e-04,\n",
       "                       -1.7755e-04,  3.0204e-05],\n",
       "                      [ 5.3936e-04, -7.2741e-05, -2.9886e-04,  ..., -3.7880e-04,\n",
       "                        3.8744e-04, -4.7839e-04]]),\n",
       "              'exp_avg_sq': tensor([[2.5824e-06, 2.4970e-06, 4.2461e-06,  ..., 1.7586e-06, 1.6498e-06,\n",
       "                       3.0302e-06],\n",
       "                      [1.0847e-06, 1.2649e-06, 1.1972e-06,  ..., 7.5941e-07, 6.0479e-07,\n",
       "                       8.2831e-07],\n",
       "                      [2.3845e-06, 2.1899e-06, 2.0306e-06,  ..., 1.7842e-06, 1.8634e-06,\n",
       "                       1.7473e-06],\n",
       "                      ...,\n",
       "                      [3.4970e-06, 3.4042e-06, 4.4444e-06,  ..., 1.9825e-06, 2.3472e-06,\n",
       "                       3.2816e-06],\n",
       "                      [2.0688e-06, 1.7314e-06, 1.9700e-06,  ..., 1.7376e-06, 1.6975e-06,\n",
       "                       1.4476e-06],\n",
       "                      [1.1634e-06, 1.3363e-06, 1.8863e-06,  ..., 8.7934e-07, 9.4727e-07,\n",
       "                       1.2684e-06]])},\n",
       "             Parameter containing:\n",
       "             tensor([ 0.0605,  0.0620, -0.0301, -0.0592,  0.0662, -0.0574, -0.0121,  0.0080,\n",
       "                     -0.0367, -0.0133, -0.1057, -0.0053, -0.0198,  0.0874, -0.0888,  0.0542,\n",
       "                      0.1079, -0.0600, -0.1003, -0.0687,  0.0791, -0.0848, -0.0019,  0.0267,\n",
       "                      0.0524, -0.0438,  0.0435,  0.0089,  0.0156, -0.0098,  0.0322,  0.0927,\n",
       "                     -0.0725,  0.0114,  0.0954,  0.0099,  0.0752,  0.0557,  0.0750,  0.0775,\n",
       "                      0.0176,  0.1202, -0.0666,  0.1097, -0.0385, -0.1019, -0.0773, -0.0401,\n",
       "                     -0.0225, -0.0867,  0.0500,  0.0445, -0.0408, -0.0372, -0.0712,  0.0318,\n",
       "                      0.1344,  0.1030,  0.0531,  0.0848, -0.0932, -0.0319,  0.0747, -0.0569,\n",
       "                     -0.0152, -0.0651, -0.0359, -0.0232, -0.0715,  0.0275,  0.0417, -0.0749,\n",
       "                     -0.0610, -0.0410, -0.0028, -0.0106, -0.0443,  0.0050, -0.0677,  0.0143,\n",
       "                      0.0216, -0.0175, -0.0577, -0.0493], requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([-5.6972e-04,  2.9301e-04, -4.3671e-05,  8.0687e-05, -6.2425e-04,\n",
       "                      -1.0811e-03, -1.7121e-05,  3.0498e-04, -7.5140e-04,  2.7679e-04,\n",
       "                       9.9896e-05,  3.2711e-05,  4.2977e-04, -2.5302e-04, -6.5652e-05,\n",
       "                       8.3400e-04,  8.2798e-05, -1.9400e-05,  1.6178e-04,  4.1515e-04,\n",
       "                      -1.3510e-04,  4.5444e-04, -8.6746e-04,  2.8968e-04,  6.1702e-04,\n",
       "                      -4.9379e-04,  6.9335e-04, -1.3918e-03,  5.2562e-05,  8.6569e-04,\n",
       "                      -3.9760e-04,  5.3588e-06,  5.7636e-04, -7.6035e-04, -6.8335e-04,\n",
       "                      -2.3677e-05, -8.0505e-04, -8.8425e-05, -1.9076e-04, -5.9762e-04,\n",
       "                      -3.8149e-04, -2.8460e-04, -5.6574e-04,  3.2907e-05, -2.3881e-04,\n",
       "                       3.1405e-04, -1.3233e-03,  2.5848e-04, -7.5154e-04,  6.9126e-04,\n",
       "                      -3.0286e-05, -2.9385e-04, -1.2156e-03,  1.0148e-04, -3.5878e-04,\n",
       "                      -1.6555e-04,  4.3877e-04,  3.4587e-04,  7.2278e-04,  4.0894e-04,\n",
       "                      -8.7287e-04,  1.0044e-03,  8.3044e-04,  1.6842e-04, -2.7764e-04,\n",
       "                       2.9015e-04,  3.6721e-04,  2.3349e-05, -1.0578e-04,  1.2885e-03,\n",
       "                       1.8250e-04,  4.4722e-04,  1.3018e-03,  1.6624e-04, -5.1057e-04,\n",
       "                      -1.7649e-04,  2.2929e-04, -2.7965e-04, -2.7356e-04, -1.3687e-04,\n",
       "                       3.8853e-04,  1.5884e-03,  1.8226e-04, -8.2190e-04]),\n",
       "              'exp_avg_sq': tensor([6.5361e-06, 2.3495e-06, 4.9490e-06, 2.6146e-06, 1.2632e-05, 6.5130e-06,\n",
       "                      4.0822e-06, 7.9657e-06, 5.3137e-06, 2.3598e-06, 1.0478e-05, 4.4581e-06,\n",
       "                      6.4550e-06, 8.5233e-06, 5.2561e-06, 7.5918e-06, 3.3452e-06, 5.6663e-06,\n",
       "                      5.0254e-06, 3.9227e-06, 6.2860e-06, 4.6163e-06, 9.3180e-06, 7.2294e-06,\n",
       "                      5.3702e-06, 5.8671e-06, 8.7771e-06, 7.4638e-06, 5.1706e-06, 8.5328e-06,\n",
       "                      6.9062e-06, 5.6336e-06, 6.2301e-06, 9.0245e-06, 4.3279e-06, 3.7035e-06,\n",
       "                      7.8736e-06, 3.3853e-06, 5.0422e-06, 3.6877e-06, 3.0555e-06, 4.7417e-06,\n",
       "                      4.4388e-06, 5.5205e-06, 3.3372e-06, 4.3700e-06, 6.5160e-06, 3.3749e-06,\n",
       "                      3.6000e-06, 4.2093e-06, 6.5954e-06, 5.3698e-06, 5.4827e-06, 3.8668e-06,\n",
       "                      4.9716e-06, 7.5211e-06, 6.9631e-06, 4.5073e-06, 4.7366e-06, 6.5347e-06,\n",
       "                      7.8558e-06, 4.4585e-06, 8.2433e-06, 4.3882e-06, 5.9365e-06, 5.2510e-06,\n",
       "                      4.4038e-06, 3.7448e-06, 4.0556e-06, 6.3586e-06, 6.9829e-06, 4.8464e-06,\n",
       "                      9.0835e-06, 9.3641e-06, 5.2177e-06, 7.5539e-06, 2.9076e-06, 4.3544e-06,\n",
       "                      1.0456e-05, 7.6663e-06, 6.5905e-06, 7.3540e-06, 4.5661e-06, 2.9396e-06])},\n",
       "             Parameter containing:\n",
       "             tensor([[-1.8076e-01, -1.7700e-01, -1.4004e-01,  3.4465e-02,  1.5724e-01,\n",
       "                       1.8587e-01, -1.5697e-02, -1.6544e-01, -1.7517e-02,  8.2159e-02,\n",
       "                       9.4693e-02,  1.4183e-01, -2.1136e-03,  1.2890e-01, -2.2965e-01,\n",
       "                       2.3741e-01,  1.0997e-01,  1.8402e-01, -2.3483e-01, -7.9912e-02,\n",
       "                       2.4338e-01,  1.4566e-01, -1.6826e-01, -3.7799e-02, -1.2563e-01,\n",
       "                      -1.5425e-01, -1.3748e-01, -2.3191e-01,  2.0601e-01, -7.9115e-02,\n",
       "                      -1.7665e-01,  1.8280e-01,  5.1070e-03, -1.8510e-01,  2.6427e-01,\n",
       "                      -1.6285e-01,  6.5499e-03,  1.6312e-01,  1.3287e-01,  6.3881e-02,\n",
       "                      -1.5663e-01, -8.5675e-02, -1.2423e-01, -3.3589e-02, -2.6557e-02,\n",
       "                      -2.2539e-01, -2.6906e-01, -4.5880e-02, -7.1805e-02,  2.2038e-02,\n",
       "                      -2.1636e-01,  1.0512e-01,  1.2766e-01,  9.7870e-02, -8.1464e-02,\n",
       "                       2.0329e-01, -1.6568e-01, -1.9607e-01,  1.7094e-01, -2.5586e-01,\n",
       "                      -1.1429e-01,  1.4533e-01, -1.7968e-01,  3.3362e-02,  2.0656e-01,\n",
       "                       1.2132e-01,  1.9835e-01, -6.7421e-02,  1.3262e-01, -1.0973e-01,\n",
       "                       7.6751e-02, -1.8440e-01, -1.4384e-01, -1.7846e-01, -1.2723e-01,\n",
       "                      -7.9562e-02,  2.4491e-01, -2.5354e-02,  3.4168e-02,  1.4191e-01,\n",
       "                       1.4524e-01,  1.5761e-01, -1.9484e-01,  1.3246e-01],\n",
       "                     [-1.2731e-01,  6.2022e-02,  1.4573e-01,  8.5221e-02,  4.3613e-03,\n",
       "                      -1.4094e-01,  2.0789e-01,  1.5320e-02,  1.8506e-01, -1.9713e-01,\n",
       "                      -2.0197e-01, -2.1195e-01, -2.3393e-01, -2.1002e-01, -2.6876e-02,\n",
       "                      -1.1471e-01,  1.8779e-01, -1.6022e-01, -3.7315e-02,  7.2235e-02,\n",
       "                       1.8892e-01, -8.4911e-02, -1.4635e-01, -7.0025e-02,  1.7347e-01,\n",
       "                       9.6300e-02,  1.2136e-01, -2.1213e-01, -1.1306e-01, -2.1636e-02,\n",
       "                       1.1421e-01,  1.9184e-01,  2.1591e-01,  1.8482e-01,  2.4711e-01,\n",
       "                       1.2697e-01,  2.1582e-01, -1.0937e-01, -1.7178e-01, -1.5446e-01,\n",
       "                      -2.2618e-01,  7.8847e-02, -1.4332e-01,  1.6059e-02, -1.4005e-01,\n",
       "                      -2.0246e-01, -3.9273e-02,  1.6547e-01,  2.1957e-01, -1.7429e-01,\n",
       "                       6.3180e-02,  5.4074e-02, -2.0219e-01,  1.2506e-01,  1.8927e-01,\n",
       "                      -1.8442e-01,  1.6024e-01,  3.3103e-03, -1.6915e-01, -2.0445e-01,\n",
       "                      -2.2336e-01, -8.1928e-02, -2.3265e-03, -2.0862e-01, -1.7578e-01,\n",
       "                      -1.3238e-01, -1.4034e-01,  1.4004e-01, -2.1596e-01,  1.8631e-01,\n",
       "                       9.0306e-02, -2.2449e-01,  1.1037e-01, -1.5326e-01,  2.8288e-02,\n",
       "                       6.7169e-02, -9.0401e-02,  2.0847e-01,  1.7732e-01, -1.2067e-01,\n",
       "                      -1.9442e-01, -1.0760e-01,  1.9373e-01, -1.3235e-01],\n",
       "                     [-6.4716e-02,  4.7855e-02, -2.2713e-01,  7.7804e-02, -1.1437e-01,\n",
       "                      -1.4142e-02, -1.2467e-01, -1.8453e-01, -1.3175e-01,  3.6361e-02,\n",
       "                       1.0546e-01, -1.8802e-01, -1.2925e-01,  1.5598e-01, -1.9567e-01,\n",
       "                       2.1079e-01,  1.3663e-01, -8.6199e-03,  2.1210e-01, -1.7767e-01,\n",
       "                       2.1170e-01,  9.6377e-02,  1.6208e-01,  1.9071e-01,  2.0393e-02,\n",
       "                       6.2470e-03,  4.1272e-02, -2.0822e-01, -9.1298e-02, -1.5180e-01,\n",
       "                      -1.1565e-01,  1.8924e-01, -8.0610e-02, -1.3712e-01,  1.1259e-01,\n",
       "                      -1.6920e-01,  2.8313e-02,  1.2760e-02, -2.4763e-01, -1.6914e-01,\n",
       "                       4.8622e-02,  1.3352e-01,  2.1555e-01,  1.9910e-01, -5.8304e-02,\n",
       "                      -1.1913e-02,  2.2102e-01,  7.7734e-02,  5.1544e-02,  1.1931e-01,\n",
       "                      -2.1947e-01,  2.1049e-01, -1.8727e-01, -1.8754e-01, -1.9505e-01,\n",
       "                      -2.3177e-01, -6.4246e-02,  2.1238e-01,  1.2177e-01, -6.8133e-02,\n",
       "                      -2.9878e-03, -1.8113e-01,  1.4614e-01, -2.1844e-01,  2.3104e-02,\n",
       "                       1.6918e-02, -1.8774e-01,  1.8760e-01,  4.6468e-02,  1.8530e-01,\n",
       "                       2.1724e-01, -9.7908e-02, -1.2438e-01,  6.3787e-03, -4.4485e-02,\n",
       "                      -2.3142e-01, -1.3090e-01,  1.7036e-01, -1.6427e-01, -1.9429e-01,\n",
       "                       2.2703e-01, -9.0927e-02, -2.5672e-01,  3.4689e-02],\n",
       "                     [-6.6525e-02,  5.2440e-02,  1.7350e-01,  8.3022e-02, -1.6444e-01,\n",
       "                      -1.2778e-01, -1.6356e-01, -2.1147e-01, -5.1606e-02,  5.9324e-02,\n",
       "                       1.4479e-01, -1.8754e-01,  3.2238e-02, -2.0206e-01, -1.8718e-01,\n",
       "                      -9.3726e-02,  9.6482e-02, -6.9189e-02,  1.8770e-01, -3.5922e-02,\n",
       "                      -1.1718e-01,  1.6004e-01,  2.2177e-01,  1.5897e-01, -4.7989e-02,\n",
       "                       1.1856e-01, -1.9142e-01,  2.2899e-01, -1.8291e-01,  1.0580e-01,\n",
       "                      -7.5394e-02, -8.1210e-02,  1.7713e-01, -1.5624e-01, -8.0807e-02,\n",
       "                      -1.8819e-01, -1.3147e-01, -1.3798e-01,  1.6840e-01, -7.8818e-02,\n",
       "                      -1.8159e-01, -1.2270e-01,  1.1501e-01, -2.2611e-01, -1.3596e-01,\n",
       "                       1.7679e-01,  1.9122e-01, -1.0314e-01, -4.0878e-02, -6.9880e-02,\n",
       "                       1.4037e-01,  2.4404e-02,  1.4271e-01, -1.4607e-01, -4.9239e-02,\n",
       "                      -1.4453e-01,  9.6761e-02, -1.2010e-01,  9.2804e-02,  9.0981e-02,\n",
       "                      -1.9180e-01,  7.2424e-02, -1.2243e-01,  6.8446e-02,  1.3377e-01,\n",
       "                       1.7280e-01, -2.1670e-01,  5.3488e-02, -8.8172e-02,  1.3936e-01,\n",
       "                       1.8937e-01,  2.8463e-02,  8.3086e-02, -1.1110e-01, -7.3503e-02,\n",
       "                       1.8272e-01,  6.4695e-02,  1.6385e-01,  2.2431e-01, -1.6182e-01,\n",
       "                      -6.7687e-02,  1.7431e-01,  1.3734e-01,  7.7648e-02],\n",
       "                     [ 1.3620e-01,  1.8589e-01,  7.7037e-02, -1.8598e-01,  1.5196e-01,\n",
       "                       1.7214e-01,  7.9132e-02, -8.8433e-02,  1.0948e-01, -2.2468e-01,\n",
       "                      -1.6353e-01,  1.5341e-01, -2.3192e-01, -1.5907e-02,  1.8381e-02,\n",
       "                      -2.9809e-02, -1.3597e-01, -2.2911e-02, -2.0906e-01,  1.0104e-01,\n",
       "                      -9.1422e-02, -1.4926e-01,  2.5171e-01,  1.5401e-03, -1.5432e-01,\n",
       "                       5.6607e-02, -2.9224e-02,  2.0777e-01,  5.2177e-02, -2.5851e-01,\n",
       "                       1.3276e-02, -7.0213e-02, -1.4205e-01,  1.9252e-01, -1.1524e-02,\n",
       "                       8.6202e-02,  2.1419e-01,  1.5822e-01,  2.5086e-04,  1.3720e-01,\n",
       "                       7.8742e-02,  1.2665e-01,  1.2531e-01,  8.1420e-02,  2.0635e-01,\n",
       "                       1.2751e-01,  5.9600e-02, -8.3578e-02,  2.1292e-01, -1.5149e-01,\n",
       "                       5.5227e-02, -1.3346e-01,  8.5538e-02, -1.4837e-01,  1.5290e-01,\n",
       "                      -1.4061e-01, -1.9502e-01,  9.7033e-02, -1.4959e-01,  1.3815e-01,\n",
       "                       1.1669e-01, -1.4175e-01, -2.4231e-01,  1.2955e-01,  2.2218e-02,\n",
       "                       1.0026e-01,  4.3607e-02,  9.2306e-02, -4.7850e-02, -1.5141e-01,\n",
       "                      -3.5248e-02,  1.2718e-01, -1.7252e-01,  2.0505e-01,  2.0776e-01,\n",
       "                      -1.2775e-01,  1.6107e-01, -3.2292e-02,  1.6860e-01, -7.5405e-03,\n",
       "                      -6.7879e-02, -9.3846e-02, -3.4172e-03,  1.7520e-01],\n",
       "                     [ 1.6412e-01, -1.3807e-01,  1.0267e-01,  1.7194e-02,  2.4877e-01,\n",
       "                       2.1801e-01,  1.3617e-02,  1.6298e-01, -7.9348e-02,  1.0812e-01,\n",
       "                       1.0046e-01,  1.0941e-02,  1.5002e-01,  1.8850e-01,  2.2974e-02,\n",
       "                      -1.5279e-01,  1.8254e-01, -9.7054e-02,  8.1768e-02, -1.2470e-01,\n",
       "                      -1.3609e-01, -8.9197e-02, -1.4989e-03,  2.5420e-01,  2.2279e-01,\n",
       "                       2.1750e-01,  1.1320e-01,  9.8303e-02, -2.4666e-01,  1.1572e-01,\n",
       "                       2.1185e-01,  1.1882e-01,  2.1095e-02,  1.5420e-01, -1.1482e-01,\n",
       "                      -4.2281e-02, -1.5275e-01, -1.1617e-01, -1.7254e-02,  5.8443e-02,\n",
       "                      -3.3419e-02, -2.1635e-01, -7.8058e-02, -1.9323e-01, -1.6531e-01,\n",
       "                       1.5621e-01,  1.7168e-01, -1.7932e-01, -7.5985e-02, -1.6042e-01,\n",
       "                       1.9178e-01,  2.2431e-01,  1.2877e-01, -4.3705e-02,  2.5511e-02,\n",
       "                       2.1011e-01,  2.0074e-01, -2.4216e-01, -3.3131e-02,  2.1446e-01,\n",
       "                       1.4926e-01,  4.4030e-02, -2.2530e-01,  9.6601e-02, -1.2749e-01,\n",
       "                      -1.3275e-01, -4.7818e-03, -1.3289e-01,  1.3046e-01, -2.3839e-01,\n",
       "                       1.5354e-01,  1.7913e-01, -1.5321e-01, -1.1267e-01, -4.7266e-02,\n",
       "                       1.5839e-01,  4.6346e-02, -4.8356e-02, -1.8354e-01,  1.8518e-01,\n",
       "                       5.6987e-02,  1.9500e-02,  5.8582e-02,  1.1812e-01],\n",
       "                     [ 1.2057e-01, -1.7989e-01, -1.9186e-01, -9.3675e-02,  1.7595e-01,\n",
       "                       1.1400e-02, -1.2539e-01,  1.3412e-01,  1.4135e-01,  1.3200e-02,\n",
       "                      -8.2167e-02,  9.9350e-03, -1.6222e-01,  6.7464e-02, -1.9330e-01,\n",
       "                       1.0810e-01,  3.8945e-03,  1.2966e-01,  2.2667e-01,  1.5909e-01,\n",
       "                       1.4031e-01, -9.9652e-02,  2.1146e-02, -2.0192e-01,  2.2805e-01,\n",
       "                       5.7979e-02,  1.6654e-01,  8.0115e-02, -2.1672e-01, -1.2880e-01,\n",
       "                       2.1561e-01,  1.6060e-01, -9.1962e-02, -1.5811e-01,  5.7325e-02,\n",
       "                       1.5656e-01,  1.9377e-01,  1.2833e-01,  3.1462e-02,  3.8251e-02,\n",
       "                      -2.2849e-03, -3.0654e-01, -1.1236e-01, -6.8626e-02,  2.6892e-02,\n",
       "                      -1.6041e-01, -2.1515e-01, -6.5684e-02,  1.3736e-01,  2.2159e-01,\n",
       "                      -1.8848e-01,  1.8453e-01, -4.0538e-02, -1.3785e-01,  1.3925e-01,\n",
       "                       1.5016e-01,  1.9482e-01, -1.1233e-01, -2.1920e-01, -2.3163e-01,\n",
       "                       1.6906e-01, -1.5246e-01,  1.0076e-01,  2.2151e-01,  1.3388e-01,\n",
       "                      -1.3214e-01, -6.4922e-02,  3.0046e-03,  1.9825e-01, -2.1290e-01,\n",
       "                      -6.6641e-02, -9.8663e-02, -8.6443e-02,  8.8738e-02,  1.6425e-01,\n",
       "                       1.4199e-01,  6.6725e-02, -1.4931e-01, -2.2633e-01,  1.9308e-01,\n",
       "                       1.0912e-01, -1.5505e-01, -6.5515e-02, -9.5826e-02],\n",
       "                     [-2.4754e-01,  1.1333e-01,  3.7306e-02, -1.9995e-01,  2.7708e-02,\n",
       "                      -1.6991e-01,  1.5191e-01,  1.4247e-01, -1.4449e-01, -5.4443e-03,\n",
       "                      -2.5628e-01,  8.8626e-02,  3.1688e-02, -3.8202e-02,  9.7619e-02,\n",
       "                       2.6567e-01, -1.6962e-01, -8.1941e-02, -1.3864e-01, -7.3852e-03,\n",
       "                      -1.0972e-01,  1.8193e-01, -7.7867e-02,  2.0451e-01,  2.2162e-02,\n",
       "                      -2.1638e-01,  1.8294e-01, -1.6634e-01,  8.8141e-02,  1.5521e-01,\n",
       "                      -2.3610e-01, -5.6332e-02,  1.6087e-01, -1.9574e-01, -1.1646e-01,\n",
       "                      -1.7498e-02, -1.3639e-01, -4.2744e-02, -2.7502e-02,  6.4616e-02,\n",
       "                       1.6657e-01, -1.4812e-01, -4.2320e-02,  1.1538e-01, -2.2743e-02,\n",
       "                       1.3974e-01, -7.9045e-02,  1.9403e-01, -6.3438e-02, -1.7836e-01,\n",
       "                       1.5957e-01, -8.2439e-02, -2.3205e-01,  3.2828e-02, -1.2694e-01,\n",
       "                      -9.5712e-02,  1.5534e-01,  1.5919e-01,  1.0626e-01,  1.9448e-01,\n",
       "                      -2.1885e-01,  7.3591e-02,  1.3247e-02,  7.7833e-02, -2.1236e-01,\n",
       "                       1.4697e-01,  9.0256e-02, -8.9197e-02, -1.3901e-01,  1.3189e-01,\n",
       "                       8.9415e-02,  1.8169e-01,  1.4468e-01,  2.1561e-01, -1.4685e-01,\n",
       "                      -2.1792e-01,  1.0655e-01, -1.0481e-01, -2.2870e-01, -1.4911e-02,\n",
       "                       2.2333e-01,  1.9479e-01,  8.0723e-02, -1.0523e-01],\n",
       "                     [ 2.8466e-02, -4.9069e-02,  1.0061e-01,  1.1322e-01, -1.8825e-01,\n",
       "                      -1.1902e-01, -1.4892e-01, -3.5195e-02, -1.5549e-01,  7.0649e-02,\n",
       "                       1.6098e-01, -9.7582e-02,  9.7242e-04,  1.6243e-01,  1.2787e-01,\n",
       "                      -1.0194e-01,  3.7896e-02,  2.4601e-01,  1.2135e-01, -1.1129e-01,\n",
       "                      -1.4698e-01,  9.6947e-02, -1.4859e-01, -2.0110e-01,  7.2967e-02,\n",
       "                      -1.1384e-01,  7.7202e-02,  7.4992e-02,  1.3438e-01, -1.5157e-01,\n",
       "                       8.9987e-02, -1.6666e-01, -1.9177e-01,  1.9698e-01, -9.6660e-02,\n",
       "                      -2.6295e-02, -1.3679e-01, -1.2167e-01,  1.7257e-01, -1.2252e-01,\n",
       "                      -1.0008e-01,  5.1214e-02, -1.8693e-01,  1.2048e-01, -1.9106e-01,\n",
       "                       1.6446e-01,  7.0968e-02,  3.1108e-02, -5.0218e-02,  1.4697e-01,\n",
       "                      -1.2091e-01, -1.7989e-01,  1.0595e-01,  6.8148e-02, -1.2995e-01,\n",
       "                       1.2764e-01, -1.3024e-01,  3.2835e-02, -8.0108e-02, -1.7556e-02,\n",
       "                       1.4612e-01, -5.9266e-02,  2.0969e-01, -4.3720e-02, -9.5878e-02,\n",
       "                      -1.5328e-01, -6.2504e-02,  1.4612e-01,  1.5701e-01, -1.2720e-02,\n",
       "                      -1.5825e-01,  1.2429e-01,  1.1783e-01,  6.3946e-02, -1.1679e-01,\n",
       "                       1.3206e-01, -5.4064e-02,  1.4899e-01,  2.0085e-01,  2.4497e-01,\n",
       "                      -2.0737e-02, -1.9781e-01,  1.0793e-01, -7.8331e-03],\n",
       "                     [ 2.1098e-01,  8.9738e-02, -4.3004e-03, -6.4315e-02, -2.2889e-01,\n",
       "                       4.6699e-03, -4.9632e-02,  9.9629e-02,  1.7838e-01, -6.7823e-02,\n",
       "                       2.4355e-01,  1.8641e-01,  1.9710e-01, -2.0551e-01,  2.2848e-02,\n",
       "                      -1.5295e-01, -1.3927e-01, -2.0389e-01, -8.5548e-02,  1.8816e-01,\n",
       "                      -2.4250e-02,  1.5404e-01, -8.8083e-02,  1.3183e-01, -1.3645e-01,\n",
       "                       1.6144e-01, -2.1668e-01, -1.8048e-02,  2.6320e-02,  1.7030e-01,\n",
       "                       1.5739e-01, -1.1654e-01,  5.2637e-02, -6.0105e-02, -1.6276e-01,\n",
       "                       1.4564e-01, -2.3692e-01,  9.7340e-02,  5.9214e-02, -8.6020e-02,\n",
       "                       1.4394e-01,  7.4025e-02,  2.1480e-02,  1.3495e-01,  1.1303e-01,\n",
       "                       1.6825e-01, -1.5099e-01, -1.4454e-01, -1.0001e-01, -1.0399e-01,\n",
       "                       9.6015e-02, -6.2467e-02,  7.2560e-02,  1.4426e-02,  1.4879e-01,\n",
       "                       1.2468e-01, -1.0663e-01,  7.4658e-02, -1.5148e-01, -2.1731e-02,\n",
       "                       1.3473e-01,  8.8760e-02,  3.7993e-02, -8.1286e-02,  1.1748e-01,\n",
       "                       1.4807e-01,  2.1265e-01,  1.1163e-01, -1.2785e-01,  4.9639e-02,\n",
       "                      -2.0935e-01,  1.3274e-01,  2.0347e-01, -1.7241e-01, -1.8524e-01,\n",
       "                       1.6082e-01,  1.6003e-01, -1.1300e-01,  1.0508e-01, -1.3035e-01,\n",
       "                      -1.5719e-01,  1.7987e-01, -4.3792e-03,  9.7385e-02]],\n",
       "                    requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([[ 6.4814e-04, -1.2025e-03, -5.7067e-05,  1.0160e-03,  8.6939e-04,\n",
       "                        7.0089e-05, -2.3159e-04, -9.4584e-04,  5.6359e-04,  3.5681e-04,\n",
       "                       -2.8630e-04,  4.2575e-04, -1.7828e-03,  1.0160e-03, -1.1832e-03,\n",
       "                        1.4351e-04,  2.4593e-03,  2.2843e-03,  3.9339e-04,  1.1260e-03,\n",
       "                        2.1574e-05, -1.4637e-03, -2.1958e-04, -2.3929e-03, -2.8567e-04,\n",
       "                       -4.1804e-04, -6.7413e-04,  1.0874e-03,  2.4642e-04, -2.7360e-03,\n",
       "                        5.1475e-04,  3.5691e-05, -1.6149e-03, -1.2299e-04,  7.0391e-04,\n",
       "                       -5.1801e-04,  1.9170e-03,  4.0174e-04,  2.8690e-03,  8.2725e-04,\n",
       "                       -1.2606e-03, -9.0998e-04, -2.7345e-03,  1.9626e-05, -6.1430e-04,\n",
       "                       -5.5591e-04,  3.3875e-05, -1.6627e-03,  1.0572e-03,  9.8083e-04,\n",
       "                       -8.6241e-04, -3.2197e-04,  2.9251e-03,  2.6106e-04,  1.5037e-03,\n",
       "                        9.0408e-04, -3.1426e-04, -2.0259e-03, -1.8731e-03, -1.5992e-03,\n",
       "                        5.7332e-04, -7.3417e-04,  1.8231e-04,  2.5549e-03,  1.1049e-03,\n",
       "                       -1.9287e-04, -8.3274e-04, -6.3729e-04,  2.0273e-03, -1.1134e-03,\n",
       "                       -2.1002e-03, -9.6163e-04, -1.1869e-03, -5.5002e-04,  9.7783e-04,\n",
       "                        6.5668e-04,  9.1458e-04,  1.0235e-04,  1.5720e-03,  2.0602e-03,\n",
       "                       -1.7777e-03, -8.7206e-04, -3.9723e-04,  5.5472e-04],\n",
       "                      [-8.9155e-05, -1.0430e-03, -1.0520e-03, -1.3822e-03,  9.7702e-04,\n",
       "                        4.8049e-04, -2.5381e-04,  7.1943e-04,  2.0867e-04, -3.4228e-04,\n",
       "                        3.9794e-04,  1.0967e-03,  4.8674e-04,  1.7677e-03,  9.7696e-05,\n",
       "                        1.1031e-03, -1.2913e-03,  1.2423e-03, -8.9391e-04,  2.7844e-04,\n",
       "                        1.3274e-03,  1.2457e-04, -1.3857e-03, -8.5715e-04,  9.5332e-04,\n",
       "                       -1.2048e-03,  1.3996e-03, -1.1509e-03,  6.4736e-04, -7.6268e-04,\n",
       "                       -4.1471e-04,  6.0219e-04, -1.3855e-03, -7.6885e-04,  3.9475e-04,\n",
       "                        2.9633e-04,  2.0615e-04,  1.2030e-03, -6.1570e-04,  1.0376e-03,\n",
       "                        1.3544e-03, -3.2107e-04, -1.2124e-03,  1.3542e-03,  1.3826e-03,\n",
       "                       -6.5660e-04, -9.7500e-04,  6.2350e-04, -4.7707e-04,  8.3029e-04,\n",
       "                       -1.0655e-03,  6.1144e-04, -9.0360e-04,  3.0333e-04, -3.4199e-04,\n",
       "                        1.1453e-03, -7.4540e-04,  7.8513e-04,  4.4440e-05, -3.8339e-04,\n",
       "                        8.2298e-04, -5.3030e-04,  5.7123e-04,  7.5431e-04, -5.7564e-04,\n",
       "                       -7.8087e-04,  1.0222e-03, -3.3090e-04,  1.3482e-03, -1.2908e-03,\n",
       "                       -9.4658e-04,  4.1109e-04, -8.5474e-05,  1.1209e-03,  4.0901e-04,\n",
       "                       -1.6778e-03,  8.9414e-04, -8.9348e-04, -8.4006e-04,  1.5207e-03,\n",
       "                        8.6851e-04, -2.9231e-04, -9.7803e-04, -7.1498e-04],\n",
       "                      [ 1.5401e-03, -3.4497e-04, -1.2755e-04, -1.0216e-03,  1.4123e-03,\n",
       "                        1.6216e-03,  6.4914e-05,  3.8148e-04,  1.1131e-03, -1.4186e-04,\n",
       "                       -4.6081e-04,  2.8660e-04,  8.0782e-04,  2.6491e-04,  1.4018e-04,\n",
       "                       -6.3247e-04, -5.8327e-04, -8.1792e-04, -2.7969e-04,  1.0600e-04,\n",
       "                       -1.8719e-04, -3.2832e-04,  1.6801e-04,  5.2280e-04, -1.5354e-04,\n",
       "                        1.4316e-03, -4.6700e-04,  3.9171e-04, -7.1303e-04,  1.0100e-03,\n",
       "                        7.2073e-04,  1.0052e-03, -1.9393e-04,  1.3693e-04, -1.0534e-04,\n",
       "                        1.7349e-04, -6.9118e-04,  1.1503e-03, -4.6741e-04,  1.3953e-03,\n",
       "                        4.8861e-04, -5.9183e-04,  1.4814e-04, -8.9479e-04,  8.7489e-04,\n",
       "                       -6.5431e-04, -5.3524e-04, -9.1299e-04, -2.3859e-04, -3.3395e-04,\n",
       "                       -1.4459e-04,  1.3396e-03,  2.7518e-05, -6.5195e-04,  8.1122e-04,\n",
       "                        6.3711e-04,  4.4877e-04, -8.7765e-04,  2.4468e-04,  2.8068e-04,\n",
       "                        9.9472e-04,  7.3397e-04, -1.1536e-03,  1.9915e-04,  4.9602e-04,\n",
       "                       -2.7996e-04,  5.3036e-04, -1.3600e-03,  3.3632e-04, -1.4107e-03,\n",
       "                        4.8186e-04,  2.7393e-04,  8.7734e-05, -5.3265e-04, -9.7044e-05,\n",
       "                        1.0949e-03,  7.4175e-05, -1.1428e-03, -6.4894e-04, -4.7805e-04,\n",
       "                       -2.7485e-04,  5.8871e-04,  1.4044e-04,  3.2158e-04],\n",
       "                      [ 2.8320e-03,  2.1480e-04,  2.2646e-04, -3.0208e-04,  2.7228e-04,\n",
       "                        3.3520e-04, -6.4897e-04,  1.1158e-03,  1.1247e-03, -5.3559e-04,\n",
       "                       -1.0508e-03,  1.0568e-03, -1.7860e-03, -9.4054e-05,  1.5922e-03,\n",
       "                       -1.9875e-04, -1.5410e-04, -7.5083e-04,  1.0219e-04,  3.6232e-04,\n",
       "                       -9.1950e-04, -1.5792e-03,  1.5503e-05,  6.5966e-04, -7.9376e-04,\n",
       "                        1.0213e-03, -4.9549e-04,  1.1080e-03, -3.1541e-04, -1.5687e-04,\n",
       "                        2.3597e-03, -1.5717e-03, -5.6956e-04,  9.7671e-04, -6.4670e-04,\n",
       "                        1.0436e-03,  1.3956e-03,  4.0853e-04,  7.3119e-04,  5.9777e-04,\n",
       "                        6.4644e-04,  5.1646e-04, -1.2211e-04, -6.0229e-04,  1.9214e-04,\n",
       "                        1.1700e-03,  2.8672e-04, -8.3615e-04,  1.7843e-03, -1.2010e-03,\n",
       "                        6.4262e-04, -1.4174e-03,  6.9255e-04, -1.4145e-03,  2.3316e-03,\n",
       "                        1.1169e-03,  1.5748e-04, -6.2370e-04, -2.4925e-03,  1.0133e-03,\n",
       "                        2.1695e-03, -3.3038e-04, -1.1281e-03,  1.9890e-03,  5.4789e-04,\n",
       "                        3.6153e-04,  4.0511e-04,  1.1998e-03,  1.6777e-04, -2.5805e-03,\n",
       "                       -9.2861e-05,  1.1224e-03, -7.0582e-04, -1.2793e-04,  1.2195e-03,\n",
       "                        3.2164e-05,  1.0207e-03, -1.9790e-03,  9.9159e-04,  1.2513e-03,\n",
       "                        5.9908e-05, -5.0389e-04, -1.5696e-05,  6.8901e-04],\n",
       "                      [ 7.6374e-04, -5.2350e-03, -3.0174e-03,  3.8216e-03,  1.4248e-03,\n",
       "                        2.8640e-03, -4.0583e-03, -1.7198e-03, -1.8396e-03,  2.4966e-03,\n",
       "                        3.8232e-03, -3.8010e-03,  1.9997e-03,  2.7426e-03, -4.5544e-03,\n",
       "                        6.7952e-04,  4.5883e-03,  2.3362e-03,  4.8557e-03, -1.6723e-03,\n",
       "                        3.8200e-03,  9.1162e-04,  1.4699e-03,  1.0216e-05,  2.4349e-03,\n",
       "                        1.9608e-03, -8.1479e-04, -3.7284e-04, -5.0227e-03,  1.9770e-03,\n",
       "                        6.7356e-04,  3.8598e-03, -1.1328e-03, -1.9542e-03,  2.7974e-03,\n",
       "                       -2.0566e-03, -2.0447e-03, -2.0965e-03, -3.2224e-04, -5.4829e-04,\n",
       "                       -4.1205e-03, -5.0920e-03, -2.3931e-03, -4.8213e-03, -2.7269e-03,\n",
       "                       -3.7700e-03,  1.1894e-03, -1.4783e-03, -1.6461e-03,  5.1252e-03,\n",
       "                       -4.1395e-03,  4.5133e-03, -4.0982e-04, -2.9936e-03, -1.9360e-03,\n",
       "                        3.9350e-03,  1.5654e-03, -4.9539e-03,  2.0330e-03, -4.6909e-03,\n",
       "                        1.6653e-04,  1.4675e-04,  1.5535e-03,  8.3923e-04,  2.4781e-03,\n",
       "                       -3.4578e-03, -3.7595e-03, -3.7776e-03,  4.9756e-03, -1.2268e-03,\n",
       "                        1.6585e-03, -4.0930e-03, -1.5958e-03, -2.7454e-03, -1.0810e-03,\n",
       "                        3.3453e-03, -1.3223e-03,  1.1571e-03, -2.5220e-03,  3.9093e-03,\n",
       "                        1.8108e-03,  2.7221e-05, -3.3893e-03,  2.5373e-05],\n",
       "                      [-2.9625e-03, -7.6913e-04,  1.9367e-03,  6.6111e-05, -1.0249e-03,\n",
       "                       -1.7579e-04,  2.1487e-03,  1.7274e-03, -2.9899e-03,  1.9770e-03,\n",
       "                        2.6389e-03,  1.1713e-03,  4.2092e-03, -1.5501e-03,  1.0317e-03,\n",
       "                       -2.2355e-03, -3.3370e-03, -1.0299e-03, -1.2269e-03, -2.0310e-03,\n",
       "                       -3.3460e-03,  2.4351e-03, -2.4136e-03,  2.1340e-03,  1.5019e-03,\n",
       "                       -7.7858e-04,  3.6607e-04,  1.2431e-03,  1.5919e-03,  3.6205e-03,\n",
       "                       -2.6149e-03, -3.5179e-05,  3.0418e-03,  1.7030e-03, -2.6073e-03,\n",
       "                        2.1842e-04, -4.5341e-03, -1.0095e-03, -5.7949e-04, -7.4886e-05,\n",
       "                        1.7419e-03, -2.4743e-03,  9.0810e-04, -2.1619e-03, -8.6586e-04,\n",
       "                        3.1013e-03, -8.3554e-04,  1.7750e-03, -4.6206e-03, -1.5801e-03,\n",
       "                        2.1928e-03, -2.3639e-04, -3.4497e-04,  3.4318e-03, -2.9455e-03,\n",
       "                       -4.7134e-04,  1.4208e-04,  1.0702e-03,  3.0487e-03,  2.7941e-03,\n",
       "                       -1.2344e-03,  2.7861e-03, -1.3730e-03, -2.1971e-03, -3.2752e-03,\n",
       "                        5.6696e-04,  3.1683e-03, -1.5973e-03, -1.7637e-03,  2.7034e-03,\n",
       "                        2.1459e-03,  1.6814e-03,  1.0833e-03,  7.6145e-04, -2.9094e-03,\n",
       "                        2.4618e-03,  3.6697e-04,  3.6276e-04, -2.4317e-03,  3.9690e-04,\n",
       "                        2.4929e-03,  2.4483e-03,  2.0464e-03,  8.9336e-04],\n",
       "                      [ 5.5242e-04, -8.0661e-04, -1.5858e-03, -8.5708e-05,  1.1794e-03,\n",
       "                        2.2676e-04,  8.8213e-05, -1.0010e-03,  1.6492e-03, -8.6121e-04,\n",
       "                       -1.5884e-03, -1.2641e-03, -1.6015e-03,  1.9030e-03, -1.8103e-03,\n",
       "                        1.3886e-03,  2.0049e-03,  9.7997e-04,  5.1419e-04,  8.9710e-04,\n",
       "                        1.4548e-03, -1.5770e-03,  5.2923e-04, -1.2145e-03,  2.1360e-04,\n",
       "                        4.3158e-05,  2.6107e-04, -5.8872e-04, -1.0031e-03, -1.8034e-03,\n",
       "                        8.3906e-04,  1.5696e-03, -1.8318e-03, -1.0039e-03,  1.5601e-03,\n",
       "                       -1.4151e-04,  1.4878e-03,  1.3425e-03, -3.6039e-04,  2.4301e-04,\n",
       "                       -1.3033e-03,  7.8473e-04, -1.5024e-03,  1.7191e-03,  3.7602e-04,\n",
       "                       -1.4597e-03, -3.5314e-04, -8.5678e-04,  1.7326e-03,  1.1923e-03,\n",
       "                       -1.8653e-03,  1.0549e-03, -1.6900e-04, -7.8369e-04,  1.6346e-03,\n",
       "                        4.1417e-04,  1.4021e-04, -8.5988e-04, -1.3021e-03, -1.6492e-03,\n",
       "                       -1.3352e-04, -1.2325e-03,  1.1560e-03,  8.3657e-04,  1.8119e-03,\n",
       "                       -1.1400e-03, -2.0585e-03,  4.6147e-04,  1.6965e-03, -1.2377e-03,\n",
       "                       -1.1564e-03, -1.7600e-03, -1.8977e-03, -3.1748e-04,  8.8109e-04,\n",
       "                       -8.2488e-04, -7.3332e-04,  6.6759e-04,  3.9798e-04, -4.6762e-05,\n",
       "                       -1.1221e-03, -1.3856e-03, -1.7850e-03, -6.1537e-04],\n",
       "                      [-2.1030e-03,  3.5987e-03,  1.6206e-03, -3.1101e-03, -2.0683e-03,\n",
       "                       -2.1808e-03,  2.6234e-03,  2.4233e-03, -9.0073e-04,  6.1320e-04,\n",
       "                       -2.3193e-03,  1.8594e-03,  1.2742e-03, -1.7203e-03,  3.0246e-03,\n",
       "                        5.2634e-04, -3.1392e-03, -3.3652e-03, -2.7299e-03, -4.6181e-05,\n",
       "                       -1.8286e-03,  1.3358e-03, -2.0126e-03,  2.7766e-03, -8.9592e-04,\n",
       "                       -1.6969e-03,  2.1543e-03, -2.0930e-03,  2.7080e-03,  1.6408e-03,\n",
       "                       -1.1889e-03, -1.6918e-03,  2.8101e-03, -1.2137e-03, -1.9013e-03,\n",
       "                        8.2750e-04, -1.3972e-03,  5.3533e-04, -6.9507e-04,  4.3614e-04,\n",
       "                        3.2982e-03,  3.4795e-03,  2.4803e-03,  3.5143e-03,  9.7886e-04,\n",
       "                        2.6521e-03, -1.3052e-03,  2.5187e-03, -1.1961e-03, -3.4683e-03,\n",
       "                        3.0742e-03, -2.3448e-03, -1.4670e-03,  2.5185e-03, -7.5605e-04,\n",
       "                       -1.6767e-03,  1.0537e-03,  3.9127e-03,  6.0498e-04,  3.2168e-03,\n",
       "                       -1.9814e-03,  1.4785e-03, -7.4002e-04, -2.3332e-03, -3.2493e-03,\n",
       "                        2.5549e-03,  2.1839e-03,  1.6924e-03, -3.6069e-03,  2.8140e-03,\n",
       "                        1.2831e-03,  2.6568e-03,  2.0341e-03,  1.6586e-03, -1.5408e-03,\n",
       "                       -2.5031e-03, -3.6363e-05, -1.0020e-03, -1.2268e-03, -3.1147e-03,\n",
       "                        1.0568e-03,  1.8033e-03,  2.3136e-03, -1.3975e-03],\n",
       "                      [-1.1279e-03,  3.1750e-03,  1.0574e-03, -1.2413e-03, -7.7217e-04,\n",
       "                       -1.2742e-03,  5.4709e-04, -1.0359e-04,  6.1057e-05, -1.3337e-03,\n",
       "                       -1.7634e-03, -7.4736e-04, -1.1528e-03, -2.7241e-03,  1.6404e-03,\n",
       "                        8.7733e-04, -1.2375e-03, -1.1120e-03, -6.9517e-04,  9.4405e-04,\n",
       "                        5.6731e-05, -2.6921e-04,  2.6161e-03,  1.2299e-03, -4.3813e-04,\n",
       "                       -3.2272e-04,  1.5536e-03, -3.2943e-04,  5.7881e-04,  1.9353e-04,\n",
       "                       -1.7931e-03, -1.3675e-03,  1.1592e-03, -4.8074e-04,  1.3097e-04,\n",
       "                        6.8655e-04,  1.3264e-03, -1.0863e-03, -7.3329e-04, -6.1089e-04,\n",
       "                        6.8062e-04,  1.1736e-03,  3.2013e-03,  2.3121e-04,  1.1294e-03,\n",
       "                       -2.9141e-04,  8.5625e-04,  1.6100e-03,  1.7088e-03, -1.9185e-03,\n",
       "                        2.3198e-03, -1.3315e-03, -1.3547e-03, -5.6359e-04, -1.5967e-04,\n",
       "                       -3.7657e-03,  1.3795e-03,  2.5330e-03,  1.2653e-03,  1.0597e-03,\n",
       "                       -1.3332e-03, -8.5826e-04, -2.2493e-04, -1.0660e-03, -1.1883e-03,\n",
       "                        1.2863e-03, -5.0646e-04,  4.4753e-04, -3.1014e-03,  2.1519e-03,\n",
       "                        1.0910e-03,  7.2805e-04,  1.3246e-03,  2.4346e-03,  1.1575e-03,\n",
       "                       -1.8263e-03, -6.1468e-04,  1.1408e-03,  3.5845e-04, -2.9420e-03,\n",
       "                        7.1348e-04, -2.6031e-04,  9.3032e-04, -1.2350e-03],\n",
       "                      [-5.3841e-05,  2.4126e-03,  9.9869e-04,  2.2394e-03, -2.2697e-03,\n",
       "                       -1.9673e-03, -2.7952e-04, -2.5972e-03,  1.0099e-03, -2.2290e-03,\n",
       "                        6.0898e-04, -8.4029e-05, -2.4545e-03, -1.6056e-03,  2.1137e-05,\n",
       "                       -1.6516e-03,  6.8974e-04,  2.3317e-04, -3.9853e-05,  3.5496e-05,\n",
       "                       -3.9926e-04,  4.1038e-04,  1.2328e-03, -2.8686e-03, -2.5367e-03,\n",
       "                       -3.5780e-05, -3.2833e-03,  7.0469e-04,  1.2817e-03, -2.9828e-03,\n",
       "                        9.0377e-04, -2.4063e-03, -2.8249e-04,  2.7278e-03, -3.2650e-04,\n",
       "                       -5.2973e-04,  2.3342e-03, -8.4914e-04,  1.7337e-04, -3.3030e-03,\n",
       "                       -1.5257e-03,  3.4349e-03,  1.2267e-03,  1.6418e-03, -7.2685e-04,\n",
       "                        4.6450e-04,  1.6379e-03, -7.8016e-04,  1.8957e-03,  3.7322e-04,\n",
       "                       -1.5207e-04, -1.8673e-03,  1.0038e-03, -1.0737e-04, -1.4183e-04,\n",
       "                       -2.2390e-03, -3.8274e-03,  1.0401e-03, -1.5734e-03, -4.2000e-05,\n",
       "                       -4.4627e-05, -1.4598e-03,  1.1566e-03, -1.5768e-03,  1.8496e-03,\n",
       "                        1.0818e-03, -1.5253e-04,  3.9019e-03, -2.0797e-03,  1.1907e-03,\n",
       "                       -2.3644e-03, -5.8969e-05,  9.4206e-04, -1.7022e-03,  9.8322e-04,\n",
       "                       -7.5874e-04, -5.6390e-04,  1.5865e-03,  4.3494e-03, -2.5568e-03,\n",
       "                       -3.8277e-03, -1.5534e-03,  1.1345e-03,  1.4788e-03]]),\n",
       "              'exp_avg_sq': tensor([[1.2455e-05, 2.0037e-05, 2.5758e-05, 1.6713e-05, 1.9560e-05, 2.7533e-05,\n",
       "                       2.9255e-05, 1.4515e-05, 2.1226e-05, 2.8332e-05, 2.3323e-05, 2.0414e-05,\n",
       "                       2.1460e-05, 3.8813e-05, 2.0274e-05, 2.6611e-05, 2.0078e-05, 2.7135e-05,\n",
       "                       1.8534e-05, 2.1249e-05, 1.5668e-05, 2.3799e-05, 2.0568e-05, 1.8173e-05,\n",
       "                       2.0525e-05, 3.0276e-05, 1.4532e-05, 1.6910e-05, 2.9655e-05, 1.9006e-05,\n",
       "                       2.2936e-05, 1.5290e-05, 2.4203e-05, 2.6031e-05, 1.4727e-05, 2.3925e-05,\n",
       "                       2.3315e-05, 2.5915e-05, 2.4364e-05, 1.7301e-05, 2.0197e-05, 1.8776e-05,\n",
       "                       1.7903e-05, 2.3731e-05, 1.9081e-05, 1.9505e-05, 2.7813e-05, 2.7246e-05,\n",
       "                       2.9554e-05, 2.9069e-05, 3.1500e-05, 1.7950e-05, 2.5470e-05, 1.7042e-05,\n",
       "                       2.0656e-05, 2.9334e-05, 2.5376e-05, 2.2505e-05, 2.6240e-05, 1.9086e-05,\n",
       "                       1.5425e-05, 1.5875e-05, 1.9884e-05, 1.9090e-05, 2.7947e-05, 2.0529e-05,\n",
       "                       1.7707e-05, 1.6126e-05, 3.0159e-05, 1.9270e-05, 2.2600e-05, 1.6466e-05,\n",
       "                       2.5723e-05, 1.3934e-05, 2.2964e-05, 2.2195e-05, 2.0266e-05, 1.5983e-05,\n",
       "                       1.9208e-05, 2.7499e-05, 2.7137e-05, 2.1454e-05, 3.5851e-05, 2.0482e-05],\n",
       "                      [1.8277e-05, 1.9711e-05, 2.5286e-05, 2.7506e-05, 2.5960e-05, 4.1046e-05,\n",
       "                       3.3603e-05, 1.7129e-05, 1.9876e-05, 2.2094e-05, 2.6084e-05, 3.4782e-05,\n",
       "                       1.8410e-05, 2.6806e-05, 2.1685e-05, 2.2076e-05, 2.9587e-05, 2.3669e-05,\n",
       "                       2.1873e-05, 1.7567e-05, 1.8857e-05, 1.8013e-05, 2.2469e-05, 2.3496e-05,\n",
       "                       2.0453e-05, 1.8310e-05, 1.8421e-05, 1.8557e-05, 2.4546e-05, 1.8142e-05,\n",
       "                       2.1801e-05, 1.7705e-05, 1.9223e-05, 3.1984e-05, 1.6785e-05, 2.4805e-05,\n",
       "                       1.8808e-05, 2.4052e-05, 2.1722e-05, 3.7018e-05, 3.0871e-05, 2.6312e-05,\n",
       "                       2.7899e-05, 2.4620e-05, 2.9465e-05, 2.0016e-05, 1.9268e-05, 3.9228e-05,\n",
       "                       1.8021e-05, 2.6614e-05, 2.1508e-05, 2.5723e-05, 2.3642e-05, 3.3801e-05,\n",
       "                       2.0272e-05, 2.1610e-05, 2.4077e-05, 1.7291e-05, 2.7590e-05, 2.0269e-05,\n",
       "                       1.9723e-05, 1.2429e-05, 2.6116e-05, 3.3498e-05, 2.3174e-05, 2.4016e-05,\n",
       "                       2.7854e-05, 3.0132e-05, 2.5080e-05, 2.9192e-05, 2.1373e-05, 2.2387e-05,\n",
       "                       2.4685e-05, 1.5211e-05, 2.0322e-05, 1.9744e-05, 3.2400e-05, 3.2195e-05,\n",
       "                       2.6935e-05, 2.8636e-05, 3.0521e-05, 2.4960e-05, 2.8720e-05, 2.7628e-05],\n",
       "                      [4.5975e-05, 3.9593e-05, 3.0892e-05, 5.8151e-05, 6.7554e-05, 4.8661e-05,\n",
       "                       6.3491e-05, 4.4390e-05, 7.2599e-05, 6.0193e-05, 5.1734e-05, 6.1698e-05,\n",
       "                       5.1068e-05, 3.6049e-05, 5.0483e-05, 3.9273e-05, 5.7964e-05, 3.9348e-05,\n",
       "                       5.7297e-05, 7.1726e-05, 2.7247e-05, 6.2212e-05, 4.1910e-05, 4.8361e-05,\n",
       "                       3.1930e-05, 3.3905e-05, 3.3680e-05, 3.5059e-05, 3.7176e-05, 3.7521e-05,\n",
       "                       4.4848e-05, 3.1608e-05, 3.6168e-05, 4.8800e-05, 2.7137e-05, 6.8658e-05,\n",
       "                       6.6902e-05, 4.6506e-05, 4.1292e-05, 5.4634e-05, 4.9549e-05, 5.8163e-05,\n",
       "                       4.8977e-05, 4.5342e-05, 6.5762e-05, 3.9493e-05, 5.9917e-05, 3.5143e-05,\n",
       "                       5.5734e-05, 4.4754e-05, 3.7223e-05, 3.7472e-05, 4.3140e-05, 3.8707e-05,\n",
       "                       7.3436e-05, 4.6603e-05, 3.7241e-05, 4.0881e-05, 6.1119e-05, 3.3198e-05,\n",
       "                       4.2195e-05, 2.9920e-05, 5.7347e-05, 5.5744e-05, 3.7524e-05, 3.9261e-05,\n",
       "                       5.9177e-05, 5.4234e-05, 3.3492e-05, 6.1072e-05, 4.8699e-05, 3.7891e-05,\n",
       "                       3.0923e-05, 2.8842e-05, 6.8814e-05, 4.2809e-05, 5.7568e-05, 5.8991e-05,\n",
       "                       4.2414e-05, 5.7380e-05, 3.9750e-05, 4.7296e-05, 3.4329e-05, 2.9132e-05],\n",
       "                      [3.9804e-05, 4.5915e-05, 5.6485e-05, 6.1264e-05, 5.8895e-05, 4.5141e-05,\n",
       "                       5.9077e-05, 4.2026e-05, 6.5393e-05, 6.9805e-05, 5.9019e-05, 4.8217e-05,\n",
       "                       6.2781e-05, 3.6427e-05, 4.4030e-05, 4.9905e-05, 6.4886e-05, 4.4805e-05,\n",
       "                       6.4914e-05, 6.1547e-05, 5.7431e-05, 5.7377e-05, 4.2627e-05, 5.7460e-05,\n",
       "                       3.4953e-05, 4.0018e-05, 3.7226e-05, 5.1269e-05, 4.3760e-05, 5.4760e-05,\n",
       "                       3.9800e-05, 4.0754e-05, 4.5104e-05, 4.5322e-05, 5.3862e-05, 6.2773e-05,\n",
       "                       8.2348e-05, 5.7315e-05, 5.5437e-05, 4.4104e-05, 4.9449e-05, 5.2480e-05,\n",
       "                       3.9564e-05, 5.1142e-05, 7.3518e-05, 6.4286e-05, 7.6747e-05, 5.4068e-05,\n",
       "                       6.7781e-05, 5.0781e-05, 5.4580e-05, 5.1232e-05, 5.9951e-05, 3.1049e-05,\n",
       "                       6.0107e-05, 4.1931e-05, 4.6659e-05, 5.2856e-05, 5.5797e-05, 5.1884e-05,\n",
       "                       4.2856e-05, 4.4724e-05, 4.6135e-05, 5.0270e-05, 3.7406e-05, 3.9108e-05,\n",
       "                       4.9732e-05, 4.7066e-05, 3.7448e-05, 5.5552e-05, 4.8097e-05, 5.1161e-05,\n",
       "                       4.2153e-05, 4.1123e-05, 7.3070e-05, 5.3343e-05, 4.3046e-05, 4.5010e-05,\n",
       "                       3.5148e-05, 5.7713e-05, 3.3292e-05, 5.0318e-05, 5.7503e-05, 3.5174e-05],\n",
       "                      [7.7295e-05, 8.3078e-05, 5.7873e-05, 6.2975e-05, 3.6816e-05, 2.8705e-05,\n",
       "                       3.4524e-05, 3.3789e-05, 8.2691e-05, 5.9001e-05, 3.0535e-05, 8.4712e-05,\n",
       "                       4.8792e-05, 4.7818e-05, 7.8016e-05, 4.7787e-05, 9.1109e-05, 5.0684e-05,\n",
       "                       7.9798e-05, 7.2009e-05, 8.0652e-05, 4.7348e-05, 4.0199e-05, 2.8375e-05,\n",
       "                       7.1083e-05, 3.5723e-05, 4.3076e-05, 4.7164e-05, 6.2950e-05, 4.3479e-05,\n",
       "                       3.9406e-05, 7.7740e-05, 3.8389e-05, 4.3968e-05, 6.7225e-05, 8.3336e-05,\n",
       "                       5.5805e-05, 6.6422e-05, 5.0965e-05, 3.6920e-05, 7.7076e-05, 5.8740e-05,\n",
       "                       3.7598e-05, 7.8352e-05, 9.1381e-05, 8.0984e-05, 3.5028e-05, 6.0396e-05,\n",
       "                       3.9766e-05, 7.0438e-05, 6.1262e-05, 8.9987e-05, 5.2159e-05, 4.2386e-05,\n",
       "                       8.1319e-05, 2.6451e-05, 6.1104e-05, 7.3545e-05, 7.5102e-05, 6.8014e-05,\n",
       "                       6.8792e-05, 2.9116e-05, 3.0825e-05, 4.3206e-05, 4.1708e-05, 4.9167e-05,\n",
       "                       7.0504e-05, 4.7523e-05, 5.7246e-05, 4.2925e-05, 7.7072e-05, 7.7214e-05,\n",
       "                       4.2752e-05, 3.6338e-05, 4.5476e-05, 3.3684e-05, 5.5870e-05, 5.4424e-05,\n",
       "                       5.9626e-05, 4.0631e-05, 6.0134e-05, 3.2559e-05, 5.2408e-05, 4.6045e-05],\n",
       "                      [4.1295e-05, 4.5860e-05, 4.9273e-05, 5.6284e-05, 3.7168e-05, 3.0017e-05,\n",
       "                       4.4943e-05, 3.0155e-05, 5.3662e-05, 7.6445e-05, 6.0635e-05, 3.3341e-05,\n",
       "                       6.3430e-05, 4.5530e-05, 3.7157e-05, 5.3947e-05, 7.2972e-05, 3.8804e-05,\n",
       "                       5.7934e-05, 5.4338e-05, 5.6860e-05, 4.0041e-05, 3.1116e-05, 4.6001e-05,\n",
       "                       4.1096e-05, 4.0845e-05, 3.3550e-05, 5.6716e-05, 4.5505e-05, 4.8814e-05,\n",
       "                       4.3752e-05, 3.5285e-05, 3.6422e-05, 3.9503e-05, 4.8965e-05, 4.7862e-05,\n",
       "                       7.4859e-05, 4.6697e-05, 5.8336e-05, 2.4540e-05, 5.1926e-05, 4.1597e-05,\n",
       "                       3.0722e-05, 5.7435e-05, 7.0836e-05, 5.4943e-05, 4.9393e-05, 6.4678e-05,\n",
       "                       6.8697e-05, 4.3080e-05, 4.9599e-05, 4.6176e-05, 7.6356e-05, 2.9747e-05,\n",
       "                       4.3628e-05, 4.7559e-05, 4.2297e-05, 6.1109e-05, 4.3003e-05, 5.1040e-05,\n",
       "                       4.1286e-05, 4.2174e-05, 3.9107e-05, 3.8713e-05, 3.2027e-05, 3.7348e-05,\n",
       "                       3.4207e-05, 2.9540e-05, 4.1762e-05, 4.6569e-05, 3.7710e-05, 4.4317e-05,\n",
       "                       3.8742e-05, 3.8354e-05, 5.8228e-05, 7.3438e-05, 2.9481e-05, 3.0280e-05,\n",
       "                       3.3170e-05, 4.9072e-05, 3.0786e-05, 4.3661e-05, 4.4364e-05, 3.5410e-05],\n",
       "                      [3.1077e-05, 3.9247e-05, 3.1309e-05, 2.1540e-05, 3.2185e-05, 2.0863e-05,\n",
       "                       3.0869e-05, 1.3002e-05, 2.4490e-05, 2.4817e-05, 2.3636e-05, 1.9844e-05,\n",
       "                       3.6341e-05, 4.2163e-05, 3.1964e-05, 2.1420e-05, 2.7972e-05, 2.8480e-05,\n",
       "                       2.6536e-05, 2.2048e-05, 1.7956e-05, 3.4780e-05, 2.0406e-05, 3.0802e-05,\n",
       "                       2.0957e-05, 1.9025e-05, 2.5495e-05, 2.0068e-05, 2.8415e-05, 3.2815e-05,\n",
       "                       2.6854e-05, 3.3570e-05, 3.7867e-05, 2.7052e-05, 1.3545e-05, 2.4664e-05,\n",
       "                       4.0763e-05, 3.2591e-05, 2.4886e-05, 1.8013e-05, 2.3168e-05, 2.3475e-05,\n",
       "                       2.5067e-05, 1.8815e-05, 2.7445e-05, 2.1936e-05, 1.8681e-05, 3.3377e-05,\n",
       "                       2.7422e-05, 3.9806e-05, 3.4979e-05, 2.6825e-05, 2.4980e-05, 3.1045e-05,\n",
       "                       2.1522e-05, 2.6787e-05, 1.9894e-05, 2.2416e-05, 2.4247e-05, 2.6748e-05,\n",
       "                       3.4349e-05, 3.2361e-05, 1.8863e-05, 3.0772e-05, 3.1720e-05, 3.0899e-05,\n",
       "                       1.7596e-05, 1.9683e-05, 4.3386e-05, 4.0813e-05, 2.1878e-05, 2.2979e-05,\n",
       "                       3.8802e-05, 1.6090e-05, 3.6532e-05, 2.2662e-05, 1.8386e-05, 2.2505e-05,\n",
       "                       2.9221e-05, 3.5544e-05, 1.9553e-05, 3.6356e-05, 2.8962e-05, 1.4100e-05],\n",
       "                      [4.7395e-05, 6.8130e-05, 3.7495e-05, 3.9440e-05, 5.3719e-05, 5.4014e-05,\n",
       "                       4.4057e-05, 3.9456e-05, 4.5146e-05, 3.5088e-05, 3.6467e-05, 5.9173e-05,\n",
       "                       6.6869e-05, 4.3302e-05, 6.0884e-05, 3.5917e-05, 4.9103e-05, 5.5005e-05,\n",
       "                       6.2274e-05, 4.3738e-05, 4.9422e-05, 7.1397e-05, 4.4434e-05, 5.5717e-05,\n",
       "                       3.8335e-05, 2.8017e-05, 2.6088e-05, 4.6236e-05, 5.3061e-05, 5.6745e-05,\n",
       "                       4.0954e-05, 5.7589e-05, 4.9316e-05, 4.9374e-05, 4.9985e-05, 4.8429e-05,\n",
       "                       7.3147e-05, 3.8355e-05, 4.1358e-05, 3.5296e-05, 5.7197e-05, 4.0865e-05,\n",
       "                       3.4629e-05, 5.6470e-05, 3.7413e-05, 5.9605e-05, 4.5577e-05, 3.9429e-05,\n",
       "                       5.6703e-05, 6.0057e-05, 5.9090e-05, 5.4929e-05, 4.6883e-05, 4.8833e-05,\n",
       "                       4.4433e-05, 4.1111e-05, 2.8904e-05, 5.9832e-05, 4.3698e-05, 5.9625e-05,\n",
       "                       5.2140e-05, 4.9931e-05, 3.5515e-05, 4.5379e-05, 4.6643e-05, 6.6438e-05,\n",
       "                       5.7486e-05, 3.2647e-05, 5.6522e-05, 6.3923e-05, 4.4796e-05, 6.3956e-05,\n",
       "                       6.1094e-05, 2.5468e-05, 7.6118e-05, 4.3203e-05, 5.2511e-05, 5.1252e-05,\n",
       "                       3.6487e-05, 5.4291e-05, 3.2441e-05, 6.9580e-05, 4.4701e-05, 3.2221e-05],\n",
       "                      [4.7918e-05, 4.7084e-05, 5.4860e-05, 9.4654e-05, 7.1060e-05, 5.5251e-05,\n",
       "                       7.9929e-05, 4.1661e-05, 6.9095e-05, 7.3936e-05, 1.0349e-04, 6.2268e-05,\n",
       "                       6.5282e-05, 6.5177e-05, 5.2292e-05, 6.8793e-05, 7.5058e-05, 6.0397e-05,\n",
       "                       5.4696e-05, 6.0560e-05, 5.8671e-05, 6.2179e-05, 5.7925e-05, 6.0579e-05,\n",
       "                       4.9144e-05, 4.4022e-05, 4.4890e-05, 4.8807e-05, 5.2269e-05, 5.2263e-05,\n",
       "                       4.9809e-05, 4.7835e-05, 7.3709e-05, 6.2845e-05, 5.4205e-05, 6.0207e-05,\n",
       "                       8.4676e-05, 5.5739e-05, 6.7991e-05, 6.4483e-05, 7.2833e-05, 6.3949e-05,\n",
       "                       5.5238e-05, 6.7179e-05, 9.1160e-05, 6.2918e-05, 4.8423e-05, 5.1621e-05,\n",
       "                       7.7039e-05, 6.6564e-05, 5.1841e-05, 5.5666e-05, 8.2490e-05, 5.9866e-05,\n",
       "                       6.3795e-05, 5.1682e-05, 6.6696e-05, 5.0789e-05, 5.4572e-05, 4.2889e-05,\n",
       "                       6.5799e-05, 4.1094e-05, 7.9745e-05, 5.9908e-05, 4.6446e-05, 6.3082e-05,\n",
       "                       5.2794e-05, 6.2760e-05, 5.8318e-05, 5.5670e-05, 6.3300e-05, 4.8554e-05,\n",
       "                       5.1484e-05, 4.0097e-05, 8.4383e-05, 6.3200e-05, 5.4712e-05, 6.5071e-05,\n",
       "                       5.5905e-05, 6.6287e-05, 4.4482e-05, 7.5832e-05, 6.0050e-05, 4.2753e-05],\n",
       "                      [8.1513e-05, 1.1471e-04, 8.8171e-05, 7.7223e-05, 6.2535e-05, 4.9518e-05,\n",
       "                       6.1468e-05, 5.8677e-05, 9.0426e-05, 7.4136e-05, 5.4275e-05, 1.2141e-04,\n",
       "                       8.2802e-05, 6.7257e-05, 1.2088e-04, 7.2431e-05, 1.1786e-04, 8.3490e-05,\n",
       "                       1.1860e-04, 8.7891e-05, 1.1712e-04, 6.9925e-05, 5.8115e-05, 5.1792e-05,\n",
       "                       9.3221e-05, 4.7995e-05, 5.7777e-05, 6.3513e-05, 9.5102e-05, 6.8482e-05,\n",
       "                       5.4219e-05, 1.2453e-04, 5.7830e-05, 5.9331e-05, 1.1052e-04, 1.0444e-04,\n",
       "                       8.9033e-05, 6.5967e-05, 8.3852e-05, 4.8209e-05, 1.0289e-04, 8.0200e-05,\n",
       "                       4.0389e-05, 1.0444e-04, 8.6979e-05, 1.2133e-04, 6.2640e-05, 7.3231e-05,\n",
       "                       6.9403e-05, 1.0113e-04, 9.8461e-05, 1.2245e-04, 7.4093e-05, 6.1106e-05,\n",
       "                       8.5902e-05, 4.4290e-05, 7.4515e-05, 9.3337e-05, 8.1431e-05, 1.0017e-04,\n",
       "                       8.0792e-05, 5.5757e-05, 4.5783e-05, 5.3730e-05, 5.5772e-05, 8.6483e-05,\n",
       "                       1.1071e-04, 6.6607e-05, 8.7895e-05, 5.7116e-05, 9.9535e-05, 1.2319e-04,\n",
       "                       7.6604e-05, 4.6373e-05, 8.3794e-05, 5.0086e-05, 9.2966e-05, 8.8815e-05,\n",
       "                       7.7237e-05, 5.8059e-05, 7.9096e-05, 6.7463e-05, 8.5669e-05, 5.3877e-05]])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.1114,  0.0304,  0.0186,  0.0146, -0.0768,  0.1037, -0.0095, -0.0328,\n",
       "                      0.0502, -0.0444], requires_grad=True): {'step': tensor(938.),\n",
       "              'exp_avg': tensor([ 1.0995e-03, -5.9789e-05, -4.3288e-04,  4.3128e-04, -5.1524e-03,\n",
       "                      -2.4648e-03,  2.1629e-03,  3.7514e-03, -8.0302e-04,  1.4677e-03]),\n",
       "              'exp_avg_sq': tensor([5.4552e-05, 5.3889e-05, 9.7539e-05, 1.0354e-04, 1.1608e-04, 9.8622e-05,\n",
       "                      5.2597e-05, 9.5696e-05, 1.4415e-04, 1.5802e-04])}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6d7aa",
   "metadata": {},
   "source": [
    "To store data related to the training that are not automatically registered in `state_dict`, one can add them manually to the object to store. For attributes of the model or the optimizer, either they do not change during training, so it is not necessary to store them, or they change, and it is desirable to change them into `Parameter` and register them by using `self.register_parameter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b2df9",
   "metadata": {},
   "source": [
    "**Quesion 3 (optional)**\n",
    "\n",
    "When we want to ensure reproducibility, we set manually the seed of the Random Number Generator (RNG) of PyTorch with `torch.manual_seed`. When performing two trainings with the same seed, the trained models will be exactly the same.\n",
    "\n",
    "Test two trainings with the same seed.\n",
    "\n",
    "When using checkpoints, to ensure reproducibility, one has to store the state of the RNG (of CPU RNG and/or CUDA RNG). This can be done with `torch.get_rng_state()` and `torch.cuda.get_rng_state()`. Also, `torch.backends.cudnn.deterministic = True` can help.\n",
    "\n",
    "Change the save/load procedure to ensure reproducibility even with checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb6e6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nepochs, save_path = None,\n",
    "               load_path = None):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    acc_eval = []\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if load_path is not None:\n",
    "        checkpoint = torch.load(load_path, weights_only = False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        acc_eval = checkpoint[\"acc_eval\"]\n",
    "        train_losses = checkpoint[\"train_losses\"]\n",
    "        test_losses = checkpoint[\"test_losses\"]\n",
    "        torch.set_rng_state(checkpoint[\"rng_state\"])\n",
    "        if with_cuda:\n",
    "            torch.cuda.set_rng_state(checkpoint[\"cuda_rng_state\"])\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device = device)\n",
    "            target = target.to(device = device)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device = device)\n",
    "                target = target.to(device = device)\n",
    "\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "        acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "        \n",
    "        # Save\n",
    "        if save_path is not None:\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"acc_eval\": acc_eval,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"test_losses\": test_losses,\n",
    "                \"rng_state\": torch.get_rng_state(),\n",
    "                \"cuda_rng_state\": torch.cuda.get_rng_state() if with_cuda else None\n",
    "            }, save_path)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "120555a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.214411 \tValidation Loss: 0.073278\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "if with_cuda:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "model = LeNet().to(device = device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "nepochs = 1\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs, save_path = \"checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0da0881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.061066 \tValidation Loss: 0.050335\n",
      "Epoch: 2 \tTraining Loss: 0.040765 \tValidation Loss: 0.049560\n",
      "Epoch: 3 \tTraining Loss: 0.029245 \tValidation Loss: 0.054061\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device = device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "nepochs = 4\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs, load_path = \"checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d419032b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.061066 \tValidation Loss: 0.050335\n",
      "Epoch: 2 \tTraining Loss: 0.040765 \tValidation Loss: 0.049560\n",
      "Epoch: 3 \tTraining Loss: 0.029245 \tValidation Loss: 0.054061\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device = device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "nepochs = 4\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs, load_path = \"checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585527f",
   "metadata": {},
   "source": [
    "# Forward/backward hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ad7b5",
   "metadata": {},
   "source": [
    "To help debugging, it is possible to use forward and backward hooks. They are functions that the user can define and register into a module in order to call them during the forward/backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5ec77",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Use the method `register_forward_hook` on a submodule (for instance `fc3`) of `model` to print the input and the output of the layer at each pass.\n",
    "\n",
    "For the backward, use `register_full_backward_hook` to print the gradients according to the output and to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1034933",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device = device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df80ae00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x741b10545f70>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook(module, input, output):\n",
    "    print(\"=== FORWARD ===\")\n",
    "    print(\"module:\", module)\n",
    "    print(\"input:\", input)\n",
    "    print(\"output:\", output)\n",
    "\n",
    "model.fc3.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9359b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORWARD ===\n",
      "module: Linear(in_features=84, out_features=10, bias=True)\n",
      "input: (tensor([[-0.1850,  0.0130, -0.0162,  ..., -0.0356, -0.1693,  0.0188],\n",
      "        [-0.1382,  0.0142,  0.0050,  ..., -0.0167, -0.2115, -0.1005],\n",
      "        [-0.0450,  0.0172,  0.0338,  ...,  0.1435, -0.1974, -0.0749],\n",
      "        ...,\n",
      "        [-0.1050, -0.0192, -0.0201,  ..., -0.1139, -0.2045, -0.0720],\n",
      "        [-0.1691,  0.0025,  0.0037,  ...,  0.0588, -0.1763, -0.1052],\n",
      "        [-0.1117,  0.0422, -0.0276,  ...,  0.0561, -0.2093, -0.0418]]),)\n",
      "output: tensor([[-3.3657e-02, -2.1945e-02, -1.0510e-01, -8.1433e-02, -4.6607e-02,\n",
      "          3.6759e-02,  1.4856e-01,  6.4578e-03,  1.6736e-01,  1.2749e-01],\n",
      "        [-5.6749e-02, -1.7369e-02,  4.3454e-02, -1.3543e-02, -4.2700e-02,\n",
      "          9.9895e-02,  1.6199e-02, -3.6899e-02,  2.7296e-02,  9.3829e-02],\n",
      "        [-5.0294e-02, -4.5394e-02,  1.0838e-02, -1.4505e-02, -7.9504e-02,\n",
      "          5.4853e-02,  8.3862e-02,  6.1071e-02,  1.8331e-01,  1.0990e-01],\n",
      "        [-4.0687e-02, -1.1085e-01, -2.0858e-02, -5.4023e-02,  3.9788e-02,\n",
      "          1.3164e-01,  1.5215e-01, -1.4451e-02,  6.1854e-02,  1.2218e-01],\n",
      "        [-1.0750e-01, -2.9162e-02, -6.5180e-02, -2.0617e-02, -4.4924e-02,\n",
      "          9.0009e-02,  1.4966e-01,  2.6035e-02,  8.6038e-02,  1.0553e-01],\n",
      "        [-5.7658e-02, -3.4351e-02,  5.0488e-04,  9.8153e-03, -8.5509e-02,\n",
      "          6.6868e-02,  7.1487e-02,  5.6190e-02,  1.8397e-01,  9.6146e-02],\n",
      "        [-4.1701e-02, -7.1144e-03, -1.0261e-01, -4.4528e-02, -5.3415e-02,\n",
      "          4.8876e-02,  6.4820e-02,  3.8049e-02,  1.8639e-01,  1.1697e-01],\n",
      "        [-8.9015e-02, -1.7653e-02,  1.8274e-02,  1.1496e-02, -4.6645e-02,\n",
      "          4.5996e-02,  7.1953e-02,  3.2179e-03,  8.4709e-02,  1.0509e-01],\n",
      "        [-4.1765e-02, -1.0501e-01, -2.8445e-02, -6.3763e-02,  4.7553e-02,\n",
      "          1.3317e-01,  1.0000e-01, -4.2410e-02,  2.7616e-02,  7.9190e-02],\n",
      "        [-3.1282e-02, -1.0842e-02, -8.5606e-02, -3.2895e-02, -5.0555e-02,\n",
      "          7.2272e-02,  6.8368e-02, -5.3195e-03,  1.8366e-01,  1.1845e-01],\n",
      "        [-4.8481e-02, -1.3801e-01, -6.4912e-02, -4.2172e-02,  2.8330e-02,\n",
      "          9.4988e-02,  1.6597e-01, -2.2051e-03,  5.5029e-02,  1.1101e-01],\n",
      "        [-4.6925e-02, -5.4444e-02, -4.4551e-02, -4.9964e-03, -6.9715e-02,\n",
      "          1.0319e-01,  2.4692e-02, -1.9896e-02,  6.1736e-02,  1.0389e-01],\n",
      "        [-5.8665e-02,  2.6841e-02, -8.3788e-02, -4.1571e-02, -6.2941e-02,\n",
      "          8.7653e-02,  5.9893e-02,  1.2522e-02,  1.5008e-01,  1.2095e-01],\n",
      "        [-5.7034e-02, -6.9984e-02, -8.2358e-03, -5.3724e-02,  4.4289e-02,\n",
      "          1.0582e-01,  1.5306e-01, -5.2138e-02,  1.2921e-01,  1.3164e-01],\n",
      "        [ 1.0399e-02, -4.8310e-02,  6.0381e-03,  1.4465e-04, -8.5876e-02,\n",
      "          2.0906e-02, -2.2974e-03,  1.1343e-02,  1.3052e-01,  2.9248e-02],\n",
      "        [-4.2394e-02, -5.3399e-02, -6.7541e-02, -7.5459e-02, -4.9613e-02,\n",
      "          1.9290e-02,  1.1096e-01, -2.6319e-02,  1.5352e-01,  1.0501e-01],\n",
      "        [-3.6868e-02, -4.1969e-02, -8.4207e-02, -4.7430e-02, -2.1108e-02,\n",
      "          9.5334e-02,  6.3317e-02, -2.0738e-02,  1.2000e-01,  1.1208e-01],\n",
      "        [-5.8378e-03, -2.8412e-02, -6.9127e-02, -1.0826e-01, -1.5537e-02,\n",
      "          3.4759e-02,  1.4839e-01,  9.7285e-03,  1.4366e-01,  1.0922e-01],\n",
      "        [-1.0900e-01,  4.9677e-02, -1.3155e-02,  1.9483e-02, -7.7750e-02,\n",
      "          7.2512e-02,  4.7192e-02, -8.1494e-02,  1.7136e-01,  1.5746e-01],\n",
      "        [-6.2508e-02, -3.4101e-03, -6.8787e-02, -1.7059e-02, -6.0393e-02,\n",
      "          8.0733e-02,  4.7594e-02,  1.4568e-03,  1.5289e-01,  9.7751e-02],\n",
      "        [-2.6399e-02,  5.6901e-03, -6.5344e-02, -2.0755e-02, -8.9771e-02,\n",
      "          9.2273e-02,  9.9730e-02,  3.5539e-02,  1.9679e-01,  9.2008e-02],\n",
      "        [-9.4924e-02, -3.9836e-03, -3.1798e-02,  2.4042e-02, -6.9395e-02,\n",
      "          9.2285e-02,  6.1511e-02, -5.1906e-03,  9.0830e-02,  1.0106e-01],\n",
      "        [-4.8087e-02, -5.1074e-02, -5.3914e-03,  7.5254e-03,  1.8365e-03,\n",
      "          1.1054e-01,  8.1034e-02,  2.1766e-03,  7.4318e-02,  1.1956e-01],\n",
      "        [-4.5796e-02, -1.8774e-02, -8.0391e-02, -4.7500e-02, -4.4278e-02,\n",
      "          6.2147e-02,  3.1497e-02,  8.0355e-03,  1.0716e-01,  1.0181e-01],\n",
      "        [-7.7237e-02, -2.9855e-02, -7.3867e-02, -5.1065e-02, -3.0383e-02,\n",
      "          6.4265e-02,  8.6399e-02, -9.1436e-03,  6.3697e-02,  1.0197e-01],\n",
      "        [-3.4471e-02, -1.2905e-01, -6.9230e-02, -2.7738e-02,  5.0849e-02,\n",
      "          1.6351e-01,  1.6740e-01, -1.9013e-02,  3.5006e-02,  1.1201e-01],\n",
      "        [-2.6055e-02, -7.3135e-02, -7.8939e-02, -6.1634e-02, -6.9802e-02,\n",
      "          5.1310e-02,  9.4473e-02, -6.9485e-03,  1.4564e-01,  6.2939e-02],\n",
      "        [-9.3665e-02,  9.5575e-03, -8.4391e-02, -2.5933e-02, -7.5746e-02,\n",
      "          8.3714e-02,  7.9254e-02,  9.9987e-03,  1.1473e-01,  6.8344e-02],\n",
      "        [-4.2384e-02, -1.9334e-02, -1.3760e-02, -3.5174e-02, -2.7393e-03,\n",
      "          8.6420e-02,  9.8763e-02, -1.9114e-02,  9.4249e-02,  8.9664e-02],\n",
      "        [ 1.0663e-02, -2.0513e-02, -1.7309e-02, -6.3565e-03, -9.5237e-02,\n",
      "         -9.6206e-03,  1.3162e-02,  3.2685e-02,  1.3127e-01,  6.5001e-02],\n",
      "        [-1.7613e-02, -2.3643e-03, -4.5009e-02, -2.6302e-02, -5.8780e-02,\n",
      "          3.5551e-02,  4.9694e-02,  4.9377e-03,  1.3238e-01,  8.6696e-02],\n",
      "        [-2.7111e-02, -6.5142e-02, -2.3466e-02, -3.1744e-02, -5.1563e-02,\n",
      "         -1.5841e-02,  4.7304e-02,  2.9396e-02,  1.6751e-01,  4.3680e-02],\n",
      "        [-2.2343e-02, -6.4448e-03, -7.1399e-02, -5.4035e-02, -2.5843e-02,\n",
      "          4.4898e-02,  4.6472e-02, -2.1348e-02,  6.8093e-02,  1.0338e-01],\n",
      "        [-7.0806e-02, -8.7071e-02, -3.7351e-02,  1.3211e-02,  2.2669e-02,\n",
      "          9.3280e-02,  1.7026e-01, -2.5614e-02,  7.2729e-02,  1.0058e-01],\n",
      "        [ 6.3733e-04, -1.3428e-02, -6.4180e-02, -9.2274e-02, -3.9772e-02,\n",
      "          6.7199e-02,  1.0399e-01,  4.5696e-02,  1.1571e-01,  1.4917e-01],\n",
      "        [-5.5321e-02, -8.5185e-02, -5.5942e-02, -7.2186e-02,  1.6849e-02,\n",
      "          7.6842e-02,  1.3985e-01, -3.1963e-02,  8.9534e-02,  1.1086e-01],\n",
      "        [-1.0464e-03, -6.8451e-02, -7.9451e-02, -8.5852e-02,  1.4611e-02,\n",
      "          4.7723e-02,  1.4494e-01,  4.4618e-03,  1.2075e-01,  1.4129e-01],\n",
      "        [ 1.7984e-02, -3.5683e-02, -3.1255e-03,  1.7969e-03, -8.7199e-02,\n",
      "         -1.1789e-02,  1.5423e-02,  2.1786e-02,  1.5729e-01,  5.2338e-02],\n",
      "        [-6.0341e-02, -5.8803e-02, -5.9428e-03,  2.0820e-03, -1.6208e-02,\n",
      "          9.4770e-02,  7.8275e-02,  2.0154e-02,  5.2036e-02,  1.0939e-01],\n",
      "        [-1.0045e-02, -3.6536e-02, -2.9226e-03,  2.6523e-03, -1.0094e-01,\n",
      "          6.2446e-02,  3.1640e-02,  4.1384e-02,  1.6090e-01,  4.2630e-02],\n",
      "        [-3.2949e-02, -6.2600e-02, -1.0676e-02, -5.0339e-03, -8.2933e-02,\n",
      "          3.6271e-03,  8.5365e-02,  2.7599e-02,  1.6128e-01,  7.0415e-02],\n",
      "        [-1.8439e-02, -1.8649e-02, -9.8966e-02, -1.0312e-01, -3.0551e-03,\n",
      "          2.2166e-02,  1.6114e-01,  3.4095e-02,  1.6479e-01,  1.3969e-01],\n",
      "        [-4.9960e-02,  9.9091e-03, -3.5992e-02,  1.5924e-02, -6.7159e-02,\n",
      "          8.0564e-02,  4.3937e-02,  4.3478e-02,  1.7772e-01,  1.0108e-01],\n",
      "        [-5.9974e-02, -9.0874e-03, -2.1851e-02, -5.6230e-03, -5.7075e-02,\n",
      "          7.7689e-02,  7.5260e-02,  3.3054e-03,  5.5501e-02,  8.2211e-02],\n",
      "        [ 8.6800e-03, -1.3218e-02, -5.3874e-02, -3.4054e-02, -7.0693e-02,\n",
      "          2.4309e-04,  8.8017e-02,  1.5182e-02,  8.8699e-02,  9.8568e-02],\n",
      "        [-4.0704e-02,  8.1443e-03, -8.5802e-02, -5.5743e-02, -4.5059e-02,\n",
      "          7.5385e-02,  6.1974e-02,  3.4407e-02,  1.2907e-01,  8.5748e-02],\n",
      "        [-2.2168e-02, -4.2904e-02, -4.4423e-02,  3.8633e-03, -6.4049e-02,\n",
      "          1.2748e-02,  3.9614e-02,  2.6529e-02,  9.2285e-02,  6.3787e-02],\n",
      "        [-5.9132e-02, -5.9942e-02, -7.6145e-02, -4.4523e-02, -1.2981e-02,\n",
      "          7.4424e-02,  7.1389e-02, -2.4653e-03,  6.5149e-02,  7.7977e-02],\n",
      "        [-7.9487e-02,  2.4348e-02, -6.5639e-02, -5.7126e-03, -7.3389e-02,\n",
      "          8.3641e-02,  2.7126e-02,  9.5733e-04,  1.6624e-01,  1.0846e-01],\n",
      "        [-1.4715e-01, -4.9189e-02, -7.3090e-02, -8.2946e-02, -6.8399e-03,\n",
      "          9.7197e-02,  1.5258e-01, -3.4839e-02,  1.3700e-01,  1.3004e-01],\n",
      "        [-1.0441e-01,  1.3045e-02, -5.7788e-02, -3.2118e-03, -7.4395e-02,\n",
      "          9.9275e-02,  7.4279e-02, -6.9510e-03,  1.0955e-01,  1.0546e-01],\n",
      "        [-3.6319e-02,  3.4849e-02,  1.2163e-02, -2.5903e-02, -1.8719e-02,\n",
      "          9.7642e-02,  5.1673e-02, -6.6145e-02,  1.1786e-01,  1.3602e-01],\n",
      "        [-8.2558e-02, -1.3885e-02, -5.9364e-02, -5.7844e-02, -2.7974e-02,\n",
      "          8.1582e-02,  5.2078e-02,  2.0807e-02,  9.5951e-02,  7.7401e-02],\n",
      "        [-4.1748e-02, -3.0049e-02, -7.7832e-02, -5.1968e-02, -5.1651e-02,\n",
      "          5.1304e-02,  8.5245e-02, -2.8172e-02,  1.4818e-01,  1.2431e-01],\n",
      "        [-1.1096e-01, -7.7943e-02,  1.7022e-02,  2.7550e-02, -1.6674e-02,\n",
      "          1.4216e-01,  7.5871e-02, -3.0839e-02,  6.7221e-02,  1.1557e-01],\n",
      "        [-8.2334e-02,  4.4130e-03, -6.4582e-02, -4.7024e-02, -4.3356e-02,\n",
      "          9.4791e-02,  9.6649e-02,  7.3635e-03,  1.1333e-01,  1.0189e-01],\n",
      "        [-6.8458e-02, -2.9212e-02, -6.5200e-02, -3.2005e-02, -2.3062e-02,\n",
      "          8.9558e-02,  7.4930e-02, -1.4553e-02,  1.1013e-01,  1.2753e-01],\n",
      "        [-4.7351e-02, -3.9356e-02,  7.3655e-03, -1.5855e-02, -8.1442e-02,\n",
      "          4.4962e-02,  6.6903e-02,  4.9043e-02,  1.9272e-01,  9.5376e-02],\n",
      "        [-6.9778e-02, -4.0716e-03, -8.6611e-02, -5.5343e-02, -3.8268e-02,\n",
      "          1.0674e-01,  3.8464e-02, -1.2719e-02,  1.4487e-01,  8.8336e-02],\n",
      "        [-4.7817e-02, -7.2837e-03, -4.5016e-02, -9.9233e-03, -9.9713e-02,\n",
      "          1.1118e-01,  7.4590e-02,  7.6375e-02,  1.4284e-01,  1.0175e-01],\n",
      "        [-3.2718e-02, -7.6685e-02, -4.1754e-02, -7.0836e-02, -5.6365e-02,\n",
      "          3.2644e-02,  9.2520e-02, -4.6960e-02,  1.6254e-01,  8.4576e-02],\n",
      "        [-2.2434e-02,  4.4289e-03, -6.8809e-02, -4.3904e-03, -3.7925e-02,\n",
      "          1.2716e-01,  8.3192e-02,  5.3110e-03,  1.1107e-01,  1.1164e-01],\n",
      "        [-3.7114e-02, -6.1618e-02, -1.7481e-02, -1.6481e-02, -2.8034e-02,\n",
      "          8.3821e-02,  6.3660e-02,  6.9126e-03,  1.2319e-01,  9.7716e-02],\n",
      "        [-3.4241e-02, -2.2101e-02, -8.4738e-02, -5.8225e-02, -5.1490e-02,\n",
      "          9.1324e-02,  5.0651e-02,  2.8391e-02,  1.3498e-01,  8.0603e-02]])\n"
     ]
    }
   ],
   "source": [
    "for data, target in test_loader:\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device = device)\n",
    "        target = target.to(device = device)\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa0f7649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORWARD ===\n",
      "module: Linear(in_features=84, out_features=10, bias=True)\n",
      "input: (tensor([[-0.1850,  0.0130, -0.0162,  ..., -0.0356, -0.1693,  0.0188],\n",
      "        [-0.1382,  0.0142,  0.0050,  ..., -0.0167, -0.2115, -0.1005],\n",
      "        [-0.0450,  0.0172,  0.0338,  ...,  0.1435, -0.1974, -0.0749],\n",
      "        ...,\n",
      "        [-0.1050, -0.0192, -0.0201,  ..., -0.1139, -0.2045, -0.0720],\n",
      "        [-0.1691,  0.0025,  0.0037,  ...,  0.0588, -0.1763, -0.1052],\n",
      "        [-0.1117,  0.0422, -0.0276,  ...,  0.0561, -0.2093, -0.0418]],\n",
      "       grad_fn=<BackwardHookFunctionBackward>),)\n",
      "output: tensor([[-3.3657e-02, -2.1945e-02, -1.0510e-01, -8.1433e-02, -4.6607e-02,\n",
      "          3.6759e-02,  1.4856e-01,  6.4578e-03,  1.6736e-01,  1.2749e-01],\n",
      "        [-5.6749e-02, -1.7369e-02,  4.3454e-02, -1.3543e-02, -4.2700e-02,\n",
      "          9.9895e-02,  1.6199e-02, -3.6899e-02,  2.7296e-02,  9.3829e-02],\n",
      "        [-5.0294e-02, -4.5394e-02,  1.0838e-02, -1.4505e-02, -7.9504e-02,\n",
      "          5.4853e-02,  8.3862e-02,  6.1071e-02,  1.8331e-01,  1.0990e-01],\n",
      "        [-4.0687e-02, -1.1085e-01, -2.0858e-02, -5.4023e-02,  3.9788e-02,\n",
      "          1.3164e-01,  1.5215e-01, -1.4451e-02,  6.1854e-02,  1.2218e-01],\n",
      "        [-1.0750e-01, -2.9162e-02, -6.5180e-02, -2.0617e-02, -4.4924e-02,\n",
      "          9.0009e-02,  1.4966e-01,  2.6035e-02,  8.6038e-02,  1.0553e-01],\n",
      "        [-5.7658e-02, -3.4351e-02,  5.0488e-04,  9.8153e-03, -8.5509e-02,\n",
      "          6.6868e-02,  7.1487e-02,  5.6190e-02,  1.8397e-01,  9.6146e-02],\n",
      "        [-4.1701e-02, -7.1144e-03, -1.0261e-01, -4.4528e-02, -5.3415e-02,\n",
      "          4.8876e-02,  6.4820e-02,  3.8049e-02,  1.8639e-01,  1.1697e-01],\n",
      "        [-8.9015e-02, -1.7653e-02,  1.8274e-02,  1.1496e-02, -4.6645e-02,\n",
      "          4.5996e-02,  7.1953e-02,  3.2179e-03,  8.4709e-02,  1.0509e-01],\n",
      "        [-4.1765e-02, -1.0501e-01, -2.8445e-02, -6.3763e-02,  4.7553e-02,\n",
      "          1.3317e-01,  1.0000e-01, -4.2410e-02,  2.7616e-02,  7.9190e-02],\n",
      "        [-3.1282e-02, -1.0842e-02, -8.5606e-02, -3.2895e-02, -5.0555e-02,\n",
      "          7.2272e-02,  6.8368e-02, -5.3195e-03,  1.8366e-01,  1.1845e-01],\n",
      "        [-4.8481e-02, -1.3801e-01, -6.4912e-02, -4.2172e-02,  2.8330e-02,\n",
      "          9.4988e-02,  1.6597e-01, -2.2051e-03,  5.5029e-02,  1.1101e-01],\n",
      "        [-4.6925e-02, -5.4444e-02, -4.4551e-02, -4.9964e-03, -6.9715e-02,\n",
      "          1.0319e-01,  2.4692e-02, -1.9896e-02,  6.1736e-02,  1.0389e-01],\n",
      "        [-5.8665e-02,  2.6841e-02, -8.3788e-02, -4.1571e-02, -6.2941e-02,\n",
      "          8.7653e-02,  5.9893e-02,  1.2522e-02,  1.5008e-01,  1.2095e-01],\n",
      "        [-5.7034e-02, -6.9984e-02, -8.2358e-03, -5.3724e-02,  4.4289e-02,\n",
      "          1.0582e-01,  1.5306e-01, -5.2138e-02,  1.2921e-01,  1.3164e-01],\n",
      "        [ 1.0399e-02, -4.8310e-02,  6.0381e-03,  1.4465e-04, -8.5876e-02,\n",
      "          2.0906e-02, -2.2974e-03,  1.1343e-02,  1.3052e-01,  2.9248e-02],\n",
      "        [-4.2394e-02, -5.3399e-02, -6.7541e-02, -7.5459e-02, -4.9613e-02,\n",
      "          1.9290e-02,  1.1096e-01, -2.6319e-02,  1.5352e-01,  1.0501e-01],\n",
      "        [-3.6868e-02, -4.1969e-02, -8.4207e-02, -4.7430e-02, -2.1108e-02,\n",
      "          9.5334e-02,  6.3317e-02, -2.0738e-02,  1.2000e-01,  1.1208e-01],\n",
      "        [-5.8378e-03, -2.8412e-02, -6.9127e-02, -1.0826e-01, -1.5537e-02,\n",
      "          3.4759e-02,  1.4839e-01,  9.7285e-03,  1.4366e-01,  1.0922e-01],\n",
      "        [-1.0900e-01,  4.9677e-02, -1.3155e-02,  1.9483e-02, -7.7750e-02,\n",
      "          7.2512e-02,  4.7192e-02, -8.1494e-02,  1.7136e-01,  1.5746e-01],\n",
      "        [-6.2508e-02, -3.4101e-03, -6.8787e-02, -1.7059e-02, -6.0393e-02,\n",
      "          8.0733e-02,  4.7594e-02,  1.4568e-03,  1.5289e-01,  9.7751e-02],\n",
      "        [-2.6399e-02,  5.6901e-03, -6.5344e-02, -2.0755e-02, -8.9771e-02,\n",
      "          9.2273e-02,  9.9730e-02,  3.5539e-02,  1.9679e-01,  9.2008e-02],\n",
      "        [-9.4924e-02, -3.9836e-03, -3.1798e-02,  2.4042e-02, -6.9395e-02,\n",
      "          9.2285e-02,  6.1511e-02, -5.1906e-03,  9.0830e-02,  1.0106e-01],\n",
      "        [-4.8087e-02, -5.1074e-02, -5.3914e-03,  7.5254e-03,  1.8365e-03,\n",
      "          1.1054e-01,  8.1034e-02,  2.1766e-03,  7.4318e-02,  1.1956e-01],\n",
      "        [-4.5796e-02, -1.8774e-02, -8.0391e-02, -4.7500e-02, -4.4278e-02,\n",
      "          6.2147e-02,  3.1497e-02,  8.0355e-03,  1.0716e-01,  1.0181e-01],\n",
      "        [-7.7237e-02, -2.9855e-02, -7.3867e-02, -5.1065e-02, -3.0383e-02,\n",
      "          6.4265e-02,  8.6399e-02, -9.1436e-03,  6.3697e-02,  1.0197e-01],\n",
      "        [-3.4471e-02, -1.2905e-01, -6.9230e-02, -2.7738e-02,  5.0849e-02,\n",
      "          1.6351e-01,  1.6740e-01, -1.9013e-02,  3.5006e-02,  1.1201e-01],\n",
      "        [-2.6055e-02, -7.3135e-02, -7.8939e-02, -6.1634e-02, -6.9802e-02,\n",
      "          5.1310e-02,  9.4473e-02, -6.9485e-03,  1.4564e-01,  6.2939e-02],\n",
      "        [-9.3665e-02,  9.5575e-03, -8.4391e-02, -2.5933e-02, -7.5746e-02,\n",
      "          8.3714e-02,  7.9254e-02,  9.9987e-03,  1.1473e-01,  6.8344e-02],\n",
      "        [-4.2384e-02, -1.9334e-02, -1.3760e-02, -3.5174e-02, -2.7393e-03,\n",
      "          8.6420e-02,  9.8763e-02, -1.9114e-02,  9.4249e-02,  8.9664e-02],\n",
      "        [ 1.0663e-02, -2.0513e-02, -1.7309e-02, -6.3565e-03, -9.5237e-02,\n",
      "         -9.6206e-03,  1.3162e-02,  3.2685e-02,  1.3127e-01,  6.5001e-02],\n",
      "        [-1.7613e-02, -2.3643e-03, -4.5009e-02, -2.6302e-02, -5.8780e-02,\n",
      "          3.5551e-02,  4.9694e-02,  4.9377e-03,  1.3238e-01,  8.6696e-02],\n",
      "        [-2.7111e-02, -6.5142e-02, -2.3466e-02, -3.1744e-02, -5.1563e-02,\n",
      "         -1.5841e-02,  4.7304e-02,  2.9396e-02,  1.6751e-01,  4.3680e-02],\n",
      "        [-2.2343e-02, -6.4448e-03, -7.1399e-02, -5.4035e-02, -2.5843e-02,\n",
      "          4.4898e-02,  4.6472e-02, -2.1348e-02,  6.8093e-02,  1.0338e-01],\n",
      "        [-7.0806e-02, -8.7071e-02, -3.7351e-02,  1.3211e-02,  2.2669e-02,\n",
      "          9.3280e-02,  1.7026e-01, -2.5614e-02,  7.2729e-02,  1.0058e-01],\n",
      "        [ 6.3733e-04, -1.3428e-02, -6.4180e-02, -9.2274e-02, -3.9772e-02,\n",
      "          6.7199e-02,  1.0399e-01,  4.5696e-02,  1.1571e-01,  1.4917e-01],\n",
      "        [-5.5321e-02, -8.5185e-02, -5.5942e-02, -7.2186e-02,  1.6849e-02,\n",
      "          7.6842e-02,  1.3985e-01, -3.1963e-02,  8.9534e-02,  1.1086e-01],\n",
      "        [-1.0464e-03, -6.8451e-02, -7.9451e-02, -8.5852e-02,  1.4611e-02,\n",
      "          4.7723e-02,  1.4494e-01,  4.4618e-03,  1.2075e-01,  1.4129e-01],\n",
      "        [ 1.7984e-02, -3.5683e-02, -3.1255e-03,  1.7969e-03, -8.7199e-02,\n",
      "         -1.1789e-02,  1.5423e-02,  2.1786e-02,  1.5729e-01,  5.2338e-02],\n",
      "        [-6.0341e-02, -5.8803e-02, -5.9428e-03,  2.0820e-03, -1.6208e-02,\n",
      "          9.4770e-02,  7.8275e-02,  2.0154e-02,  5.2036e-02,  1.0939e-01],\n",
      "        [-1.0045e-02, -3.6536e-02, -2.9226e-03,  2.6523e-03, -1.0094e-01,\n",
      "          6.2446e-02,  3.1640e-02,  4.1384e-02,  1.6090e-01,  4.2630e-02],\n",
      "        [-3.2949e-02, -6.2600e-02, -1.0676e-02, -5.0339e-03, -8.2933e-02,\n",
      "          3.6271e-03,  8.5365e-02,  2.7599e-02,  1.6128e-01,  7.0415e-02],\n",
      "        [-1.8439e-02, -1.8649e-02, -9.8966e-02, -1.0312e-01, -3.0551e-03,\n",
      "          2.2166e-02,  1.6114e-01,  3.4095e-02,  1.6479e-01,  1.3969e-01],\n",
      "        [-4.9960e-02,  9.9091e-03, -3.5992e-02,  1.5924e-02, -6.7159e-02,\n",
      "          8.0564e-02,  4.3937e-02,  4.3478e-02,  1.7772e-01,  1.0108e-01],\n",
      "        [-5.9974e-02, -9.0874e-03, -2.1851e-02, -5.6230e-03, -5.7075e-02,\n",
      "          7.7689e-02,  7.5260e-02,  3.3054e-03,  5.5501e-02,  8.2211e-02],\n",
      "        [ 8.6800e-03, -1.3218e-02, -5.3874e-02, -3.4054e-02, -7.0693e-02,\n",
      "          2.4309e-04,  8.8017e-02,  1.5182e-02,  8.8699e-02,  9.8568e-02],\n",
      "        [-4.0704e-02,  8.1443e-03, -8.5802e-02, -5.5743e-02, -4.5059e-02,\n",
      "          7.5385e-02,  6.1974e-02,  3.4407e-02,  1.2907e-01,  8.5748e-02],\n",
      "        [-2.2168e-02, -4.2904e-02, -4.4423e-02,  3.8633e-03, -6.4049e-02,\n",
      "          1.2748e-02,  3.9614e-02,  2.6529e-02,  9.2285e-02,  6.3787e-02],\n",
      "        [-5.9132e-02, -5.9942e-02, -7.6145e-02, -4.4523e-02, -1.2981e-02,\n",
      "          7.4424e-02,  7.1389e-02, -2.4653e-03,  6.5149e-02,  7.7977e-02],\n",
      "        [-7.9487e-02,  2.4348e-02, -6.5639e-02, -5.7126e-03, -7.3389e-02,\n",
      "          8.3641e-02,  2.7126e-02,  9.5733e-04,  1.6624e-01,  1.0846e-01],\n",
      "        [-1.4715e-01, -4.9189e-02, -7.3090e-02, -8.2946e-02, -6.8399e-03,\n",
      "          9.7197e-02,  1.5258e-01, -3.4839e-02,  1.3700e-01,  1.3004e-01],\n",
      "        [-1.0441e-01,  1.3045e-02, -5.7788e-02, -3.2118e-03, -7.4395e-02,\n",
      "          9.9275e-02,  7.4279e-02, -6.9510e-03,  1.0955e-01,  1.0546e-01],\n",
      "        [-3.6319e-02,  3.4849e-02,  1.2163e-02, -2.5903e-02, -1.8719e-02,\n",
      "          9.7642e-02,  5.1673e-02, -6.6145e-02,  1.1786e-01,  1.3602e-01],\n",
      "        [-8.2558e-02, -1.3885e-02, -5.9364e-02, -5.7844e-02, -2.7974e-02,\n",
      "          8.1582e-02,  5.2078e-02,  2.0807e-02,  9.5951e-02,  7.7401e-02],\n",
      "        [-4.1748e-02, -3.0049e-02, -7.7832e-02, -5.1968e-02, -5.1651e-02,\n",
      "          5.1304e-02,  8.5245e-02, -2.8172e-02,  1.4818e-01,  1.2431e-01],\n",
      "        [-1.1096e-01, -7.7943e-02,  1.7022e-02,  2.7550e-02, -1.6674e-02,\n",
      "          1.4216e-01,  7.5871e-02, -3.0839e-02,  6.7221e-02,  1.1557e-01],\n",
      "        [-8.2334e-02,  4.4130e-03, -6.4582e-02, -4.7024e-02, -4.3356e-02,\n",
      "          9.4791e-02,  9.6649e-02,  7.3635e-03,  1.1333e-01,  1.0189e-01],\n",
      "        [-6.8458e-02, -2.9212e-02, -6.5200e-02, -3.2005e-02, -2.3062e-02,\n",
      "          8.9558e-02,  7.4930e-02, -1.4553e-02,  1.1013e-01,  1.2753e-01],\n",
      "        [-4.7351e-02, -3.9356e-02,  7.3655e-03, -1.5855e-02, -8.1442e-02,\n",
      "          4.4962e-02,  6.6903e-02,  4.9043e-02,  1.9272e-01,  9.5376e-02],\n",
      "        [-6.9778e-02, -4.0716e-03, -8.6611e-02, -5.5343e-02, -3.8268e-02,\n",
      "          1.0674e-01,  3.8464e-02, -1.2719e-02,  1.4487e-01,  8.8336e-02],\n",
      "        [-4.7817e-02, -7.2837e-03, -4.5016e-02, -9.9233e-03, -9.9713e-02,\n",
      "          1.1118e-01,  7.4590e-02,  7.6375e-02,  1.4284e-01,  1.0175e-01],\n",
      "        [-3.2718e-02, -7.6685e-02, -4.1754e-02, -7.0836e-02, -5.6365e-02,\n",
      "          3.2644e-02,  9.2520e-02, -4.6960e-02,  1.6254e-01,  8.4576e-02],\n",
      "        [-2.2434e-02,  4.4289e-03, -6.8809e-02, -4.3904e-03, -3.7925e-02,\n",
      "          1.2716e-01,  8.3192e-02,  5.3110e-03,  1.1107e-01,  1.1164e-01],\n",
      "        [-3.7114e-02, -6.1618e-02, -1.7481e-02, -1.6481e-02, -2.8034e-02,\n",
      "          8.3821e-02,  6.3660e-02,  6.9126e-03,  1.2319e-01,  9.7716e-02],\n",
      "        [-3.4241e-02, -2.2101e-02, -8.4738e-02, -5.8225e-02, -5.1490e-02,\n",
      "          9.1324e-02,  5.0651e-02,  2.8391e-02,  1.3498e-01,  8.0603e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "=== BACKWARD ===\n",
      "module: Linear(in_features=84, out_features=10, bias=True)\n",
      "input: (tensor([[-4.4752e-04, -6.4354e-04, -9.3029e-04,  ...,  4.8924e-04,\n",
      "          1.8002e-05, -1.2086e-03],\n",
      "        [ 6.6431e-04,  8.8430e-04, -1.0367e-03,  ...,  9.2129e-04,\n",
      "          1.8964e-04,  9.7434e-04],\n",
      "        [-4.9636e-04, -1.0910e-03,  1.1594e-03,  ..., -1.2079e-03,\n",
      "         -1.3065e-03, -1.1773e-03],\n",
      "        ...,\n",
      "        [ 1.8065e-04,  1.7698e-03,  1.3171e-03,  ..., -9.9319e-04,\n",
      "          1.1376e-03, -3.7825e-04],\n",
      "        [-4.3943e-04, -1.1343e-04, -8.8631e-05,  ..., -1.2555e-03,\n",
      "          1.0708e-03,  3.1036e-04],\n",
      "        [ 4.2523e-04,  8.8034e-04, -1.3801e-03,  ...,  1.0413e-03,\n",
      "         -1.3480e-03, -1.2230e-03]]),)\n",
      "output: (tensor([[ 0.0015,  0.0015,  0.0014,  0.0014,  0.0015,  0.0016,  0.0018, -0.0141,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0015,  0.0015, -0.0140,  0.0015,  0.0015,  0.0017,  0.0016,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0014, -0.0142,  0.0015,  0.0015,  0.0014,  0.0016,  0.0016,  0.0016,\n",
      "          0.0018,  0.0017],\n",
      "        [-0.0142,  0.0014,  0.0015,  0.0014,  0.0016,  0.0017,  0.0018,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0014,  0.0015,  0.0014,  0.0015, -0.0142,  0.0017,  0.0018,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014, -0.0142,  0.0015,  0.0015,  0.0014,  0.0016,  0.0016,  0.0016,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0015, -0.0142,  0.0016,  0.0016,  0.0016,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0014,  0.0015,  0.0016,  0.0015,  0.0015,  0.0016,  0.0016,  0.0015,\n",
      "          0.0017, -0.0139],\n",
      "        [ 0.0015,  0.0014,  0.0015,  0.0014,  0.0016, -0.0139,  0.0017,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0015,  0.0014,  0.0016,  0.0016,  0.0015,\n",
      "          0.0018, -0.0139],\n",
      "        [-0.0142,  0.0013,  0.0014,  0.0015,  0.0016,  0.0017,  0.0018,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0015,  0.0015,  0.0014,  0.0017, -0.0140,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014,  0.0016,  0.0014,  0.0015,  0.0014,  0.0017,  0.0016,  0.0015,\n",
      "          0.0018, -0.0139],\n",
      "        [-0.0142,  0.0014,  0.0015,  0.0014,  0.0016,  0.0017,  0.0018,  0.0014,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0016, -0.0141,  0.0016,  0.0015,  0.0014,  0.0016,  0.0015,  0.0016,\n",
      "          0.0018,  0.0016],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0014,  0.0015, -0.0140,  0.0017,  0.0015,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0015,  0.0015,  0.0017,  0.0016,  0.0015,\n",
      "          0.0017, -0.0139],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0014,  0.0015,  0.0016,  0.0018, -0.0141,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0014,  0.0016,  0.0015, -0.0141,  0.0014,  0.0016,  0.0016,  0.0014,\n",
      "          0.0018,  0.0018],\n",
      "        [ 0.0014,  0.0015,  0.0014,  0.0015, -0.0142,  0.0017,  0.0016,  0.0015,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0015,  0.0014,  0.0017,  0.0017,  0.0016,\n",
      "          0.0018, -0.0140],\n",
      "        [ 0.0014,  0.0015,  0.0015,  0.0016,  0.0014,  0.0017, -0.0140,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014,  0.0014,  0.0015,  0.0015,  0.0015,  0.0017, -0.0140,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0015,  0.0015, -0.0140,  0.0016,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014,  0.0015,  0.0014,  0.0015, -0.0141,  0.0017,  0.0017,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [-0.0142,  0.0013,  0.0014,  0.0015,  0.0016,  0.0018,  0.0018,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0015,  0.0014,  0.0014,  0.0015,  0.0014,  0.0016,  0.0017, -0.0141,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0014,  0.0016,  0.0014,  0.0015, -0.0142,  0.0017,  0.0017,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [-0.0142,  0.0015,  0.0015,  0.0015,  0.0015,  0.0017,  0.0017,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0016, -0.0141,  0.0015,  0.0015,  0.0014,  0.0015,  0.0016,  0.0016,\n",
      "          0.0018,  0.0016],\n",
      "        [ 0.0015,  0.0015,  0.0015, -0.0141,  0.0014,  0.0016,  0.0016,  0.0015,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0015, -0.0142,  0.0015,  0.0015,  0.0015,  0.0015,  0.0016,  0.0016,\n",
      "          0.0018,  0.0016],\n",
      "        [ 0.0015,  0.0015,  0.0014, -0.0142,  0.0015,  0.0016,  0.0016,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014,  0.0014,  0.0015,  0.0015, -0.0141,  0.0017,  0.0018,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0014,  0.0015,  0.0016,  0.0017, -0.0140,\n",
      "          0.0017,  0.0018],\n",
      "        [ 0.0015,  0.0014, -0.0142,  0.0014,  0.0016,  0.0017,  0.0018,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0015,  0.0014,  0.0014,  0.0014,  0.0015,  0.0016,  0.0018, -0.0141,\n",
      "          0.0017,  0.0018],\n",
      "        [ 0.0016, -0.0141,  0.0015,  0.0015,  0.0014,  0.0015,  0.0016,  0.0016,\n",
      "          0.0018,  0.0016],\n",
      "        [ 0.0014,  0.0014, -0.0141,  0.0015,  0.0015,  0.0017,  0.0017,  0.0016,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0015, -0.0142,  0.0015,  0.0015,  0.0014,  0.0016,  0.0016,  0.0016,\n",
      "          0.0018,  0.0016],\n",
      "        [ 0.0015, -0.0142,  0.0015,  0.0015,  0.0014,  0.0015,  0.0017,  0.0016,\n",
      "          0.0018,  0.0016],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0014,  0.0015,  0.0015,  0.0018, -0.0141,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0014,  0.0015,  0.0015,  0.0015, -0.0142,  0.0016,  0.0016,  0.0016,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0014,  0.0015, -0.0141,  0.0015,  0.0015,  0.0017,  0.0017,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [ 0.0016,  0.0015,  0.0015, -0.0141,  0.0014,  0.0015,  0.0017,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0014,  0.0015, -0.0140,  0.0016,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0015, -0.0141,  0.0015,  0.0016,  0.0015,  0.0016,  0.0016,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0015,  0.0015, -0.0142,  0.0015,  0.0015,  0.0017,  0.0017,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014,  0.0016,  0.0014,  0.0015, -0.0142,  0.0017,  0.0016,  0.0015,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0013,  0.0015,  0.0014,  0.0014, -0.0141,  0.0017,  0.0018,  0.0015,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0014,  0.0016,  0.0014,  0.0015,  0.0014,  0.0017, -0.0140,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0015,  0.0016,  0.0015, -0.0142,  0.0015,  0.0017,  0.0016,  0.0014,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014,  0.0015,  0.0015,  0.0015,  0.0015, -0.0139,  0.0016,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0015,  0.0015, -0.0140,  0.0017,  0.0015,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0014,  0.0014,  0.0016,  0.0016,  0.0015,  0.0018, -0.0140,  0.0015,\n",
      "          0.0016,  0.0017],\n",
      "        [-0.0142,  0.0015,  0.0014,  0.0015,  0.0015,  0.0017,  0.0017,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014,  0.0015,  0.0014,  0.0015, -0.0141,  0.0017,  0.0017,  0.0015,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0014, -0.0142,  0.0015,  0.0015,  0.0014,  0.0016,  0.0016,  0.0016,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0014,  0.0015,  0.0014,  0.0015,  0.0015,  0.0017,  0.0016,  0.0015,\n",
      "          0.0018, -0.0139],\n",
      "        [ 0.0014,  0.0015,  0.0014,  0.0015,  0.0014, -0.0139,  0.0016,  0.0016,\n",
      "          0.0017,  0.0017],\n",
      "        [ 0.0015,  0.0014,  0.0015,  0.0014,  0.0015,  0.0016,  0.0017, -0.0141,\n",
      "          0.0018,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0014,  0.0015,  0.0015,  0.0017,  0.0016,  0.0015,\n",
      "         -0.0139,  0.0017],\n",
      "        [ 0.0015,  0.0014,  0.0015,  0.0015,  0.0015,  0.0017,  0.0016,  0.0015,\n",
      "          0.0017, -0.0139],\n",
      "        [ 0.0015,  0.0015,  0.0014, -0.0142,  0.0015,  0.0017,  0.0016,  0.0016,\n",
      "          0.0018,  0.0017]]),)\n"
     ]
    }
   ],
   "source": [
    "def backhook(module, grad_input, grad_output):\n",
    "    print(\"=== BACKWARD ===\")\n",
    "    print(\"module:\", module)\n",
    "    print(\"input:\", grad_input)\n",
    "    print(\"output:\", grad_output)\n",
    "\n",
    "model.fc3.register_full_backward_hook(backhook)\n",
    "\n",
    "for data, target in test_loader:\n",
    "    optimizer.zero_grad()\n",
    "    data = data.to(device = device)\n",
    "    target = target.to(device = device)\n",
    "\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8584f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
