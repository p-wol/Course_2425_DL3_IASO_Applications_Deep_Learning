{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e44a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa85210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"~/datasets\"\n",
    "\n",
    "with_cuda = torch.cuda.is_available()\n",
    "if with_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a03f0a",
   "metadata": {},
   "source": [
    "# Reminders on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5fa2ba",
   "metadata": {},
   "source": [
    "## Building the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32626c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(height, width):\n",
    "    img = torch.zeros((1, height, width), device = device)\n",
    "    j_pos = torch.randint(2, width - 2, (1,))\n",
    "    for i in range(height):\n",
    "        for j in range(j_pos - 2, j_pos + 2):\n",
    "            img[0, i, j] = 1\n",
    "    cl = torch.randint(0, 4, (1,)).item()\n",
    "    img = transforms.functional.rotate(img, 45*cl)\n",
    "    return img, cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e706eb",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "With the class `torch.utils.data.TensorDataset`, build a dataset generated by the function `generate_image`. Show some samples with matplotlib functiob `imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69fb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed96ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d20df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256748a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5e7892b",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Put the dataset into a `DataLoader`. What are the differences between a dataset and a data loader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328fabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd07e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee149cd8",
   "metadata": {},
   "source": [
    "What are Conv2d, Linear and MaxPool?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a94006",
   "metadata": {},
   "source": [
    "In a dataset, the elements can be accessed individually given an index (with `[i]`). \n",
    "\n",
    "A data loader is an `Iterable`, which means that its elements can only be accessed through an `Iterator` that traverses the dataset in a way that is provided by the arguments of the data loader (batch size, random/deterministic choice of the data points, etc.). In particular, it can be traversed with a `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cfb19",
   "metadata": {},
   "source": [
    "## Simple NN/training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc174d",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Write a neural network that classifies the data points previously generated. One can use a convolutional and a fully-connected layer.\n",
    "\n",
    "What are the parameters of a convolutional layer? What is their shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b7f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCVNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCVNN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6adc31e",
   "metadata": {},
   "source": [
    "Parameters: `conv.bias`, `conv.weight`.\n",
    "\n",
    "`conv.bias`: size `out_channels`\n",
    "\n",
    "`conv.weight`: size `out_channels` * `in_channels` * `kernel_height` * `kernel_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe07fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94871624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b3f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nepochs):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # accuracy\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(data_loader.dataset)\n",
    "        accuracy = correct/len(data_loader.dataset)*100\n",
    "        train_acc.append(accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # print losses statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining accuracy: {:.6f}'.format(\n",
    "            epoch, train_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffcf3f",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Choose the loss and train the NN. Why do we choose to minimize the loss instead of maximizing the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc7126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a565687",
   "metadata": {},
   "source": [
    "The derivative of the accuracy w.r.t. the parameters of the NN is zero, so the NN cannot be trained with this \"loss\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ae167",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoders (VAE): MNIST and FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b01cea",
   "metadata": {},
   "source": [
    "## Building a standard fully-connected VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56309857",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Complete the class `VAE_FC`:\n",
    " * build an encoder module;\n",
    " * build a decoder module;\n",
    " * write the method `encode`, which takes an input `x` and returns means and log-variances;\n",
    " * write the method `decode`, which takes a random variable `z` and returns the reconstructed image;\n",
    " * write the method `reparameterization`, taking means and variances and returning normal samples with these means and variances;\n",
    " * write the method `forward`, which takes an input `x` and returns its reconstruction `x_hat`, and the mean/variance of the latent representation.\n",
    "\n",
    "Additionally:\n",
    " * write the method `sample`, which generates new data;\n",
    " * write the method `reconstruct`, which attempts to reconstruct the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e2df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "#mean, std = .2860, .3530\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed00d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_FC(nn.Module):\n",
    "    def __init__(self, layers, latent_dim = 200, leak = .1):\n",
    "        super(VAE_FC, self).__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # encoder\n",
    "        \n",
    "        # latent mean and variance\n",
    "        self.fc_mean = nn.Linear(layers[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(layers[-1], latent_dim)\n",
    "        \n",
    "        # decoder input\n",
    "     \n",
    "    def encode(self, x):\n",
    "        pass\n",
    "\n",
    "    def reparameterization(self, mean, logvar):\n",
    "        pass\n",
    "\n",
    "    def decode(self, x):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        pass\n",
    "    \n",
    "    def reconstruct(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd01c8",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Write the function `build_loss_vae`, which builds the loss function for the VAE. The reconstruction loss and the KL loss will be balanced by two parameters `lambda_reconstruct` and `lambda_kl` (by default, they are equal to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40288bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0715b0e6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2f3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_vae(data_loader, model, criterion, optimizer, nepochs):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (input_, target) in enumerate(data_loader):\n",
    "            input_ = input_.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            # ...\n",
    "\n",
    "            # calculate the batch loss\n",
    "            # ...\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * input_.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(data_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # print losses statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb759d",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Train the VAE on MNIST or FashionMNIST. Don't forget to save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03abd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b78e6e",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "With the resulting model:\n",
    " * show some generated samples;\n",
    " * check the quality of the reconstruction;\n",
    " * show some interpolations between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb96ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18949e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816bd42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68c769aa",
   "metadata": {},
   "source": [
    "## Conditional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f592233",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Modify the VAE to build a conditional VAE:\n",
    " * the input vectors should be enriched with a one-hot vector coding for the class;\n",
    " * went sent to the decoder, the latent representation should be enriched with a one-hot vector coding for the class.\n",
    "\n",
    "Modify also the function `train_model_vae`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76704ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4757b060",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 20 (2948407004.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 20\n"
     ]
    }
   ],
   "source": [
    "def train_model_vae(data_loader, model, criterion, optimizer, nepochs, cond = False):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (input_, target) in enumerate(data_loader):\n",
    "            input_ = input_.to(device)\n",
    "            target = target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            if cond:\n",
    "                # ...\n",
    "            else:\n",
    "                # ...\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(input_, output_, mean, logvar)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * input_.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(data_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # print losses statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ad686",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Train the model.\n",
    "\n",
    "With the resulting model:\n",
    " * show some generated samples;\n",
    " * check the quality of the reconstruction;\n",
    " * show some interpolations between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425a662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa19a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2a004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb288c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260a11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557b612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016652b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff6f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a437366d",
   "metadata": {},
   "source": [
    "# Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59dffa",
   "metadata": {},
   "source": [
    "The goal is to build a variational inference (VI) model. We will use the family of independent Gaussian distributions, which means that each parameter $\\theta_k$ of the model will be randomly generated w.r.t. a Gaussian distribution $\\mathcal{N}(\\mu_k, \\sigma_k^2)$. The parameters to be learned are the $(\\mu_1, \\sigma_1^2, \\cdots, \\mu_p, \\sigma_p^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efeebc2",
   "metadata": {},
   "source": [
    "To simplify, we will work with a multilayer perceptron. For each pass trhough the model, we will generate randomly the weights and the biases w.r.t. their own Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a940b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "#mean, std = .2860, .3530\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539a9bf",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Build a `Module` named `LinearVI` that is similar to the `Linear` layer, but contains the means and variances of the independent Gaussian distributions generating the weights and the biases:\n",
    "* the log-variances will be stored instead the variances themselves;\n",
    "* the means/log-variances should be implemented both for the weights and the biases;\n",
    "* the `forward` method should be implemented: it is recommended to start with generating Gaussian noise, and then translate and scale it to obtain the weights and biases; one can use the function `F.Linear` to compute the output;\n",
    "* the attributes `weight_prior_logvar` and `bias_prior_logvar` contain the initialization log-variance for the weighs and for the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearVI(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        self.weight_prior_logvar = np.log(1 / n_in)\n",
    "        self.bias_prior_logvar = np.log(.01)\n",
    "        \n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b972b",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Build a multilayer perceptron `PerceptronVI` made of fully-connected variational layers (`LinearVI`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eaf0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronVI(torch.nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df895e6",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Complete the train function below. The `loss_kl` is defined as:\n",
    "$$\n",
    "L(\\boldsymbol{\\theta}) = \\lambda \\sum_{k = 1}^p \\left[ \\frac{1}{2} \\log\\left(\\frac{\\sigma_{0k}^2}{\\sigma_k^2}\\right) +\\frac{\\sigma_k^2 + \\mu_k^2}{2 \\sigma_{0k}^2} + \\frac{1}{2}\\right] ,\n",
    "$$\n",
    "where $\\sigma_{0k}^2$ is the variance of the prior distribution on $\\theta_k$ (defined for each layer via `weight_prior_logvar` and `bias_prior_logvar`) and $\\lambda$ is the penalty factor `pen_factor`.\n",
    "\n",
    "It is recommended to add new methods to `LinearVI` and `PerceptronVI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589dde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_vi(model, criterion, optimizer, nepochs, pen_factor = 1/60000):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss_fit = criterion(output, target)\n",
    "            loss_kl = # ... TODO HERE\n",
    "            loss = loss_fit + loss_kl\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # accuracy\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        accuracy = correct/len(train_loader.dataset)*100\n",
    "        train_acc.append(accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # print losses statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining accuracy: {:.6f}'.format(\n",
    "            epoch, train_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c06c3a",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Train the model for various $\\lambda$ and plot the posterior distributions obtained for parameters of different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bab2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419877a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
